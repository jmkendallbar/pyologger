{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data upload to PelicanFS\n",
    "PelicanFS is a file system interface (fsspec) for the Pelican Platform. For more information about pelican, see PelicanFS [main website](https://pelicanplatform.org/) or [Github page](https://github.com/pelicanplatform/pelicanfs). For more information about fsspec, visit the [filesystem-spec](https://filesystem-spec.readthedocs.io/en/latest/index.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pelicanfs\n",
    "import aiohttp\n",
    "import certifi\n",
    "from pelicanfs.core import PelicanFileSystem\n",
    "import ssl\n",
    "ssl_ctx = ssl.create_default_context(cafile=certifi.where())\n",
    "conn = aiohttp.TCPConnector(ssl=ssl_ctx, loop=fsspec.asyn.get_loop())\n",
    "pelfs = PelicanFileSystem(\"pelican://osg-htc.org\", client_kwargs={\"connector\": conn})\n",
    "hello_world = pelfs.cat('/jkb-lab-public/downloaded-test.txt')\n",
    "print(str(hello_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_world = pelfs.cat('/ospool/uc-shared/public/OSG-Staff/validation/test.txt')\n",
    "print(hello_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hello_world = pelfs.cat(f\"/jkb-lab-public/{foo}?authz=Bearer%20{token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder for importing data into DiveDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, param_manager = select_and_load_deployment(\n",
    "    data_dir, dataset_id=\"oror-adult-orca_hr-sr-vid_sw_JKB-PP\", deployment_id=\"2023-10-18_oror-001\"\n",
    "    )\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info['ecg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_output.nc')\n",
    "\n",
    "# Open the NetCDF file\n",
    "data = xr.open_dataset(netcdf_path)\n",
    "\n",
    "# Display the contents of the NetCDF file\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example uploading netCDF file to DiveDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DELTA_LAKE = config['paths']['delta_lake']['local']\n",
    "logger_ids = '_'.join(data_pkl.logger_info.keys())\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond(LOCAL_DELTA_LAKE, connect_to_postgres=False)\n",
    "data_uploader = DataUploader(duckpond=duckpond)\n",
    "\n",
    "metadata = {\n",
    "            \"animal\": animal_id,\n",
    "            \"deployment\": deployment_id,\n",
    "            \"recording\": f\"{deployment_id}_{animal_id}_{logger_ids}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uploader.upload_netcdf(netcdf_path, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckpond.conn.sql(\"SELECT count(*) FROM DataLake\").df()\n",
    "\n",
    "display(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckpond.get_delta_data(\n",
    "    labels=[\"derived_data_depth\"],\n",
    "    animal_ids = animal_id,\n",
    "    frequency = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT label, avg(value) as mean_data\n",
    "    FROM (\n",
    "        SELECT label, value.float as value\n",
    "        FROM DataLake\n",
    "        WHERE label = 'sensor_data_light'\n",
    "        OR label = 'sensor_data_temperature'\n",
    "    )\n",
    "    GROUP BY label\n",
    "\"\"\").df()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filtered data\n",
    "filtered_data = duckpond.get_delta_data(    \n",
    "    animal_ids=\"apfo-001a\",\n",
    "    # Resample values to 10 Hz and make sure each signal has the same time intervals\n",
    "    frequency=10,\n",
    "    # Aggregation of events (think state events - behaviors) type: state (has state and end dates)\n",
    "    classes=\"sensor_data_accelerometer\",\n",
    "    \n",
    ")\n",
    "\n",
    "display(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"sensor_data_temperature\"\n",
    "df = duckpond.conn.execute(f\"\"\"\n",
    "    SELECT label, avg(value) as mean_data\n",
    "    FROM (\n",
    "        SELECT label, value.float as value\n",
    "        FROM DataLake\n",
    "        WHERE label = $1\n",
    "    )\n",
    "    GROUP BY label\n",
    "\"\"\", [label]).df()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Export to EDF\n",
    "\n",
    "When it's easier to work with EDF files, we can export the data to an EDF file. This is useful for working with the data in other software packages.\n",
    "\n",
    "Calling `export_to_edf(output_dir)` on a `DiveData` object creates one output EDF file for each recording in the `DiveData` relation, saved to `output_dir` with filename `<recording_id>.edf`. \n",
    "\n",
    "*Note: it currently requires a lot of memory. Can be improved.*<br/>\n",
    "*Note: it's lacking support for most info fields in the EDF file.*\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "import DiveDB.services.dive_data\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "importlib.reload(DiveDB.services.dive_data)\n",
    "\n",
    "duckpond = DuckPond(LOCAL_DELTA_LAKE, connect_to_postgres=False)\n",
    "\n",
    "dive_data = duckpond.get_delta_data(    \n",
    "    animal_ids=\"apfo-001a\",\n",
    "    labels=[\"sensor_data_temperature\", \"derived_data_depth\"],\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "output_edf_paths = dive_data.export_to_edf(\".tmp/my_output_dir/\")\n",
    "display(output_edf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example importing NetCDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "#import netcdf4\n",
    "\n",
    "# Load the NetCDF file\n",
    "file_path = f\"{data_dir}/2004001_TrackTDR_RawCurated.nc\"\n",
    "dataset = xr.open_dataset(file_path)\n",
    "\n",
    "# Print the dataset information\n",
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
