{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Import pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.utils.json_manager import ConfigManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, channel_mapping_path = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata\n",
    "\n",
    "Load in metadata stored in Notion databases. Alternatively, load in your own metadata in separate dataframes for deployments, loggers, recordings, animals, and datasets. See examples here in the `metadata_snapshot.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Notion to initialize the metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "metadata.find_relations(verbose=False)\n",
    "\n",
    "# Fetch databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dataset_db = metadata.get_metadata(\"dataset_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save metadata snapshot as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Save metadata snapshot as a pickle file\n",
    "\n",
    "# Combine DataFrames into a dictionary\n",
    "metadata_databases = {\n",
    "    'deployment_db': deployment_db,\n",
    "    'logger_db': logger_db,\n",
    "    'recording_db': recording_db,\n",
    "    'animal_db': animal_db,\n",
    "    'dataset_db': dataset_db\n",
    "}\n",
    "\n",
    "# Save metadata snapshot as a pickle file\n",
    "metadata_pickle_path = os.path.join(data_dir, \"00_Metadata/metadata_snapshot.pkl\")\n",
    "with open(metadata_pickle_path, \"wb\") as file:\n",
    "    pickle.dump(metadata_databases, file)\n",
    "\n",
    "print(f\"Metadata snapshot saved at: {metadata_pickle_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset folder\n",
    "dataset_folder = select_folder(data_dir, \"Select a dataset folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = select_folder(dataset_folder, \"Select a deployment folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deployment_id and animal_id from the folder name\n",
    "match = re.match(r\"(\\d{4}-\\d{2}-\\d{2}_[a-z]{4}-\\d{3})\", os.path.basename(deployment_folder), re.IGNORECASE)\n",
    "if match:\n",
    "    deployment_id = match.group(1)  # Extract YYYY-MM-DD_animalID\n",
    "    animal_id = deployment_id.split(\"_\")[1]  # Extract animal ID\n",
    "    print(f\"✅ Extracted deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "else:\n",
    "    raise ValueError(f\"❌ Unable to extract deployment ID from folder: {deployment_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize DataReader with dataset folder, deployment ID, and optional data subfolder\n",
    "datareader = DataReader(dataset_folder=dataset_folder, deployment_id=deployment_id, data_subfolder=\"01_raw-data\")\n",
    "# Step 5: Initialize config manager\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)\n",
    "config_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data import pending.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "### Steps for Processing Deployment Data: \n",
    "The following steps follow the [`datareader`](../pyologger/load_data/datareader.py) class and specifically the `read_files()` method.\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Prompts the user to select a deployment folder to initiate the data reading process. The folder name can include any suffix after the Deployment ID. The function checks for potential conflicts and halts the process if multiple matching folders are found.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Begins the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieves essential data from the metadata database, including logger and animal information.\n",
    "   - **Functions Used:** `metadata.fetch_databases()`, `get_animal_info()`, `get_dataset_info()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Groups files by logger ID for processing.\n",
    "   - **Function Used:** `organize_files_by_logger_id()` (within `read_files()`)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verifies if the output folder already contains processed files for each logger. Skips reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process Manufacturer-Specific Files**:\n",
    "   - **Description:** Depending on the logger's manufacturer, different processing methods are applied. The `BaseManufacturer` class and its subclasses (`CATSManufacturer`, `UFIManufacturer`) handle the specifics, such as processing `.txt` files for CATS loggers or `.ube` files for UFI loggers.\n",
    "   - **Functions Used:** `BaseManufacturer.process_files()`, `CATSManufacturer.process_files()`, `UFIManufacturer.process_files()`\n",
    "\n",
    "7. **Save Processed Data**:\n",
    "   - **Description:** Saves processed data files in the outputs folder with appropriate filenames, ensuring consistency across different formats such as CSV, Parquet, and EDF.\n",
    "   - **Functions Used:** `save_data()`, `export_to_edf()`, `save_to_netcdf()`\n",
    "\n",
    "8. **Finalize and Save DataReader Object**:\n",
    "   - **Description:** Saves the state of the `DataReader` object, including all processed data and metadata, as a pickle file for easy retrieval and future processing.\n",
    "   - **Function Used:** `save_datareader_object()`\n",
    "\n",
    "### Classes and Their Roles:\n",
    "\n",
    "- **`DataReader` Class:**\n",
    "  - **Role:** Handles the overall reading, processing, and saving of deployment data files. It manages the deployment folder path, organizes data by logger and sensor, and interfaces with manufacturer-specific processing through the `BaseManufacturer` class.\n",
    "  - **Methods:** `read_files()`, `save_datareader_object()`, `organize_files_by_logger_id()`, `save_data()`, `export_to_edf()`, `save_to_netcdf()`, and more.\n",
    "\n",
    "- **`BaseManufacturer` Class:**\n",
    "  - **Role:** Acts as a template for manufacturer-specific processing classes. It defines common methods for loading custom mappings, renaming columns, and mapping data to sensors. Subclasses like `CATSManufacturer` and `UFIManufacturer` extend this base class to implement specific processing logic.\n",
    "  - **Methods:** `process_files()`, `load_custom_mapping()`, `rename_columns()`, `map_data_to_sensors()`, `parse_txt_for_intervals()`, and more.\n",
    "\n",
    "- **`metadata` Class (from `pyologger.load_data.metadata`):**\n",
    "  - **Role:** Provides methods for fetching and managing metadata related to deployments, loggers, animals, and datasets. It plays a crucial role in ensuring the correct organization and processing of data.\n",
    "  - **Methods:** `fetch_databases()`, `get_metadata()`, and others as needed for metadata handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Read deployment files\n",
    "data_pkl = datareader.read_files(\n",
    "    metadata,\n",
    "    save_csv=False,\n",
    "    save_parq=False,\n",
    "    save_edf=False,\n",
    "    custom_mapping_path=channel_mapping_path,\n",
    "    save_netcdf=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Step 8: Update processing step\n",
    "config_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data imported.\")\n",
    "\n",
    "# Step 9: Open NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, \"outputs\", f'{deployment_id}_00_processed.nc')\n",
    "if os.path.exists(netcdf_path):\n",
    "    data = xr.open_dataset(netcdf_path)\n",
    "    print(f\"📊 NetCDF file loaded: {netcdf_path}\")\n",
    "else:\n",
    "    print(f\"⚠ NetCDF file not found at {netcdf_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_data['ecg']['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "data_pkl = datareader\n",
    "# Get timezone\n",
    "timezone = data_pkl.deployment_info.get(\"Time Zone\", \"UTC\")\n",
    "\n",
    "# Load time settings\n",
    "time_settings = config_manager.get_from_config(\n",
    "    [\"overlap_start_time\", \"overlap_end_time\", \"zoom_window_start_time\", \"zoom_window_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "if time_settings:\n",
    "    print(\"Time settings present.\")\n",
    "# If any required time settings are missing, compute and update them\n",
    "if not any(v is None for v in time_settings.values()):\n",
    "    print(\"Time settings not empty.\")\n",
    "else:\n",
    "    print(\"Adding timestamps to config.\")\n",
    "    zoom_time_window = 5  # minutes\n",
    "\n",
    "    # Extract start and end times for all sensors\n",
    "    start_times = [df['datetime'].min() for df in data_pkl.sensor_data.values()]\n",
    "    end_times = [df['datetime'].max() for df in data_pkl.sensor_data.values()]\n",
    "\n",
    "    # Compute common start, end, and zoom window\n",
    "    overlap_start_time = max(start_times)\n",
    "    overlap_end_time = min(end_times)\n",
    "    midpoint = overlap_start_time + (overlap_end_time - overlap_start_time) / 2\n",
    "    zoom_window_start, zoom_window_end = midpoint - timedelta(minutes=zoom_time_window / 2), midpoint + timedelta(minutes=zoom_time_window / 2)\n",
    "\n",
    "    # Update settings\n",
    "    time_settings = {\n",
    "        \"overlap_start_time\": str(overlap_start_time),\n",
    "        \"overlap_end_time\": str(overlap_end_time),\n",
    "        \"zoom_window_start_time\": str(zoom_window_start),\n",
    "        \"zoom_window_end_time\": str(zoom_window_end),\n",
    "    }\n",
    "    config_manager.add_to_config(entries=time_settings, section=\"settings\")\n",
    "\n",
    "if any(v is None for v in time_settings.values()):\n",
    "    print(\"YES\")\n",
    "time_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected start and end times exist in the config file\n",
    "truncate_times = config_manager.get_from_config(\n",
    "    [\"selected_start_time\", \"selected_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "truncate_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not any(v is None for v in truncate_times.values()):\n",
    "    print(\"Truncating with provided cropping times.\")\n",
    "    # Update overlap window with selected range\n",
    "    OVERLAP_START_TIME = pd.Timestamp(truncate_times['selected_start_time']).tz_convert(timezone)\n",
    "    OVERLAP_END_TIME = pd.Timestamp(truncate_times['selected_end_time']).tz_convert(timezone)\n",
    "\n",
    "    # Truncate sensor data\n",
    "    for sensor, df in data_pkl.sensor_data.items():\n",
    "        # Truncate based on selected time range\n",
    "        truncated_df = df[(df.iloc[:, 0] >= OVERLAP_START_TIME) & (df.iloc[:, 0] <= OVERLAP_END_TIME)].copy()\n",
    "        data_pkl.sensor_data[sensor] = truncated_df  # Save truncated version to new variable\n",
    "\n",
    "    # Recalculate Zoom Window (5-minute window in the middle)\n",
    "    midpoint = OVERLAP_START_TIME + (OVERLAP_END_TIME - OVERLAP_START_TIME) / 2\n",
    "    ZOOM_WINDOW_START_TIME = midpoint - timedelta(minutes=2.5)\n",
    "    ZOOM_WINDOW_END_TIME = midpoint + timedelta(minutes=2.5)\n",
    "\n",
    "    # Save new time settings\n",
    "    time_settings_update = {\n",
    "        \"overlap_start_time\": str(OVERLAP_START_TIME),\n",
    "        \"overlap_end_time\": str(OVERLAP_END_TIME),\n",
    "        \"zoom_window_start_time\": str(ZOOM_WINDOW_START_TIME),\n",
    "        \"zoom_window_end_time\": str(ZOOM_WINDOW_END_TIME)\n",
    "    }\n",
    "    config_manager.add_to_config(entries=time_settings_update, section=\"settings\")\n",
    "\n",
    "    pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "    with open(pkl_path, \"wb\") as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    zoom_start_time= ZOOM_WINDOW_START_TIME,\n",
    "    zoom_end_time= ZOOM_WINDOW_END_TIME,\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
