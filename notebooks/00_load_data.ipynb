{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Import pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.utils.param_manager import ParamManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata\n",
    "\n",
    "Load in metadata stored in Notion databases. Alternatively, load in your own metadata in separate dataframes for deployments, loggers, recordings, animals, and datasets. See examples here in the `metadata_snapshot.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "overwrite = False\n",
    "# Define the path to the metadata pickle file\n",
    "metadata_pickle_path = os.path.join(data_dir, \"00_Metadata/metadata_snapshot.pkl\")\n",
    "\n",
    "# Check if the metadata pickle file exists and is more than 5 days old\n",
    "if overwrite or not os.path.exists(metadata_pickle_path) or (datetime.now() - datetime.fromtimestamp(os.path.getmtime(metadata_pickle_path))) > timedelta(days=14):\n",
    "    \n",
    "    metadata = Metadata()\n",
    "\n",
    "    # Save database variables\n",
    "    deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "    logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "    recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "    animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "    dataset_db = metadata.get_metadata(\"dataset_DB\")\n",
    "    procedure_db = metadata.get_metadata(\"procedure_DB\")\n",
    "    observation_db = metadata.get_metadata(\"observation_DB\")\n",
    "    collaborator_db = metadata.get_metadata(\"collaborator_DB\")\n",
    "    location_db = metadata.get_metadata(\"location_DB\")\n",
    "    montage_db = metadata.get_metadata(\"montage_DB\")\n",
    "    sensor_db = metadata.get_metadata(\"sensor_DB\")\n",
    "    attachment_db = metadata.get_metadata(\"attachment_DB\")\n",
    "    originalchannel_db = metadata.get_metadata(\"originalchannel_DB\")\n",
    "    standardizedchannel_db = metadata.get_metadata(\"standardizedchannel_DB\")\n",
    "    derivedsignal_db = metadata.get_metadata(\"derivedsignal_DB\")\n",
    "    derivedchannel_db = metadata.get_metadata(\"derivedchannel_DB\")\n",
    "\n",
    "    # Get the relations map\n",
    "    relations_map = metadata.relations_map\n",
    "\n",
    "    # Define the path to save the relations map\n",
    "    relations_map_path = os.path.join(config['paths']['local_repo_path'], 'relations_map.json')\n",
    "\n",
    "    # Save the relations map as a JSON file\n",
    "    with open(relations_map_path, 'w') as file:\n",
    "        json.dump(relations_map, file, indent=4)\n",
    "\n",
    "    print(f\"Relations map saved at: {relations_map_path}\")\n",
    "\n",
    "    ## OPTIONAL: Save metadata snapshot as a pickle file\n",
    "    metadata.notion = None  # Temporarily remove the Notion client\n",
    "    # Save metadata snapshot as a pickle file\n",
    "    with open(metadata_pickle_path, \"wb\") as file:\n",
    "        pickle.dump(metadata, file)\n",
    "\n",
    "    print(f\"Metadata snapshot saved at: {metadata_pickle_path}\")\n",
    "else:\n",
    "    print(f\"Recent metadata snapshot loaded, already present at: {metadata_pickle_path}\")\n",
    "    \n",
    "    # Load the metadata snapshot from the pickle file\n",
    "    with open(metadata_pickle_path, \"rb\") as file:\n",
    "        metadata = pickle.load(file)\n",
    "    \n",
    "    # Save database variables\n",
    "    deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "    logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "    recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "    animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "    dataset_db = metadata.get_metadata(\"dataset_DB\")\n",
    "    procedure_db = metadata.get_metadata(\"procedure_DB\")\n",
    "    observation_db = metadata.get_metadata(\"observation_DB\")\n",
    "    collaborator_db = metadata.get_metadata(\"collaborator_DB\")\n",
    "    location_db = metadata.get_metadata(\"location_DB\")\n",
    "    montage_db = metadata.get_metadata(\"montage_DB\")\n",
    "    sensor_db = metadata.get_metadata(\"sensor_DB\")\n",
    "    attachment_db = metadata.get_metadata(\"attachment_DB\")\n",
    "    originalchannel_db = metadata.get_metadata(\"originalchannel_DB\")\n",
    "    standardizedchannel_db = metadata.get_metadata(\"standardizedchannel_DB\")\n",
    "    derivedsignal_db = metadata.get_metadata(\"derivedsignal_DB\")\n",
    "    derivedchannel_db = metadata.get_metadata(\"derivedchannel_DB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save metadata snapshot as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset folder\n",
    "dataset_folder = select_folder(data_dir, \"Select a dataset folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = select_folder(dataset_folder, \"Select a deployment folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deployment_id and animal_id from the folder name\n",
    "match = re.match(r\"(\\d{4}-\\d{2}-\\d{2}_[a-z]{4}-\\d{3})\", os.path.basename(deployment_folder), re.IGNORECASE)\n",
    "if match:\n",
    "    deployment_id = match.group(1)  # Extract YYYY-MM-DD_animalID\n",
    "    animal_id = deployment_id.split(\"_\")[1]  # Extract animal ID\n",
    "    print(f\"âœ… Extracted deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "else:\n",
    "    raise ValueError(f\"âŒ Unable to extract deployment ID from folder: {deployment_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "Uses [`datareader`](../pyologger/load_data/datareader.py) class and its `read_files()` method to load and standardize data from a deployment folder, map onto standardized channel names, and save as a `data_pkl` object (instance of the datareader class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print extracted values for debugging\n",
    "print(f\"ðŸ³ Deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "\n",
    "deployment_info, loggers_used = metadata.extract_essential_metadata(deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "overwrite_essential_metadata = False\n",
    "\n",
    "if overwrite_essential_metadata: # If you store your metadata differently, you can set this manually:\n",
    "    # Deployment ID and Animal ID - this is important because it sets the start date and animal ID\n",
    "    # Your dataset ID is the folder name that this deployment folder is in\n",
    "    deployment_id = \"2019-11-08_apfo-001\"\n",
    "    animal_id = \"apfo-001\"\n",
    "    print(f\"ðŸ” Manually setting essential metadata for Deployment ID: {deployment_id}\")\n",
    "\n",
    "    # Manually setting deployment metadata\n",
    "    deployment_info = {\n",
    "        \"Deployment Date\": \"2019-11-08\",\n",
    "        \"Deployment Latitude\": -77.858933,\n",
    "        \"Deployment Longitude\": 166.5139,\n",
    "        \"Time Zone\": \"Antarctica/McMurdo\"\n",
    "    }\n",
    "    print(f\"ðŸ“ Deployment Metadata: {deployment_info}\")\n",
    "\n",
    "    # Manually setting loggers used with Montage ID inside each entry\n",
    "    loggers_used = [\n",
    "        {\"Logger ID\": \"CC-35\", \"Manufacturer\": \"CATS\", \"Montage ID\": \"cats-penguin-video-montage_V1\"}\n",
    "    ]\n",
    "    print(f\"ðŸ“Ÿ Loggers Used: {loggers_used}\")\n",
    "\n",
    "    # No separate montage_id list anymore, since it's stored in loggers_used\n",
    "\n",
    "def list_available_timezones():\n",
    "    \"\"\"\n",
    "    Prints all available time zones in pytz.\n",
    "    \"\"\"\n",
    "    timezones = pytz.all_timezones\n",
    "    print(\"\\nðŸŒ Available Time Zones in pytz:\\n\")\n",
    "    for tz in timezones:\n",
    "        print(tz)\n",
    "\n",
    "# List all available time zones\n",
    "#list_available_timezones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize DataReader with dataset folder, deployment ID, and optional data subfolder\n",
    "data_pkl = DataReader(dataset_folder=dataset_folder, deployment_id=deployment_id, data_subfolder=\"01_raw-data\", montage_path=montage_path)\n",
    "# Step 5: Initialize config manager\n",
    "param_manager = ParamManager(deployment_folder=deployment_folder, deployment_id=deployment_id)\n",
    "param_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data import pending.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_manager.export_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_data = False\n",
    "pkl_path = os.path.join(deployment_folder, \"outputs\", \"data.pkl\")\n",
    "if os.path.exists(pkl_path) and not overwrite_data:\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data_pkl = pickle.load(f)\n",
    "    print(f\"ðŸ“¦ Loaded processed DataReader object from: {pkl_path}\")\n",
    "else:\n",
    "    data_pkl = DataReader(\n",
    "        dataset_folder=dataset_folder,\n",
    "        deployment_id=deployment_id,\n",
    "        data_subfolder=\"01_raw-data\",\n",
    "        montage_path=montage_path\n",
    "    )\n",
    "    param_manager = ParamManager(deployment_folder=deployment_folder, deployment_id=deployment_id)\n",
    "    param_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data import pending.\")\n",
    "\n",
    "    data_pkl.read_files(\n",
    "        deployment_info=deployment_info,\n",
    "        loggers_used=loggers_used,\n",
    "        save_parq=False,\n",
    "        save_netcdf=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.animal_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.logger_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.derived_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.animal_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info['light']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Get timezone\n",
    "timezone = data_pkl.deployment_info.get(\"Time Zone\", \"UTC\")\n",
    "\n",
    "# Load time settings\n",
    "time_settings = param_manager.get_from_config(\n",
    "    [\"overlap_start_time\", \"overlap_end_time\", \"zoom_window_start_time\", \"zoom_window_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "if time_settings:\n",
    "    print(\"Time settings present.\")\n",
    "# If any required time settings are missing, compute and update them\n",
    "if not any(v is None for v in time_settings.values()):\n",
    "    print(\"Time settings not empty.\")\n",
    "else:\n",
    "    print(\"Adding timestamps to config.\")\n",
    "    zoom_time_window = 5  # minutes\n",
    "\n",
    "    # Extract start and end times for all sensors\n",
    "    start_times = [df['datetime'].min() for df in data_pkl.sensor_data.values()]\n",
    "    end_times = [df['datetime'].max() for df in data_pkl.sensor_data.values()]\n",
    "\n",
    "    # Compute common start, end, and zoom window\n",
    "    overlap_start_time = max(start_times)\n",
    "    overlap_end_time = min(end_times)\n",
    "    midpoint = overlap_start_time + (overlap_end_time - overlap_start_time) / 2\n",
    "    zoom_window_start, zoom_window_end = midpoint - timedelta(minutes=zoom_time_window / 2), midpoint + timedelta(minutes=zoom_time_window / 2)\n",
    "\n",
    "    # Update settings\n",
    "    time_settings = {\n",
    "        \"overlap_start_time\": str(overlap_start_time),\n",
    "        \"overlap_end_time\": str(overlap_end_time),\n",
    "        \"zoom_window_start_time\": str(zoom_window_start),\n",
    "        \"zoom_window_end_time\": str(zoom_window_end),\n",
    "    }\n",
    "    param_manager.add_to_config(entries=time_settings, section=\"settings\")\n",
    "\n",
    "if any(v is None for v in time_settings.values()):\n",
    "    print(\"YES\")\n",
    "time_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp(time_settings['overlap_start_time']) + timedelta(hours = 10)\n",
    "end = pd.Timestamp(time_settings['overlap_start_time']) + timedelta(hours = 15)\n",
    "\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['eeg', 'ecg', 'pressure', 'accelerometer'],\n",
    "    time_range=(start, end),\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    zoom_range_selector_channel='eeg',\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=25\n",
    ")\n",
    "fig.show_dash(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Step 8: Update processing step\n",
    "param_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data imported.\")\n",
    "\n",
    "# Step 9: Open NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, \"outputs\", f'{deployment_id}_00_processed.nc')\n",
    "if os.path.exists(netcdf_path):\n",
    "    data = xr.open_dataset(netcdf_path)\n",
    "    print(f\"ðŸ“Š NetCDF file loaded: {netcdf_path}\")\n",
    "else:\n",
    "    print(f\"âš  NetCDF file not found at {netcdf_path}.\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected start and end times exist in the config file\n",
    "truncate_times = param_manager.get_from_config(\n",
    "    [\"selected_start_time\", \"selected_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "truncate_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not any(v is None for v in truncate_times.values()):\n",
    "    print(\"Truncating with provided cropping times.\")\n",
    "    # Update overlap window with selected range\n",
    "    OVERLAP_START_TIME = pd.Timestamp(truncate_times['selected_start_time']).tz_convert(timezone)\n",
    "    OVERLAP_END_TIME = pd.Timestamp(truncate_times['selected_end_time']).tz_convert(timezone)\n",
    "\n",
    "    # Truncate sensor data\n",
    "    for sensor, df in data_pkl.sensor_data.items():\n",
    "        # Truncate based on selected time range\n",
    "        truncated_df = df[(df.iloc[:, 0] >= OVERLAP_START_TIME) & (df.iloc[:, 0] <= OVERLAP_END_TIME)].copy()\n",
    "        data_pkl.sensor_data[sensor] = truncated_df  # Save truncated version to new variable\n",
    "\n",
    "    # Recalculate Zoom Window (5-minute window in the middle)\n",
    "    midpoint = OVERLAP_START_TIME + (OVERLAP_END_TIME - OVERLAP_START_TIME) / 2\n",
    "    ZOOM_WINDOW_START_TIME = midpoint - timedelta(minutes=2.5)\n",
    "    ZOOM_WINDOW_END_TIME = midpoint + timedelta(minutes=2.5)\n",
    "\n",
    "    # Save new time settings\n",
    "    time_settings_update = {\n",
    "        \"overlap_start_time\": str(OVERLAP_START_TIME),\n",
    "        \"overlap_end_time\": str(OVERLAP_END_TIME),\n",
    "        \"zoom_window_start_time\": str(ZOOM_WINDOW_START_TIME),\n",
    "        \"zoom_window_end_time\": str(ZOOM_WINDOW_END_TIME)\n",
    "    }\n",
    "    param_manager.add_to_config(entries=time_settings_update, section=\"settings\")\n",
    "\n",
    "    pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "    with open(pkl_path, \"wb\") as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    zoom_start_time= ZOOM_WINDOW_START_TIME,\n",
    "    zoom_end_time= ZOOM_WINDOW_END_TIME,\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
