{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import xarray as xr\n",
    "\n",
    "# Import pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.json_manager import ConfigManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, channel_mapping_path = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata\n",
    "\n",
    "Load in metadata stored in Notion databases. Alternatively, load in your own metadata in separate dataframes for deployments, loggers, recordings, animals, and datasets. See examples here in the `metadata_snapshot.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Notion to initialize the metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "metadata.find_relations(verbose=False)\n",
    "\n",
    "# Fetch databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dataset_db = metadata.get_metadata(\"dataset_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save metadata snapshot as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Save metadata snapshot as a pickle file\n",
    "\n",
    "# Combine DataFrames into a dictionary\n",
    "metadata_databases = {\n",
    "    'deployment_db': deployment_db,\n",
    "    'logger_db': logger_db,\n",
    "    'recording_db': recording_db,\n",
    "    'animal_db': animal_db,\n",
    "    'dataset_db': dataset_db\n",
    "}\n",
    "\n",
    "# Save metadata snapshot as a pickle file\n",
    "metadata_pickle_path = os.path.join(data_dir, \"00_Metadata/metadata_snapshot.pkl\")\n",
    "with open(metadata_pickle_path, \"wb\") as file:\n",
    "    pickle.dump(metadata_databases, file)\n",
    "\n",
    "print(f\"Metadata snapshot saved at: {metadata_pickle_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset folder\n",
    "dataset_folder = select_folder(data_dir, \"Select a dataset folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = select_folder(dataset_folder, \"Select a deployment folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deployment_id and animal_id from the folder name\n",
    "match = re.match(r\"(\\d{4}-\\d{2}-\\d{2}_[a-z]{4}-\\d{3})\", os.path.basename(deployment_folder), re.IGNORECASE)\n",
    "if match:\n",
    "    deployment_id = match.group(1)  # Extract YYYY-MM-DD_animalID\n",
    "    animal_id = deployment_id.split(\"_\")[1]  # Extract animal ID\n",
    "    print(f\"‚úÖ Extracted deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå Unable to extract deployment ID from folder: {deployment_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize DataReader with dataset folder, deployment ID, and optional data subfolder\n",
    "datareader = DataReader(dataset_folder=dataset_folder, deployment_id=deployment_id, data_subfolder=\"01_raw-data\")\n",
    "# Step 5: Initialize config manager\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)\n",
    "config_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data import pending.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "### Steps for Processing Deployment Data: \n",
    "The following steps follow the [`datareader`](../pyologger/load_data/datareader.py) class and specifically the `read_files()` method.\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Prompts the user to select a deployment folder to initiate the data reading process. The folder name can include any suffix after the Deployment ID. The function checks for potential conflicts and halts the process if multiple matching folders are found.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Begins the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieves essential data from the metadata database, including logger and animal information.\n",
    "   - **Functions Used:** `metadata.fetch_databases()`, `get_animal_info()`, `get_dataset_info()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Groups files by logger ID for processing.\n",
    "   - **Function Used:** `organize_files_by_logger_id()` (within `read_files()`)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verifies if the output folder already contains processed files for each logger. Skips reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process Manufacturer-Specific Files**:\n",
    "   - **Description:** Depending on the logger's manufacturer, different processing methods are applied. The `BaseManufacturer` class and its subclasses (`CATSManufacturer`, `UFIManufacturer`) handle the specifics, such as processing `.txt` files for CATS loggers or `.ube` files for UFI loggers.\n",
    "   - **Functions Used:** `BaseManufacturer.process_files()`, `CATSManufacturer.process_files()`, `UFIManufacturer.process_files()`\n",
    "\n",
    "7. **Save Processed Data**:\n",
    "   - **Description:** Saves processed data files in the outputs folder with appropriate filenames, ensuring consistency across different formats such as CSV, Parquet, and EDF.\n",
    "   - **Functions Used:** `save_data()`, `export_to_edf()`, `save_to_netcdf()`\n",
    "\n",
    "8. **Finalize and Save DataReader Object**:\n",
    "   - **Description:** Saves the state of the `DataReader` object, including all processed data and metadata, as a pickle file for easy retrieval and future processing.\n",
    "   - **Function Used:** `save_datareader_object()`\n",
    "\n",
    "### Classes and Their Roles:\n",
    "\n",
    "- **`DataReader` Class:**\n",
    "  - **Role:** Handles the overall reading, processing, and saving of deployment data files. It manages the deployment folder path, organizes data by logger and sensor, and interfaces with manufacturer-specific processing through the `BaseManufacturer` class.\n",
    "  - **Methods:** `read_files()`, `save_datareader_object()`, `organize_files_by_logger_id()`, `save_data()`, `export_to_edf()`, `save_to_netcdf()`, and more.\n",
    "\n",
    "- **`BaseManufacturer` Class:**\n",
    "  - **Role:** Acts as a template for manufacturer-specific processing classes. It defines common methods for loading custom mappings, renaming columns, and mapping data to sensors. Subclasses like `CATSManufacturer` and `UFIManufacturer` extend this base class to implement specific processing logic.\n",
    "  - **Methods:** `process_files()`, `load_custom_mapping()`, `rename_columns()`, `map_data_to_sensors()`, `parse_txt_for_intervals()`, and more.\n",
    "\n",
    "- **`metadata` Class (from `pyologger.load_data.metadata`):**\n",
    "  - **Role:** Provides methods for fetching and managing metadata related to deployments, loggers, animals, and datasets. It plays a crucial role in ensuring the correct organization and processing of data.\n",
    "  - **Methods:** `fetch_databases()`, `get_metadata()`, and others as needed for metadata handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Read deployment files\n",
    "data_pkl = datareader.read_files(\n",
    "    metadata,\n",
    "    save_csv=False,\n",
    "    save_parq=False,\n",
    "    save_edf=False,\n",
    "    custom_mapping_path=channel_mapping_path,\n",
    "    save_netcdf=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Update processing step\n",
    "config_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data imported.\")\n",
    "\n",
    "# Step 9: Open NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, \"outputs\", f'{deployment_id}_00_processed.nc')\n",
    "if os.path.exists(netcdf_path):\n",
    "    data = xr.open_dataset(netcdf_path)\n",
    "    print(f\"üìä NetCDF file loaded: {netcdf_path}\")\n",
    "else:\n",
    "    print(f\"‚ö† NetCDF file not found at {netcdf_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.accelerometer_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datareader.logger_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
