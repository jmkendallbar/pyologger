{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.config_manager import ConfigManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import *\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata\n",
    "\n",
    "Load in metadata stored in Notion databases. Alternatively, load in your own metadata in separate dataframes for deployments, loggers, recordings, animals, and datasets. See examples here in the `metadata_snapshot.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Notion to initialize the metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "metadata.find_relations(verbose=False)\n",
    "\n",
    "# Fetch databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dataset_db = metadata.get_metadata(\"dataset_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save metadata snapshot as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Save metadata snapshot as a pickle file\n",
    "\n",
    "# Combine DataFrames into a dictionary\n",
    "metadata_databases = {\n",
    "    'deployment_db': deployment_db,\n",
    "    'logger_db': logger_db,\n",
    "    'recording_db': recording_db,\n",
    "    'animal_db': animal_db,\n",
    "    'dataset_db': dataset_db\n",
    "}\n",
    "\n",
    "# Define the path to save the metadata pickle file\n",
    "metadata_pickle_path = os.path.join(data_dir, 'metadata_snapshot.pkl')\n",
    "\n",
    "# Save only the DataFrames as a pickle file\n",
    "with open(metadata_pickle_path, 'wb') as file:\n",
    "    pickle.dump(metadata_databases, file)\n",
    "\n",
    "print(f\"Metadata DataFrames saved as pickle file at: {metadata_pickle_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_databases['deployment_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the deployment dataframe.\n",
    "deployment_db_sorted = deployment_db.sort_values('Recording Date')\n",
    "deployment_db_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "### Steps for Processing Deployment Data: \n",
    "The following steps follow the [`datareader`](../pyologger/load_data/datareader.py) class and specifically the `read_files()` method.\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Prompts the user to select a deployment folder to initiate the data reading process. The folder name can include any suffix after the Deployment ID. The function checks for potential conflicts and halts the process if multiple matching folders are found.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Begins the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieves essential data from the metadata database, including logger and animal information.\n",
    "   - **Functions Used:** `metadata.fetch_databases()`, `get_animal_info()`, `get_dataset_info()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Groups files by logger ID for processing.\n",
    "   - **Function Used:** `organize_files_by_logger_id()` (within `read_files()`)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verifies if the output folder already contains processed files for each logger. Skips reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process Manufacturer-Specific Files**:\n",
    "   - **Description:** Depending on the logger's manufacturer, different processing methods are applied. The `BaseManufacturer` class and its subclasses (`CATSManufacturer`, `UFIManufacturer`) handle the specifics, such as processing `.txt` files for CATS loggers or `.ube` files for UFI loggers.\n",
    "   - **Functions Used:** `BaseManufacturer.process_files()`, `CATSManufacturer.process_files()`, `UFIManufacturer.process_files()`\n",
    "\n",
    "7. **Save Processed Data**:\n",
    "   - **Description:** Saves processed data files in the outputs folder with appropriate filenames, ensuring consistency across different formats such as CSV, Parquet, and EDF.\n",
    "   - **Functions Used:** `save_data()`, `export_to_edf()`, `save_to_netcdf()`\n",
    "\n",
    "8. **Finalize and Save DataReader Object**:\n",
    "   - **Description:** Saves the state of the `DataReader` object, including all processed data and metadata, as a pickle file for easy retrieval and future processing.\n",
    "   - **Function Used:** `save_datareader_object()`\n",
    "\n",
    "### Classes and Their Roles:\n",
    "\n",
    "- **`DataReader` Class:**\n",
    "  - **Role:** Handles the overall reading, processing, and saving of deployment data files. It manages the deployment folder path, organizes data by logger and sensor, and interfaces with manufacturer-specific processing through the `BaseManufacturer` class.\n",
    "  - **Methods:** `read_files()`, `save_datareader_object()`, `organize_files_by_logger_id()`, `save_data()`, `export_to_edf()`, `save_to_netcdf()`, and more.\n",
    "\n",
    "- **`BaseManufacturer` Class:**\n",
    "  - **Role:** Acts as a template for manufacturer-specific processing classes. It defines common methods for loading custom mappings, renaming columns, and mapping data to sensors. Subclasses like `CATSManufacturer` and `UFIManufacturer` extend this base class to implement specific processing logic.\n",
    "  - **Methods:** `process_files()`, `load_custom_mapping()`, `rename_columns()`, `map_data_to_sensors()`, `parse_txt_for_intervals()`, and more.\n",
    "\n",
    "- **`metadata` Class (from `pyologger.load_data.metadata`):**\n",
    "  - **Role:** Provides methods for fetching and managing metadata related to deployments, loggers, animals, and datasets. It plays a crucial role in ensuring the correct organization and processing of data.\n",
    "  - **Methods:** `fetch_databases()`, `get_metadata()`, and others as needed for metadata handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your custom mapping file\n",
    "channel_mapping_path = os.path.join(root_dir, 'channel_mapping.json')\n",
    "datareader = DataReader(deployment_folder_path=data_dir)\n",
    "\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)\n",
    "\n",
    "current_processing_step = \"Processing Step 00 In progress: data import pending.\"\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "edf_filename_template = os.path.join(datareader.files_info['deployment_folder_path'], 'outputs', 'edf_test_{sensor}.edf')\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=False, save_parq=False, save_edf=False, \n",
    "                          custom_mapping_path=channel_mapping_path, save_netcdf=True,\n",
    "                          edf_filename_template=edf_filename_template, edf_save_from='sensor_data')\n",
    "\n",
    "\n",
    "current_processing_step = \"Processing Step 00 DATA IMPORTED.\"\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, 'outputs', 'deployment_data.nc')\n",
    "\n",
    "# Open the NetCDF file\n",
    "data = xr.open_dataset(netcdf_path)\n",
    "\n",
    "# Display the contents of the NetCDF file\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyologger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
