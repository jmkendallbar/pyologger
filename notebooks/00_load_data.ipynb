{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import pickle\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.calibrate_acc_mag import *\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "#os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "#os.chdir(\"/Users/williamgislason/Documents/Whitecap/Seals/Finescale-HR\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "metadata.find_relations(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dataset_db = metadata.get_metadata(\"dataset_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the deployment dataframe.\n",
    "deployment_db_sorted = deployment_db.sort_values('Recording Date')\n",
    "deployment_db_sorted\n",
    "#recording_db\n",
    "#logger_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "### Steps for Processing Deployment Data:\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Prompts the user to select a deployment folder to initiate the data reading process. The folder name can include any suffix after the Deployment ID. The function checks for potential conflicts and halts the process if multiple matching folders are found.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Begins the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieves essential data from the metadata database, including logger and animal information.\n",
    "   - **Functions Used:** `metadata.fetch_databases()`, `get_animal_info()`, `get_dataset_info()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Groups files by logger ID for processing.\n",
    "   - **Function Used:** `organize_files_by_logger_id()` (within `read_files()`)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verifies if the output folder already contains processed files for each logger. Skips reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process Manufacturer-Specific Files**:\n",
    "   - **Description:** Depending on the logger's manufacturer, different processing methods are applied. The `BaseManufacturer` class and its subclasses (`CATSManufacturer`, `UFIManufacturer`) handle the specifics, such as processing `.txt` files for CATS loggers or `.ube` files for UFI loggers.\n",
    "   - **Functions Used:** `BaseManufacturer.process_files()`, `CATSManufacturer.process_files()`, `UFIManufacturer.process_files()`\n",
    "\n",
    "7. **Save Processed Data**:\n",
    "   - **Description:** Saves processed data files in the outputs folder with appropriate filenames, ensuring consistency across different formats such as CSV, Parquet, and EDF.\n",
    "   - **Functions Used:** `save_data()`, `export_to_edf()`, `save_to_netcdf()`\n",
    "\n",
    "8. **Finalize and Save DataReader Object**:\n",
    "   - **Description:** Saves the state of the `DataReader` object, including all processed data and metadata, as a pickle file for easy retrieval and future processing.\n",
    "   - **Function Used:** `save_datareader_object()`\n",
    "\n",
    "### Classes and Their Roles:\n",
    "\n",
    "- **`DataReader` Class:**\n",
    "  - **Role:** Handles the overall reading, processing, and saving of deployment data files. It manages the deployment folder path, organizes data by logger and sensor, and interfaces with manufacturer-specific processing through the `BaseManufacturer` class.\n",
    "  - **Methods:** `read_files()`, `save_datareader_object()`, `organize_files_by_logger_id()`, `save_data()`, `export_to_edf()`, `save_to_netcdf()`, and more.\n",
    "\n",
    "- **`BaseManufacturer` Class:**\n",
    "  - **Role:** Acts as a template for manufacturer-specific processing classes. It defines common methods for loading custom mappings, renaming columns, and mapping data to sensors. Subclasses like `CATSManufacturer` and `UFIManufacturer` extend this base class to implement specific processing logic.\n",
    "  - **Methods:** `process_files()`, `load_custom_mapping()`, `rename_columns()`, `map_data_to_sensors()`, `parse_txt_for_intervals()`, and more.\n",
    "\n",
    "- **`metadata` Class (from `pyologger.load_data.metadata`):**\n",
    "  - **Role:** Provides methods for fetching and managing metadata related to deployments, loggers, animals, and datasets. It plays a crucial role in ensuring the correct organization and processing of data.\n",
    "  - **Methods:** `fetch_databases()`, `get_metadata()`, and others as needed for metadata handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your custom mapping file\n",
    "channel_mapping_path = os.path.join(root_dir, 'channel_mapping.json')\n",
    "\n",
    "datareader = DataReader(deployment_folder_path=data_dir)\n",
    "\n",
    "deployment_folder = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "edf_filename_template = os.path.join(datareader.files_info['deployment_folder_path'], 'outputs', 'edf_test_{sensor}.edf')\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=False, save_parq=False, save_edf=False, \n",
    "                          custom_mapping_path=channel_mapping_path, save_netcdf=True,\n",
    "                          edf_filename_template=edf_filename_template, edf_save_from='sensor_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datareader.sensor_info['pressure']['sampling_frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to DiveDB\n",
    "\n",
    "Make sure your local DiveDB servers are running. To do so:\n",
    "- Navigate to the DiveDB directory\n",
    "- Run the command: `make up`\n",
    "- Wait until all services are running (Django, Postgres, Jupyter)\n",
    "- Make sure you have run the latest migrations: `make migrate`\n",
    "- Make sure you imported the latest logger and animal databases: `make importmetadata`\n",
    "\n",
    "Then, you're ready to upload data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow Django to run with async unsafe to run outside of Django server\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "metadata = {\n",
    "    \"animal\": datareader.animal_info[\"Animal ID\"],\n",
    "    \"deployment\": datareader.deployment_info[\"Deployment ID\"],\n",
    "    \"recording\": datareader.deployment_info[\"Recording ID\"].split(\", \")[1]\n",
    "}\n",
    "\n",
    "data_uploader.upload_netcdf('./data/2024-01-16_oror-002a/outputs/deployment_data.nc', metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "df = duckpond.get_delta_data(    \n",
    "    signal_names=[\"sensor_data_ecg\", \"sensor_data_light\", \"sensor_data_temperature\", \"sensor_data_depth\"],\n",
    "    animal_ids=\"mian-001\", # Make sure this matches the animal ID you uploaded\n",
    "    frequency=100,\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "def export_concatenated_to_edf(concatenated_df, highest_sampling_frequency, latest_start_time, edf_filename_template):\n",
    "    \"\"\"\n",
    "    Exports the concatenated DataFrame to an EDF file.\n",
    "\n",
    "    Parameters:\n",
    "    - concatenated_df: The DataFrame containing concatenated data from all loggers.\n",
    "    - highest_sampling_frequency: The highest sampling frequency among the loggers.\n",
    "    - latest_start_time: The latest start time among the loggers.\n",
    "    - edf_filename_template: Template string for the EDF filename.\n",
    "                             The string should contain `{sensor}` to be replaced with 'ALL'.\n",
    "    \"\"\"\n",
    "    if concatenated_df is None or concatenated_df.empty:\n",
    "        print(\"No data available for export. Exiting.\")\n",
    "        return\n",
    "\n",
    "    ch_names = concatenated_df.columns.tolist()\n",
    "    sfreq = highest_sampling_frequency\n",
    "\n",
    "    # Check if there are any channels to process\n",
    "    if len(ch_names) == 0:\n",
    "        print(\"No valid channels found to export. Exiting.\")\n",
    "        return\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='misc')  # Adjust ch_types as necessary\n",
    "\n",
    "    # Convert datetime to (seconds, microseconds) tuple for the latest start time\n",
    "    meas_date = (int(latest_start_time.timestamp()), int((latest_start_time.timestamp() % 1) * 1e6))\n",
    "\n",
    "    # Create MNE RawArray\n",
    "    data = concatenated_df.values.T\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.set_meas_date(meas_date)\n",
    "\n",
    "    # Step 8: Define the EDF filename and save the EDF file\n",
    "    edf_filename = edf_filename_template.format(sensor='ALL')\n",
    "\n",
    "    print(f\"Saving EDF file as {edf_filename} with shape {data.shape}.\")\n",
    "\n",
    "    # Ensure that data is within the physical range EDF expects\n",
    "    raw.export(edf_filename, fmt='edf')\n",
    "\n",
    "    print(f\"EDF file saved as {edf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.sampling import *\n",
    "import pandas as pd\n",
    "def concatenate_logger_data(datareader):\n",
    "    \"\"\"\n",
    "    Concatenates data from all loggers stored in `datareader.data`.\n",
    "\n",
    "    Parameters:\n",
    "    - datareader: The DataReader object containing logger data in `datareader.data`.\n",
    "\n",
    "    Returns:\n",
    "    - concatenated_df: A pandas DataFrame with the concatenated data from all loggers.\n",
    "    - highest_sampling_frequency: The highest sampling frequency found among the loggers.\n",
    "    - latest_start_time: The latest start time among the loggers.\n",
    "    \"\"\"\n",
    "    logger_data_info = {}\n",
    "\n",
    "    # Step 1: Extract start time, end time, and sampling frequency for each logger\n",
    "    for logger_id, df in datareader.data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"Logger {logger_id} does not contain a valid DataFrame. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if 'datetime' not in df.columns:\n",
    "            print(f\"Logger {logger_id} does not have a 'datetime' column. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        start_time = df['datetime'].iloc[0]\n",
    "        end_time = df['datetime'].iloc[-1]\n",
    "        sampling_frequency = round(1 / df['datetime'].diff().dt.total_seconds().mean())\n",
    "\n",
    "        logger_data_info[logger_id] = {\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'sampling_frequency': sampling_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"Logger {logger_id}: start_time={start_time}, end_time={end_time}, sampling_frequency={sampling_frequency} Hz\")\n",
    "\n",
    "    if not logger_data_info:\n",
    "        print(\"No valid logger data found. Exiting.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Step 2: Determine the latest start time, earliest end time, and highest sampling frequency\n",
    "    latest_start_time = max(info['start_time'] for info in logger_data_info.values())\n",
    "    earliest_end_time = min(info['end_time'] for info in logger_data_info.values())\n",
    "    highest_sampling_frequency = max(info['sampling_frequency'] for info in logger_data_info.values())\n",
    "\n",
    "    print(f\"Latest start time: {latest_start_time}\")\n",
    "    print(f\"Earliest end time: {earliest_end_time}\")\n",
    "    print(f\"Highest sampling frequency: {highest_sampling_frequency} Hz\")\n",
    "\n",
    "    # Step 3: Initialize an empty DataFrame for concatenation\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Step 4: Crop dataframes, upsample as necessary, and concatenate\n",
    "    for logger_id, df in datareader.data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "\n",
    "        # Crop dataframe\n",
    "        df_cropped = df[(df['datetime'] >= latest_start_time) & (df['datetime'] <= earliest_end_time)]\n",
    "        print(f\"Logger {logger_id}: Cropped data from {len(df)} rows to {len(df_cropped)} rows.\")\n",
    "\n",
    "        # Determine upsampling factor\n",
    "        upsampling_factor = highest_sampling_frequency / logger_data_info[logger_id]['sampling_frequency']\n",
    "\n",
    "        if upsampling_factor > 1:\n",
    "            original_length = len(df_cropped)\n",
    "            df_cropped = df_cropped.set_index('datetime')\n",
    "\n",
    "            # Upsample each sensor column that is not \"extra\"\n",
    "            for column in df_cropped.columns:\n",
    "                sensor_info = None\n",
    "\n",
    "                # Search for the sensor type in `datareader.sensor_info`\n",
    "                for sensor_name, sensor_details in datareader.sensor_info.items():\n",
    "                    if column in sensor_details['channels']:\n",
    "                        sensor_info = sensor_details\n",
    "                        break\n",
    "\n",
    "                if not sensor_info:\n",
    "                    continue\n",
    "\n",
    "                sensor_type = sensor_info['metadata'][column]['sensor']\n",
    "                if sensor_type != 'extra':\n",
    "                    print(f\"Upsampling column {column} from logger {logger_id} by factor {upsampling_factor}.\")\n",
    "                    df_cropped[column] = upsample(df_cropped[column].values, int(upsampling_factor), original_length)\n",
    "\n",
    "            df_cropped = df_cropped.reset_index()\n",
    "\n",
    "        # Remove \"extra\" sensor columns and append to the concatenated DataFrame\n",
    "        columns_to_keep = []\n",
    "        for column in df_cropped.columns:\n",
    "            sensor_info = None\n",
    "\n",
    "#datareader.sensor_data['accelerometer']\n",
    "#datareader.files_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally look at first notes that have been read in\n",
    "\n",
    "datareader.event_data[0:5]\n",
    "#datareader.logger_data['CC-96']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data\n",
    "Make an interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.logger_info['UF-01']['channelinfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load color mappings\n",
    "color_mapping_path = os.path.join(root_dir, 'color_mappings.json')\n",
    "\n",
    "# Streamlit sidebar for time range selection\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Define the overlapping time range\n",
    "imu_df = data_pkl.logger_data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.logger_data[ephys_logger_to_use]\n",
    "overlap_start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "overlap_end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'sensor': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'exhalation_breath': {'sensor': 'depth', 'symbol': 'triangle-up', 'color': 'blue'},\n",
    "}\n",
    "\n",
    "# Plotting with the updated function call\n",
    "fig = plot_tag_data_interactive4(\n",
    "    data_pkl=data_pkl,\n",
    "    time_range=(overlap_start_time, overlap_end_time), \n",
    "    note_annotations=notes_to_plot, \n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=10\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = plot_tag_data_interactive(data_pkl, imu_channels=['depth', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'mx', 'my', 'mz'], \n",
    "                                ephys_channels=['ecg'], \n",
    "                                imu_logger=imu_logger_to_use, \n",
    "                                ephys_logger=ephys_logger_to_use, \n",
    "                                time_range=(overlap_start_time, overlap_end_time), \n",
    "                                note_annotations=notes_to_plot, \n",
    "                                color_mapping_path=color_mapping_path)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "\n",
    "def plot_tag_data_interactive(data_pkl, sensors=None, channels=None, time_range=None, note_annotations=None, \n",
    "                              color_mapping_path=None, target_sampling_rate=10):\n",
    "    \"\"\"\n",
    "    Function to plot tag data interactively using Plotly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pkl : object\n",
    "        The object containing the sensor data and metadata.\n",
    "    sensors : list, optional\n",
    "        List of sensors to plot. If None, plot all available sensors.\n",
    "    channels : dict, optional\n",
    "        Dictionary specifying the channels to plot for each sensor.\n",
    "        E.g., {'ecg': ['ecg'], 'depth': ['depth']}\n",
    "        If None, plot all channels for the specified sensors.\n",
    "    time_range : tuple, optional\n",
    "        Tuple specifying the start and end time for plotting.\n",
    "    note_annotations : dict, optional\n",
    "        Dictionary of annotations to plot. E.g., {'heartbeat_manual_ok': 'ecg'}\n",
    "    color_mapping_path : str, optional\n",
    "        Path to the JSON file containing the color mappings.\n",
    "    target_sampling_rate : int, optional\n",
    "        The target sampling rate to downsample the data for plotting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the color mapping\n",
    "    color_mapping = load_color_mapping(color_mapping_path) if color_mapping_path else {}\n",
    "\n",
    "    # Determine the sensors to plot\n",
    "    if sensors is None:\n",
    "        sensors = list(data_pkl.sensor_data.keys())\n",
    "\n",
    "    # Set up the figure\n",
    "    fig = make_subplots(rows=len(sensors), cols=1, shared_xaxes=True, vertical_spacing=0.03)\n",
    "    \n",
    "    row_counter = 1\n",
    "\n",
    "    for sensor in sensors:\n",
    "        sensor_df = data_pkl.sensor_data[sensor]\n",
    "        sensor_info = data_pkl.sensor_info[sensor]\n",
    "\n",
    "        # Determine the channels to plot for the current sensor\n",
    "        if channels is None or sensor not in channels:\n",
    "            sensor_channels = sensor_info['channels']\n",
    "        else:\n",
    "            sensor_channels = channels[sensor]\n",
    "\n",
    "        # Filter data to the time range\n",
    "        if time_range:\n",
    "            start_time, end_time = time_range\n",
    "            sensor_df_filtered = sensor_df[(sensor_df['datetime'] >= start_time) & (sensor_df['datetime'] <= end_time)]\n",
    "        else:\n",
    "            sensor_df_filtered = sensor_df\n",
    "\n",
    "        # Calculate original sampling rate\n",
    "        original_fs = 1 / sensor_df_filtered['datetime'].diff().dt.total_seconds().mean()\n",
    "\n",
    "        # Downsample the data\n",
    "        def downsample(df, original_fs, target_fs):\n",
    "            if target_fs >= original_fs:\n",
    "                return df\n",
    "            conversion_factor = int(original_fs / target_fs)\n",
    "            return df.iloc[::conversion_factor, :]\n",
    "\n",
    "        sensor_df_filtered = downsample(sensor_df_filtered, original_fs, target_sampling_rate)\n",
    "\n",
    "        # Plot each channel\n",
    "        for channel in sensor_channels:\n",
    "            if channel in sensor_df_filtered.columns:\n",
    "                x_data = sensor_df_filtered['datetime']\n",
    "                y_data = sensor_df_filtered[channel]\n",
    "\n",
    "                original_name = sensor_info['metadata'][channel]['original_name']\n",
    "                unit = sensor_info['metadata'][channel]['unit']\n",
    "                y_label = f\"{original_name} [{unit}]\"\n",
    "\n",
    "                color = color_mapping.get(original_name, generate_random_color())\n",
    "                color_mapping[original_name] = color\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=x_data,\n",
    "                    y=y_data,\n",
    "                    mode='lines',\n",
    "                    name=y_label,\n",
    "                    line=dict(color=color)\n",
    "                ), row=row_counter, col=1)\n",
    "\n",
    "        # Handle annotations for this sensor\n",
    "        if note_annotations:\n",
    "            for note_type, note_channel in note_annotations.items():\n",
    "                if note_channel in sensor_df_filtered.columns:\n",
    "                    filtered_notes = data_pkl.notes_df[data_pkl.notes_df['key'] == note_type]\n",
    "                    if not filtered_notes.empty:\n",
    "                        for dt in filtered_notes['datetime']:\n",
    "                            fig.add_trace(go.Scatter(\n",
    "                                x=[dt, dt],\n",
    "                                y=[sensor_df_filtered[note_channel].min(), sensor_df_filtered[note_channel].max()],\n",
    "                                mode='lines',\n",
    "                                line=dict(color=color_mapping.get(note_type, 'rgba(128, 128, 128, 0.5)'), width=1, dash='dot'),\n",
    "                                showlegend=False\n",
    "                            ), row=row_counter, col=1)\n",
    "\n",
    "        fig.update_yaxes(title_text=sensor, row=row_counter, col=1)\n",
    "        row_counter += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=200 * len(sensors),\n",
    "        width=1200,\n",
    "        hovermode=\"x unified\",  # Enables the vertical hover line across subplots\n",
    "        title_text=f\"{data_pkl.selected_deployment['Deployment Name']}\",\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",  # Horizontal legend\n",
    "            xanchor='center',  # Anchor the legend horizontally at the center\n",
    "            yanchor='top'   # Anchor the legend vertically at the top of the legend box\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Datetime\", row=row_counter-1, col=1)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'depth', 'accelerometer'],  # Specify which sensors to plot\n",
    "    channels={'ecg': ['ecg'], 'depth': ['depth']},  # Optionally, specify channels for each sensor\n",
    "    time_range=(overlap_start_time, overlap_end_time),  # Optionally specify a time range\n",
    "    note_annotations={'heartbeat_manual_ok': 'ecg', 'exhalation_breath': 'depth'},  # Optionally specify annotations\n",
    "    color_mapping_path=color_mapping_path  # Optionally specify a color mapping path\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting again (this takes longer without subplots but allows you to track the time across all plots in a grid)\n",
    "fig = plot_tag_data_interactive2(data_pkl, imu_channels=['depth', 'accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ', 'magX', 'magY', 'magZ'], \n",
    "                                ephys_channels=['ecg'], \n",
    "                                imu_logger=imu_logger_to_use, \n",
    "                                ephys_logger=ephys_logger_to_use, \n",
    "                                time_range=(overlap_start_time, overlap_end_time), \n",
    "                                note_annotations=notes_to_plot, \n",
    "                                color_mapping_path=color_mapping_path)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Function to apply a low-pass filter to extract the static component (gravity)\n",
    "def low_pass_filter(data, cutoff, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to calculate ODBA\n",
    "def calculate_odba(accX, accY, accZ, cutoff=0.1, fs=10):\n",
    "    # Apply low-pass filter to get the static acceleration\n",
    "    accX_static = low_pass_filter(accX, cutoff, fs)\n",
    "    accY_static = low_pass_filter(accY, cutoff, fs)\n",
    "    accZ_static = low_pass_filter(accZ, cutoff, fs)\n",
    "\n",
    "    # Subtract the static component to get the dynamic acceleration\n",
    "    accX_dynamic = accX - accX_static\n",
    "    accY_dynamic = accY - accY_static\n",
    "    accZ_dynamic = accZ - accZ_static\n",
    "\n",
    "    # Calculate ODBA\n",
    "    odba = np.abs(accX_dynamic) + np.abs(accY_dynamic) + np.abs(accZ_dynamic)\n",
    "    \n",
    "    return odba\n",
    "\n",
    "# Example usage with your data\n",
    "accX = data_pkl.data['CC-96']['accX_adjusted'].values\n",
    "accY = data_pkl.data['CC-96']['accY_adjusted'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ_adjusted'].values\n",
    "\n",
    "odba = calculate_odba(accX, accY, accZ)\n",
    "\n",
    "data_pkl.data['CC-96']['odba'] = odba\n",
    "\n",
    "imu_channels_to_plot = ['depth', 'accX', 'accY', 'accZ', 'odba', 'pitch_deg', 'roll_deg', 'heading_deg']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'exhalation_breath': 'depth'\n",
    "}\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=1, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, note_annotations= notes_to_plot,\n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
