{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Import pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.utils.json_manager import ConfigManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata\n",
    "\n",
    "Load in metadata stored in Notion databases. Alternatively, load in your own metadata in separate dataframes for deployments, loggers, recordings, animals, and datasets. See examples here in the `metadata_snapshot.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = Metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save database variables\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dataset_db = metadata.get_metadata(\"dataset_DB\")\n",
    "procedure_db = metadata.get_metadata(\"procedure_DB\")\n",
    "observation_db = metadata.get_metadata(\"observation_DB\")\n",
    "collaborator_db = metadata.get_metadata(\"collaborator_DB\")\n",
    "location_db = metadata.get_metadata(\"location_DB\")\n",
    "montage_db = metadata.get_metadata(\"montage_DB\")\n",
    "sensor_db = metadata.get_metadata(\"sensor_DB\")\n",
    "attachment_db = metadata.get_metadata(\"attachment_DB\")\n",
    "originalchannel_db = metadata.get_metadata(\"originalchannel_DB\")\n",
    "standardizedchannel_db = metadata.get_metadata(\"standardizedchannel_DB\")\n",
    "derivedsignal_db = metadata.get_metadata(\"derivedsignal_DB\")\n",
    "derivedchannel_db = metadata.get_metadata(\"derivedchannel_DB\")\n",
    "\n",
    "# Get the relations map\n",
    "relations_map = metadata.relations_map\n",
    "\n",
    "# Define the path to save the relations map\n",
    "relations_map_path = os.path.join(config['paths']['local_repo_path'], 'relations_map.json')\n",
    "\n",
    "# Save the relations map as a JSON file\n",
    "with open(relations_map_path, 'w') as file:\n",
    "    json.dump(relations_map, file, indent=4)\n",
    "\n",
    "print(f\"Relations map saved at: {relations_map_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizedchannel_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save metadata snapshot as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Save metadata snapshot as a pickle file\n",
    "\n",
    "# Combine DataFrames into a dictionary\n",
    "metadata_databases = {\n",
    "    'deployment_db': deployment_db,\n",
    "    'logger_db': logger_db,\n",
    "    'recording_db': recording_db,\n",
    "    'animal_db': animal_db,\n",
    "    'dataset_db': dataset_db,\n",
    "    'procedure_db': procedure_db,\n",
    "    'observation_db': observation_db,\n",
    "    'collaborator_db': collaborator_db,\n",
    "    'location_db': location_db,\n",
    "    'montage_db': montage_db,\n",
    "    'sensor_db': sensor_db,\n",
    "    'attachment_db': attachment_db,\n",
    "    'originalchannel_db': originalchannel_db,\n",
    "    'standardizedchannel_db': standardizedchannel_db,\n",
    "    'derivedsignal_db': derivedsignal_db,\n",
    "    'derivedchannel_db': derivedchannel_db\n",
    "}\n",
    "\n",
    "# Save metadata snapshot as a pickle file\n",
    "metadata_pickle_path = os.path.join(data_dir, \"00_Metadata/metadata_snapshot.pkl\")\n",
    "with open(metadata_pickle_path, \"wb\") as file:\n",
    "    pickle.dump(metadata_databases, file)\n",
    "\n",
    "print(f\"Metadata snapshot saved at: {metadata_pickle_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset folder\n",
    "dataset_folder = select_folder(data_dir, \"Select a dataset folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = select_folder(dataset_folder, \"Select a deployment folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deployment_id and animal_id from the folder name\n",
    "match = re.match(r\"(\\d{4}-\\d{2}-\\d{2}_[a-z]{4}-\\d{3})\", os.path.basename(deployment_folder), re.IGNORECASE)\n",
    "if match:\n",
    "    deployment_id = match.group(1)  # Extract YYYY-MM-DD_animalID\n",
    "    animal_id = deployment_id.split(\"_\")[1]  # Extract animal ID\n",
    "    print(f\"✅ Extracted deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "else:\n",
    "    raise ValueError(f\"❌ Unable to extract deployment ID from folder: {deployment_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "### Steps for Processing Deployment Data: \n",
    "The following steps follow the [`datareader`](../pyologger/load_data/datareader.py) class and specifically the `read_files()` method.\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Prompts the user to select a deployment folder to initiate the data reading process. The folder name can include any suffix after the Deployment ID. The function checks for potential conflicts and halts the process if multiple matching folders are found.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Begins the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieves essential data from the metadata database, including logger and animal information.\n",
    "   - **Functions Used:** `metadata.fetch_databases()`, `get_animal_info()`, `get_dataset_info()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Groups files by logger ID for processing.\n",
    "   - **Function Used:** `organize_files_by_logger_id()` (within `read_files()`)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verifies if the output folder already contains processed files for each logger. Skips reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process Manufacturer-Specific Files**:\n",
    "   - **Description:** Depending on the logger's manufacturer, different processing methods are applied. The `BaseManufacturer` class and its subclasses (`CATSManufacturer`, `UFIManufacturer`) handle the specifics, such as processing `.txt` files for CATS loggers or `.ube` files for UFI loggers.\n",
    "   - **Functions Used:** `BaseManufacturer.process_files()`, `CATSManufacturer.process_files()`, `UFIManufacturer.process_files()`\n",
    "\n",
    "7. **Save Processed Data**:\n",
    "   - **Description:** Saves processed data files in the outputs folder with appropriate filenames, ensuring consistency across different formats such as CSV, Parquet, and EDF.\n",
    "   - **Functions Used:** `save_data()`, `export_to_edf()`, `save_to_netcdf()`\n",
    "\n",
    "8. **Finalize and Save DataReader Object**:\n",
    "   - **Description:** Saves the state of the `DataReader` object, including all processed data and metadata, as a pickle file for easy retrieval and future processing.\n",
    "   - **Function Used:** `save_datareader_object()`\n",
    "\n",
    "### Classes and Their Roles:\n",
    "\n",
    "- **`DataReader` Class:**\n",
    "  - **Role:** Handles the overall reading, processing, and saving of deployment data files. It manages the deployment folder path, organizes data by logger and sensor, and interfaces with manufacturer-specific processing through the `BaseManufacturer` class.\n",
    "  - **Methods:** `read_files()`, `save_datareader_object()`, `organize_files_by_logger_id()`, `save_data()`, `export_to_edf()`, `save_to_netcdf()`, and more.\n",
    "\n",
    "- **`BaseManufacturer` Class:**\n",
    "  - **Role:** Acts as a template for manufacturer-specific processing classes. It defines common methods for loading custom mappings, renaming columns, and mapping data to sensors. Subclasses like `CATSManufacturer` and `UFIManufacturer` extend this base class to implement specific processing logic.\n",
    "  - **Methods:** `process_files()`, `load_custom_mapping()`, `rename_columns()`, `map_data_to_sensors()`, `parse_txt_for_intervals()`, and more.\n",
    "\n",
    "- **`metadata` Class (from `pyologger.load_data.metadata`):**\n",
    "  - **Role:** Provides methods for fetching and managing metadata related to deployments, loggers, animals, and datasets. It plays a crucial role in ensuring the correct organization and processing of data.\n",
    "  - **Methods:** `fetch_databases()`, `get_metadata()`, and others as needed for metadata handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print extracted values for debugging\n",
    "print(f\"🐳 Deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "\n",
    "deployment_info, loggers_used = metadata.extract_essential_metadata(deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "overwrite = False\n",
    "\n",
    "if overwrite: # If you store your metadata differently, you can set this manually:\n",
    "    # Deployment ID and Animal ID - this is important because it sets the start date and animal ID\n",
    "    # Your dataset ID is the folder name that this deployment folder is in\n",
    "    deployment_id = \"2019-11-08_apfo-001\"\n",
    "    animal_id = \"apfo-001\"\n",
    "    print(f\"🔍 Manually setting essential metadata for Deployment ID: {deployment_id}\")\n",
    "\n",
    "    # Manually setting deployment metadata\n",
    "    deployment_info = {\n",
    "        \"Deployment Date\": \"2019-11-08\",\n",
    "        \"Deployment Latitude\": -77.858933,\n",
    "        \"Deployment Longitude\": 166.5139,\n",
    "        \"Time Zone\": \"Antarctica/McMurdo\"\n",
    "    }\n",
    "    print(f\"📍 Deployment Metadata: {deployment_info}\")\n",
    "\n",
    "    # Manually setting loggers used with Montage ID inside each entry\n",
    "    loggers_used = [\n",
    "        {\"Logger ID\": \"CC-35\", \"Manufacturer\": \"CATS\", \"Montage ID\": \"cats-penguin-video-montage_V1\"}\n",
    "    ]\n",
    "    print(f\"📟 Loggers Used: {loggers_used}\")\n",
    "\n",
    "    # No separate montage_id list anymore, since it's stored in loggers_used\n",
    "\n",
    "def list_available_timezones():\n",
    "    \"\"\"\n",
    "    Prints all available time zones in pytz.\n",
    "    \"\"\"\n",
    "    timezones = pytz.all_timezones\n",
    "    print(\"\\n🌍 Available Time Zones in pytz:\\n\")\n",
    "    for tz in timezones:\n",
    "        print(tz)\n",
    "\n",
    "# List all available time zones\n",
    "list_available_timezones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize DataReader with dataset folder, deployment ID, and optional data subfolder\n",
    "data_pkl = DataReader(dataset_folder=dataset_folder, deployment_id=deployment_id, data_subfolder=\"01_raw-data\", montage_path=montage_path)\n",
    "# Step 5: Initialize config manager\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)\n",
    "config_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data import pending.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager.export_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass it to DataReader\n",
    "data_pkl.read_files(\n",
    "    deployment_info= deployment_info,\n",
    "    loggers_used= loggers_used,\n",
    "    save_parq= False,\n",
    "    overwrite= False,\n",
    "    save_netcdf= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info['light']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Step 8: Update processing step\n",
    "config_manager.add_to_config(\"current_processing_step\", \"Processing Step 00: Data imported.\")\n",
    "\n",
    "# Step 9: Open NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, \"outputs\", f'{deployment_id}_00_processed.nc')\n",
    "if os.path.exists(netcdf_path):\n",
    "    data = xr.open_dataset(netcdf_path)\n",
    "    print(f\"📊 NetCDF file loaded: {netcdf_path}\")\n",
    "else:\n",
    "    print(f\"⚠ NetCDF file not found at {netcdf_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "\n",
    "# Define the path to the EDF file\n",
    "edf_file_path = os.path.join(deployment_folder, '01_raw-data', '2019-10-25_mian-001a_NL-02_001.edf')\n",
    "\n",
    "# Read the EDF file\n",
    "edf_reader = pyedflib.EdfReader(edf_file_path)\n",
    "\n",
    "# Print some basic information about the EDF file\n",
    "print(f\"Number of signals: {edf_reader.signals_in_file}\")\n",
    "print(f\"Signal labels: {edf_reader.getSignalLabels()}\")\n",
    "print(f\"Signal frequencies: {edf_reader.getSampleFrequencies()}\")\n",
    "\n",
    "# Close the EDF reader\n",
    "edf_reader.close()\n",
    "\n",
    "# Count the number of EXG channels (start with 'Ch')\n",
    "exg_channels = [label for label in current_order if label.startswith('Ch')]\n",
    "num_exg_channels = len(exg_channels)\n",
    "\n",
    "# Count the number of Acc channels\n",
    "acc_channels = [label for label in current_order if label.startswith('Acc')]\n",
    "num_acc_channels = len(acc_channels)\n",
    "\n",
    "# Count the number of Mag channels\n",
    "mag_channels = [label for label in current_order if label.startswith('Mag')]\n",
    "num_mag_channels = len(mag_channels)\n",
    "\n",
    "# Count the number of Gyr channels\n",
    "gyr_channels = [label for label in current_order if label.startswith('Gyr')]\n",
    "num_gyr_channels = len(gyr_channels)\n",
    "\n",
    "# Check for other signals\n",
    "other_signals = [label for label in current_order if label not in exg_channels + acc_channels + mag_channels + gyr_channels]\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Number of EXG channels: {num_exg_channels}\")\n",
    "print(f\"Number of Acc channels: {num_acc_channels}\")\n",
    "print(f\"Number of Mag channels: {num_mag_channels}\")\n",
    "print(f\"Number of Gyr channels: {num_gyr_channels}\")\n",
    "print(f\"Other signals: {other_signals}\")\n",
    "\n",
    "check_path = '/Volumes/WORK-SSD/Datasets/Unpublished/mian-juv-nese_sleep_lml-ano_JKB/2019-10-25_mian-001a/01_raw-data/2019-10-25_mian-001a_NL-02_001.edf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Get timezone\n",
    "timezone = data_pkl.deployment_info.get(\"Time Zone\", \"UTC\")\n",
    "\n",
    "# Load time settings\n",
    "time_settings = config_manager.get_from_config(\n",
    "    [\"overlap_start_time\", \"overlap_end_time\", \"zoom_window_start_time\", \"zoom_window_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "if time_settings:\n",
    "    print(\"Time settings present.\")\n",
    "# If any required time settings are missing, compute and update them\n",
    "if not any(v is None for v in time_settings.values()):\n",
    "    print(\"Time settings not empty.\")\n",
    "else:\n",
    "    print(\"Adding timestamps to config.\")\n",
    "    zoom_time_window = 5  # minutes\n",
    "\n",
    "    # Extract start and end times for all sensors\n",
    "    start_times = [df['datetime'].min() for df in data_pkl.sensor_data.values()]\n",
    "    end_times = [df['datetime'].max() for df in data_pkl.sensor_data.values()]\n",
    "\n",
    "    # Compute common start, end, and zoom window\n",
    "    overlap_start_time = max(start_times)\n",
    "    overlap_end_time = min(end_times)\n",
    "    midpoint = overlap_start_time + (overlap_end_time - overlap_start_time) / 2\n",
    "    zoom_window_start, zoom_window_end = midpoint - timedelta(minutes=zoom_time_window / 2), midpoint + timedelta(minutes=zoom_time_window / 2)\n",
    "\n",
    "    # Update settings\n",
    "    time_settings = {\n",
    "        \"overlap_start_time\": str(overlap_start_time),\n",
    "        \"overlap_end_time\": str(overlap_end_time),\n",
    "        \"zoom_window_start_time\": str(zoom_window_start),\n",
    "        \"zoom_window_end_time\": str(zoom_window_end),\n",
    "    }\n",
    "    config_manager.add_to_config(entries=time_settings, section=\"settings\")\n",
    "\n",
    "if any(v is None for v in time_settings.values()):\n",
    "    print(\"YES\")\n",
    "time_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = data_pkl.sensor_data[\"pressure\"][\"datetime\"].min()\n",
    "end = data_pkl.sensor_data[\"pressure\"][\"datetime\"].max()\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    time_range=(start, end),\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected start and end times exist in the config file\n",
    "truncate_times = config_manager.get_from_config(\n",
    "    [\"selected_start_time\", \"selected_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "truncate_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not any(v is None for v in truncate_times.values()):\n",
    "    print(\"Truncating with provided cropping times.\")\n",
    "    # Update overlap window with selected range\n",
    "    OVERLAP_START_TIME = pd.Timestamp(truncate_times['selected_start_time']).tz_convert(timezone)\n",
    "    OVERLAP_END_TIME = pd.Timestamp(truncate_times['selected_end_time']).tz_convert(timezone)\n",
    "\n",
    "    # Truncate sensor data\n",
    "    for sensor, df in data_pkl.sensor_data.items():\n",
    "        # Truncate based on selected time range\n",
    "        truncated_df = df[(df.iloc[:, 0] >= OVERLAP_START_TIME) & (df.iloc[:, 0] <= OVERLAP_END_TIME)].copy()\n",
    "        data_pkl.sensor_data[sensor] = truncated_df  # Save truncated version to new variable\n",
    "\n",
    "    # Recalculate Zoom Window (5-minute window in the middle)\n",
    "    midpoint = OVERLAP_START_TIME + (OVERLAP_END_TIME - OVERLAP_START_TIME) / 2\n",
    "    ZOOM_WINDOW_START_TIME = midpoint - timedelta(minutes=2.5)\n",
    "    ZOOM_WINDOW_END_TIME = midpoint + timedelta(minutes=2.5)\n",
    "\n",
    "    # Save new time settings\n",
    "    time_settings_update = {\n",
    "        \"overlap_start_time\": str(OVERLAP_START_TIME),\n",
    "        \"overlap_end_time\": str(OVERLAP_END_TIME),\n",
    "        \"zoom_window_start_time\": str(ZOOM_WINDOW_START_TIME),\n",
    "        \"zoom_window_end_time\": str(ZOOM_WINDOW_END_TIME)\n",
    "    }\n",
    "    config_manager.add_to_config(entries=time_settings_update, section=\"settings\")\n",
    "\n",
    "    pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "    with open(pkl_path, \"wb\") as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    zoom_start_time= ZOOM_WINDOW_START_TIME,\n",
    "    zoom_end_time= ZOOM_WINDOW_END_TIME,\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
