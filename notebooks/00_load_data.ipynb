{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pyologger` data processing pipeline with `DiveDB`\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deployment metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import pickle\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import plot_tag_data_interactive, plot_tag_data, load_color_mapping, save_color_mapping, plot_depth_correction\n",
    "from pyologger.calibrate_data.calibrate_acc_mag import *\n",
    "\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "#import netcdf4\n",
    "\n",
    "# Load the NetCDF file\n",
    "file_path = f\"{data_dir}/2004001_TrackTDR_RawCurated.nc\"\n",
    "dataset = xr.open_dataset(file_path)\n",
    "\n",
    "# Print the dataset information\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the deployment dataframe.\n",
    "dep_db_sorted = dep_db.sort_values('Rec Date')\n",
    "dep_db_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files in deployment folder\n",
    "\n",
    "Steps for Processing Deployment Data:\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Asks the user for input to select a deployment folder to kick off the data reading process. In your folder name, you can have any suffix after Deployment ID. It will check and stop if there are two that fit.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Starts the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieve necessary data from the metadata database, including logger information.\n",
    "   - **Function Used:** `metadata.fetch_databases()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Group files by logger ID for processing.\n",
    "   - **Function Used:** `read_files()` (This is the main function)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verify if the outputs folder already contains processed files for each logger. Skip reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process UBE Files**:\n",
    "   - **Description:** For each UFI logger with UBE files, process and save the data.\n",
    "   - **Function Used:** `process_ube_file()`\n",
    "\n",
    "7. **Process CSV Files**:\n",
    "   - **Description:** For each logger with multiple CSV files, concatenate them, and save the combined data.\n",
    "   - **Function Used:** `concatenate_and_save_csvs()`\n",
    "\n",
    "8. **Final Outputs**:\n",
    "   - **Description:** Ensure all processed data is saved in the outputs folder with appropriate filenames.\n",
    "   - **Functions Used:** `save_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the metadata and dep_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(dep_db, data_dir)\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=True, save_parq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally look at first notes that have been read in\n",
    "#datareader.selected_deployment['Time Zone']\n",
    "#datareader.info['UF-01']\n",
    "datareader.notes_df[0:5]\n",
    "#datareader.data['CC-96']\n",
    "#datareader.data['UF-01']\n",
    "#datareader.metadata['channelnames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")\n",
    "\n",
    "data_pkl.info['CC-96']['datetime_metadata']['fs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data\n",
    "Make an interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load color mappings\n",
    "color_mapping_path = os.path.join(root_dir, 'color_mappings.json')\n",
    "\n",
    "# Streamlit sidebar for time range selection\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "overlap_start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "overlap_end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': 'ecg',\n",
    "    'exhalation_breath': 'depth'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig = plot_tag_data_interactive(data_pkl, imu_channels=['depth', 'corrdepth', 'accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ', 'magX', 'magY', 'magZ'], \n",
    "                                ephys_channels=['ecg'], \n",
    "                                imu_logger=imu_logger_to_use, \n",
    "                                ephys_logger=ephys_logger_to_use, \n",
    "                                time_range=(overlap_start_time, overlap_end_time), \n",
    "                                note_annotations=notes_to_plot, \n",
    "                                color_mapping_path=color_mapping_path)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dives\n",
    "Involves a zero offset correction with `zoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Load the depth and temperature data\n",
    "depth_data = data_pkl.data['CC-96']['depth'].values\n",
    "temp_data = data_pkl.data['CC-96'].get('tempIMU')\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "datetime_data = pd.to_datetime(data_pkl.data['CC-96']['datetime'])\n",
    "\n",
    "# Sidebar for parameters\n",
    "threshold = 0.1 # meters\n",
    "min_duration = 30 # seconds\n",
    "depth_threshold = 5 # meters\n",
    "apply_temp_correction = False\n",
    "\n",
    "# Dive detection parameters\n",
    "min_depth_threshold = 1.0\n",
    "dive_duration_threshold = 10\n",
    "smoothing_window = value=5\n",
    "\n",
    "# Process depth data\n",
    "first_derivative, downsampled_depth = smooth_downsample_derivative(depth_data, original_sampling_rate=sampling_rate, downsampled_sampling_rate=1)\n",
    "\n",
    "# Detect flat chunks (potential surface intervals)\n",
    "flat_chunks = detect_flat_chunks(\n",
    "    depth=downsampled_depth, \n",
    "    datetime_data=datetime_data[::int(sampling_rate)],  # Adjust datetime data to match downsampled depth\n",
    "    first_derivative=first_derivative, \n",
    "    threshold=threshold, \n",
    "    min_duration=min_duration, \n",
    "    depth_threshold=depth_threshold, \n",
    "    original_sampling_rate=400, \n",
    "    downsampled_sampling_rate=1\n",
    ")\n",
    "num_flat_chunks = len(flat_chunks)\n",
    "print(f\"Number of potential surface intervals detected: {num_flat_chunks}\")\n",
    "\n",
    "# Apply zero offset correction\n",
    "corrected_depth_temp, corrected_depth_no_temp, depth_correction = apply_zero_offset_correction(\n",
    "    depth=downsampled_depth, \n",
    "    temp=temp_data.values if temp_data is not None else None, \n",
    "    flat_chunks=flat_chunks\n",
    ")\n",
    "\n",
    "# Upsample and adjust the corrected depths to match original sampling rate\n",
    "upsampling_factor = int(sampling_rate / 1)\n",
    "repeated_corrected_depth_temp = upsample(corrected_depth_temp, upsampling_factor, len(depth_data))\n",
    "repeated_corrected_depth_no_temp = upsample(corrected_depth_no_temp, upsampling_factor, len(depth_data))\n",
    "\n",
    "# Detect dives in the corrected depth data\n",
    "dives = find_dives(\n",
    "    depth_series=repeated_corrected_depth_no_temp,\n",
    "    datetime_data=datetime_data,\n",
    "    min_depth_threshold=min_depth_threshold,\n",
    "    sampling_rate=sampling_rate,\n",
    "    duration_threshold=dive_duration_threshold,\n",
    "    smoothing_window=smoothing_window\n",
    ")\n",
    "num_dives = len(dives)\n",
    "print(f\"Number of dives detected: {num_dives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot_depth_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "dec_factor = int(upsampling_factor)\n",
    "\n",
    "fig = plot_depth_correction(datetime_data, dec_factor, depth_data, first_derivative, \n",
    "                            repeated_corrected_depth_temp, repeated_corrected_depth_no_temp, \n",
    "                            depth_correction, dives, flat_chunks, temp_data, apply_temp_correction)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corrected depth back to the data structure\n",
    "if apply_temp_correction:\n",
    "    data_pkl.data['CC-96']['corrdepth'] = repeated_corrected_depth_temp\n",
    "else:\n",
    "    data_pkl.data['CC-96']['corrdepth'] = repeated_corrected_depth_no_temp\n",
    "\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate ACC & MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX'].values\n",
    "accY = data_pkl.data['CC-96']['accY'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ'].values\n",
    "magX = data_pkl.data['CC-96']['magX'].values\n",
    "magY = data_pkl.data['CC-96']['magY'].values\n",
    "magZ = data_pkl.data['CC-96']['magZ'].values\n",
    "\n",
    "# Combine the accelerometer and magnetometer data into nx3 matrices\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "mag_data = np.vstack((magX, magY, magZ)).T\n",
    "\n",
    "# Get the sampling rate from the data structure\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "# Call the check_AM function\n",
    "AMcheck = compute_field_intensity_and_inclination(acc_data, mag_data, sampling_rate)\n",
    "\n",
    "# Access the field intensity and inclination angle\n",
    "field_intensity_acc = AMcheck['field_intensity'][:, 0]  # Field intensity of accelerometer data\n",
    "field_intensity_mag = AMcheck['field_intensity'][:, 1]  # Field intensity of magnetometer data\n",
    "inclination_angle = AMcheck['inclination_angle']\n",
    "\n",
    "# Print results\n",
    "print(\"Field Intensity:\\n\", field_intensity_acc)\n",
    "print(\"Field Intensity:\\n\", field_intensity_mag)\n",
    "print(\"Inclination Angle (degrees):\\n\", inclination_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new fields to data_pkl.data['CC-96']\n",
    "data_pkl.data['CC-96']['field_intensity_acc'] = field_intensity_acc\n",
    "data_pkl.data['CC-96']['field_intensity_mag'] = field_intensity_mag\n",
    "data_pkl.data['CC-96']['inclination_angle'] = inclination_angle\n",
    "\n",
    "# Prepare channels to plot\n",
    "imu_channels_to_plot = ['accX', 'accY', 'accZ', 'field_intensity_acc', 'field_intensity_mag', 'inclination_angle']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, \n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the fix_offset_3d function to adjust accelerometer data\n",
    "result = estimate_offset_triaxial(acc_data)\n",
    "\n",
    "# Extract the adjusted data and calibration info\n",
    "adjusted_data_acc = result['X']\n",
    "calibration_info_acc = result['G']\n",
    "\n",
    "print(\"Adjusted Data:\\n\", adjusted_data_acc)\n",
    "print(\"Calibration Info:\\n\", calibration_info_acc)\n",
    "\n",
    "data_pkl.data['CC-96']['accX_adjusted'] = adjusted_data_acc[:, 0]\n",
    "data_pkl.data['CC-96']['accY_adjusted'] = adjusted_data_acc[:, 1]\n",
    "data_pkl.data['CC-96']['accZ_adjusted'] = adjusted_data_acc[:, 2]\n",
    "data_pkl.info['CC-96']['calibration_info'] = {}\n",
    "data_pkl.info['CC-96']['calibration_info'] = calibration_info_acc\n",
    "\n",
    "# Apply the fix_offset_3d function to adjust magnetometer data\n",
    "result = estimate_offset_triaxial(mag_data)\n",
    "\n",
    "# Extract the adjusted data and calibration info\n",
    "adjusted_data_mag = result['X']\n",
    "calibration_info_mag = result['G']\n",
    "\n",
    "print(\"Adjusted Data:\\n\", adjusted_data_mag)\n",
    "print(\"Calibration Info:\\n\", calibration_info_mag)\n",
    "\n",
    "data_pkl.data['CC-96']['magX_adjusted'] = adjusted_data_mag[:, 0]\n",
    "data_pkl.data['CC-96']['magY_adjusted'] = adjusted_data_mag[:, 1]\n",
    "data_pkl.data['CC-96']['magZ_adjusted'] = adjusted_data_mag[:, 2]\n",
    "data_pkl.info['CC-96']['calibration_info'] = {}\n",
    "data_pkl.info['CC-96']['calibration_info'] = calibration_info_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run check AM. \n",
    "Re-run the field intensity and inclination angle calculations to see if now the values are closer to expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX_adjusted'].values\n",
    "accY = data_pkl.data['CC-96']['accY_adjusted'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ_adjusted'].values\n",
    "magX = data_pkl.data['CC-96']['magX_adjusted'].values\n",
    "magY = data_pkl.data['CC-96']['magY_adjusted'].values\n",
    "magZ = data_pkl.data['CC-96']['magZ_adjusted'].values\n",
    "\n",
    "# Combine the accelerometer and magnetometer data into nx3 matrices\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "mag_data = np.vstack((magX, magY, magZ)).T\n",
    "\n",
    "# Get the sampling rate from the data structure\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "# Call the check_AM function\n",
    "AMcheck2 = compute_field_intensity_and_inclination(acc_data, mag_data, sampling_rate)\n",
    "\n",
    "# Access the field intensity and inclination angle\n",
    "field_intensity_acc2 = AMcheck2['field_intensity'][:, 0]  # Field intensity of accelerometer data\n",
    "field_intensity_mag2 = AMcheck2['field_intensity'][:, 1]  # Field intensity of magnetometer data\n",
    "inclination_angle2 = AMcheck2['inclination_angle']\n",
    "\n",
    "# Print results\n",
    "print(\"Field Intensity:\\n\", field_intensity_acc2)\n",
    "print(\"Field Intensity:\\n\", field_intensity_mag2)\n",
    "print(\"Inclination Angle (degrees):\\n\", inclination_angle2)\n",
    "\n",
    "# Save new fields to data_pkl.data['CC-96']\n",
    "data_pkl.data['CC-96']['field_intensity_acc2'] = field_intensity_acc2\n",
    "data_pkl.data['CC-96']['field_intensity_mag2'] = field_intensity_mag2\n",
    "data_pkl.data['CC-96']['inclination_angle2'] = inclination_angle2\n",
    "\n",
    "# Example usage in Streamlit\n",
    "imu_channels_to_plot = ['accX', 'accY', 'accZ', 'field_intensity_acc2', 'field_intensity_mag2', 'inclination_angle2']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, \n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix inclination angle\n",
    "Here is where you would fix the axes of triaxial sensor data (if the sensor axis differs from the tag axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag to animal frame\n",
    "### Start with an orientation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTAB Matrix Example\n",
    "OTAB = np.array([\n",
    "    [0, 0, 0.0, 0.0, 0.0]       # Initial orientation at time 0\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.calibrate_data.prh_predictor import *\n",
    "\n",
    "?prh_predictor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_acc_for_exhalation_breaths(data_pkl):\n",
    "    \"\"\"\n",
    "    Plot accX, accY, and accZ values for each exhalation breath event and \n",
    "    return the average acceleration vector (abar0) around the events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pkl : object\n",
    "        The structured data object containing sensor data and notes_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    abar0 : numpy.ndarray\n",
    "        A vector containing the mean of accX, accY, and accZ during the 10 seconds \n",
    "        surrounding each exhalation breath event.\n",
    "    \"\"\"\n",
    "    # Extract the relevant accelerometer data\n",
    "    accX = data_pkl.data['CC-96']['accX'].values\n",
    "    accY = data_pkl.data['CC-96']['accY'].values\n",
    "    accZ = data_pkl.data['CC-96']['accZ'].values\n",
    "    datetime_data = data_pkl.data['CC-96']['datetime'].values\n",
    "\n",
    "    # Convert datetime_data to numpy.datetime64 for proper subtraction\n",
    "    datetime_data = np.array(datetime_data, dtype='datetime64[ns]')\n",
    "\n",
    "    # Filter the notes_df for 'exhalation_breath' events\n",
    "    breath_events = data_pkl.notes_df[data_pkl.notes_df['key'] == 'exhalation_breath']\n",
    "\n",
    "    # Initialize lists to store the surrounding data\n",
    "    accX_segments = []\n",
    "    accY_segments = []\n",
    "    accZ_segments = []\n",
    "\n",
    "    # Plot the accelerometer data for each breath event\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for _, event in breath_events.iterrows():\n",
    "        event_time = event['datetime']  # Convert event_time to numpy.datetime64\n",
    "\n",
    "        # Round to the nearest second\n",
    "        event_time_rounded = event_time.round('1s')\n",
    "\n",
    "        # Find the index of the rounded event time\n",
    "        rounded_time_index = datetime_data.tolist().index(event_time_rounded)\n",
    "\n",
    "        # Define the window of ±5 seconds around the breath event\n",
    "        time_window = int(5 * data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "        start_index = max(rounded_time_index - time_window, 0)\n",
    "        end_index = min(rounded_time_index + time_window, len(datetime_data))\n",
    "\n",
    "        # Extract the segments\n",
    "        accX_segment = accX[start_index:end_index]\n",
    "        accY_segment = accY[start_index:end_index]\n",
    "        accZ_segment = accZ[start_index:end_index]\n",
    "        time_segment = datetime_data[start_index:end_index]\n",
    "\n",
    "        # Append to lists\n",
    "        accX_segments.append(accX_segment)\n",
    "        accY_segments.append(accY_segment)\n",
    "        accZ_segments.append(accZ_segment)\n",
    "\n",
    "        # Plot accX, accY, and accZ around the breath event\n",
    "        plt.plot(time_segment, accX_segment, label='accX', color='blue', alpha=0.5)\n",
    "        plt.plot(time_segment, accY_segment, label='accY', color='green', alpha=0.5)\n",
    "        plt.plot(time_segment, accZ_segment, label='accZ', color='red', alpha=0.5)\n",
    "\n",
    "        # Highlight the breath event\n",
    "        plt.axvline(datetime_data[closest_time_index], color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Acceleration (g)')\n",
    "    plt.title('Accelerometer Data (accX, accY, accZ) During Exhalation Breaths')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the mean vector abar0\n",
    "    mean_accX = np.mean(np.concatenate(accX_segments))\n",
    "    mean_accY = np.mean(np.concatenate(accY_segments))\n",
    "    mean_accZ = np.mean(np.concatenate(accZ_segments))\n",
    "    abar0 = np.array([mean_accX, mean_accY, mean_accZ])\n",
    "\n",
    "    return abar0\n",
    "\n",
    "# Example usage (doesn't work currently)\n",
    "#abar0 = plot_acc_for_exhalation_breaths(data_pkl)\n",
    "#print(\"Average acceleration vector (abar0):\", abar0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "abar0 = [0, 0, -9.8] # over-writing to check \n",
    "\n",
    "def orientation_and_heading_correction(abar0, acc_data, mag_data):\n",
    "    # Normalize abar0 to create abar\n",
    "    abar = abar0 / np.linalg.norm(abar0)\n",
    "    \n",
    "    # Calculate initial pitch (p0) and roll (r0)\n",
    "    p0 = np.arcsin(abar[0])\n",
    "    r0 = np.arctan2(abar[1], abar[2])\n",
    "    # Constrain p to [-pi / 2, pi / 2]\n",
    "    if p0 > np.pi / 2:\n",
    "        p0 = np.pi / 2 - p0\n",
    "        r0 = r0 + np.pi\n",
    "\n",
    "    print(\"p0:\", p0)\n",
    "    print(\"r0:\", r0)\n",
    "    \n",
    "    # Define rotation matrices for pitch and roll\n",
    "    def rotP(p):\n",
    "        return np.array([[np.cos(p), 0, np.sin(p)],\n",
    "                         [0, 1, 0],\n",
    "                         [-np.sin(p), 0, np.cos(p)]])\n",
    "    \n",
    "    def rotR(r):\n",
    "        return np.array([[1, 0, 0],\n",
    "                         [0, np.cos(r), -np.sin(r)],\n",
    "                         [0, np.sin(r), np.cos(r)]])\n",
    "    \n",
    "    # Calculate rotation matrix W\n",
    "    W = np.matmul(rotP(p0), rotR(r0)).T\n",
    "    print(\"W:\", W)\n",
    "\n",
    "    # Sanity check 1\n",
    "    print(\"Sanity check 1: tag acc (abar) should equal product [0 0 -g]QW^-1 (here Q=I)\")\n",
    "    print(\"abar:\", abar)\n",
    "    print(\"Expected [0 0 -g]QW^-1:\", np.matmul(np.array([0, 0, -1]), W.T))\n",
    "\n",
    "    # Sanity check 2\n",
    "    print(\"Sanity check 2: a_animal [0 0 -g] = abar * W abar times W should yield [0 0 -1]\")\n",
    "    print(\"abar * W:\", np.matmul(abar, W))\n",
    "    \n",
    "    # Correct the accelerometer and magnetometer data for the entire dataset\n",
    "    acc_corr = np.matmul(acc_data, W)\n",
    "    mag_corr = np.matmul(mag_data, W)\n",
    "    \n",
    "    # Calculate magnitude of the corrected accelerometer vectors\n",
    "    A = np.linalg.norm(acc_corr, axis=1)\n",
    "    \n",
    "    # Calculate pitch and roll in degrees from corrected accelerometer data\n",
    "    pitch_deg = np.degrees(np.arcsin(acc_corr[:, 0] / A))\n",
    "    roll_deg = np.degrees(np.arctan2(acc_corr[:, 1], acc_corr[:, 2]))\n",
    "    \n",
    "    b = data_pkl.data['CC-96']['field_intensity_mag2']\n",
    "    i = data_pkl.data['CC-96']['inclination_angle2']\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate heading in degrees from corrected magnetometer data\n",
    "    # TODO: flatten first\n",
    "    heading_deg = np.degrees(np.arctan2(-mag_corr[:, 1], mag_corr[:, 0]))\n",
    "    \n",
    "    # Return the corrected pitch, roll, and heading for the entire dataset\n",
    "    return pitch_deg, roll_deg, heading_deg, acc_corr, mag_corr\n",
    "\n",
    "# Use the function to get corrected orientation and heading for the entire dataset\n",
    "pitch_deg, roll_deg, heading_deg, acc_corr, mag_corr = orientation_and_heading_correction(abar0, acc_data, mag_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot_tag_data_interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corrected accelerometer and magnetometer data back to data_pkl\n",
    "data_pkl.data['CC-96']['corr_accX'] = acc_corr[:, 0]\n",
    "data_pkl.data['CC-96']['corr_accY'] = acc_corr[:, 1]\n",
    "data_pkl.data['CC-96']['corr_accZ'] = acc_corr[:, 2]\n",
    "data_pkl.data['CC-96']['corr_magX'] = mag_corr[:, 0]\n",
    "data_pkl.data['CC-96']['corr_magY'] = mag_corr[:, 1]\n",
    "data_pkl.data['CC-96']['corr_magZ'] = mag_corr[:, 2]\n",
    "# Add the calculated pitch, roll, and heading to the data_pkl structure\n",
    "data_pkl.data['CC-96']['pitch_deg'] = pitch_deg\n",
    "data_pkl.data['CC-96']['roll_deg'] = roll_deg\n",
    "data_pkl.data['CC-96']['heading_deg'] = heading_deg\n",
    "\n",
    "\n",
    "imu_channels_to_plot = ['depth', 'accX', 'accY', 'accZ', 'corr_accX', 'corr_accY', 'corr_accZ', 'magX', 'magY', 'magZ', 'pitch_deg', 'roll_deg', 'heading_deg']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'exhalation_breath': 'depth'\n",
    "}\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=1, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, note_annotations= notes_to_plot,\n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorrAccX: (AccX/10)*0.999954056289382 + (AccY/10)*(0.00561014920265745) + (AccZ/10)*(-0.00777248585299344)\n",
    "CorrAccY: (AccX/10)*(0) + (AccY/10)*(0.810843233429072) + (AccZ/10)*(0.585263402924091)\n",
    "CorrAccZ: (AccX/10)*0.0095856825740821 + (AccY/10)*(-0.585236513751672) + (AccZ/10)*(0.810805980282199)\n",
    "\n",
    "Pitch: RadtoDeg(-ASin(CorrAccX/Sqrt(CorrAccX^2+CorrAccY^2+CorrAccZ^2)))\n",
    "Roll: RadToDeg(ATan2((-CorrAccX/Sqrt(CorrAccY^2+CorrAccY^2+CorrAccZ^2)),(-CorrAccZ/Sqrt(CorrAccX^2+CorrAccY^2+CorrAccZ^2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Function to apply a low-pass filter to extract the static component (gravity)\n",
    "def low_pass_filter(data, cutoff, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to calculate ODBA\n",
    "def calculate_odba(accX, accY, accZ, cutoff=0.1, fs=10):\n",
    "    # Apply low-pass filter to get the static acceleration\n",
    "    accX_static = low_pass_filter(accX, cutoff, fs)\n",
    "    accY_static = low_pass_filter(accY, cutoff, fs)\n",
    "    accZ_static = low_pass_filter(accZ, cutoff, fs)\n",
    "\n",
    "    # Subtract the static component to get the dynamic acceleration\n",
    "    accX_dynamic = accX - accX_static\n",
    "    accY_dynamic = accY - accY_static\n",
    "    accZ_dynamic = accZ - accZ_static\n",
    "\n",
    "    # Calculate ODBA\n",
    "    odba = np.abs(accX_dynamic) + np.abs(accY_dynamic) + np.abs(accZ_dynamic)\n",
    "    \n",
    "    return odba\n",
    "\n",
    "# Example usage with your data\n",
    "accX = data_pkl.data['CC-96']['accX_adjusted'].values\n",
    "accY = data_pkl.data['CC-96']['accY_adjusted'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ_adjusted'].values\n",
    "\n",
    "odba = calculate_odba(accX, accY, accZ)\n",
    "\n",
    "data_pkl.data['CC-96']['odba'] = odba\n",
    "\n",
    "imu_channels_to_plot = ['depth', 'accX', 'accY', 'accZ', 'odba', 'pitch_deg', 'roll_deg', 'heading_deg']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'exhalation_breath': 'depth'\n",
    "}\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=1, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, note_annotations= notes_to_plot,\n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from pythreejs import *\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the path to the OBJ file\n",
    "model_path = os.path.join(data_dir, '6_killerWhale_v017_LP.obj')\n",
    "\n",
    "# Load the OBJ file using Trimesh\n",
    "mesh = trimesh.load(model_path)\n",
    "\n",
    "# Extract vertices and faces from the mesh\n",
    "vertices = mesh.vertices\n",
    "faces = mesh.faces.astype(np.uint32)  # Ensure the indices are unsigned integers\n",
    "\n",
    "# Create BufferGeometry for PyThreeJS\n",
    "geometry = BufferGeometry(\n",
    "    attributes={\n",
    "        'position': BufferAttribute(vertices, normalized=False),\n",
    "        'index': BufferAttribute(faces.flatten(), normalized=False)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a material\n",
    "material = MeshLambertMaterial(color='red') # , roughness=0.9, metalness=0.2\n",
    "key_light = DirectionalLight(color='white', position=[3, 5, 1], intensity=0.75)\n",
    "\n",
    "# Create the Mesh\n",
    "mesh = Mesh(geometry=geometry, material=material, position=[0, 0, 200], rotation= [0, math.pi, 0, 'XYZ'])\n",
    "\n",
    "# Move (translate) the mesh\n",
    "mesh.position = [0, 0, 200]  # Move the mesh up by 1 unit on the Y-axis\n",
    "\n",
    "# Rotate the mesh\n",
    "#mesh.rotation = [0.5, 0.5, 0, 0]  # Rotate the mesh around the X and Y axes (in radians)\n",
    "\n",
    "# Scale the mesh\n",
    "mesh.scale = [1.5, 1.5, 1.5]  # Scale the mesh uniformly by 1.5 on all axes\n",
    "\n",
    "# Create a scene, camera, and renderer\n",
    "scene = Scene(children=[mesh, AmbientLight(color='#ffffff', intensity=0.5)])\n",
    "camera = PerspectiveCamera(position=[0, 0, 5], up=[0, 1, 0], children=[key_light])\n",
    "renderer = Renderer(camera=camera, scene=scene, controls=[OrbitControls(controlling=camera)], width=400, height=400)\n",
    "\n",
    "# Display the model in the notebook\n",
    "display(renderer)\n",
    "\n",
    "ball = Mesh(geometry=SphereGeometry(), \n",
    "            material=MeshLambertMaterial(color='red'))\n",
    "\n",
    "\n",
    "c = PerspectiveCamera(position=[0, 5, 5], up=[0, 1, 0], children=[key_light])\n",
    "\n",
    "scene = Scene(children=[ball, c, AmbientLight(color='#777777')], background=None)\n",
    "\n",
    "renderer = Renderer(camera=c, \n",
    "                    scene=scene,\n",
    "                    alpha=True,\n",
    "                    clearOpacity=0,\n",
    "                    controls=[OrbitControls(controlling=c)])\n",
    "display(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.feature_generation_utils import get_heart_rate\n",
    "hr = get_heart_rate(data_pkl.data['UF-01']['ecg'])\n",
    "data_pkl.data['UF-01']['hr'] = hr\n",
    "data_pkl.info['UF-01']['channelinfo']['hr'] = 'Heart Rate'\n",
    "\n",
    "print(hr)\n",
    "\n",
    "from plotnine import ggplot, aes, geom_histogram, labs\n",
    "\n",
    "# Assuming 'hr' is a pandas Series or a column in a DataFrame\n",
    "# For demonstration, let's assume it's part of a DataFrame named df\n",
    "df = pd.DataFrame({'hr': hr})\n",
    "\n",
    "# Create the histogram\n",
    "histogram_plot = (\n",
    "    ggplot(df, aes(x='hr')) +\n",
    "    geom_histogram(binwidth=5, fill='lightblue', color='white', alpha=0.7) +\n",
    "    labs(title='Histogram of Heart Rate (HR)', x='Heart Rate (HR)', y='Count')\n",
    ")\n",
    "\n",
    "# To display the plot\n",
    "print(histogram_plot)\n",
    "\n",
    "imu_channels_to_plot = ['depth', 'accX', 'accY', 'accZ', 'odba']\n",
    "ephys_channels_to_plot = ['ecg', 'hr']\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'exhalation_breath': 'depth',\n",
    "    'heartbeat_manual_ok': 'ecg',\n",
    "}\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=1, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, note_annotations= notes_to_plot,\n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
