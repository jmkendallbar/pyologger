{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montage Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Import pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.utils.montage_manager import MontageManager\n",
    "from pyologger.utils.param_manager import ParamManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata\n",
    "\n",
    "Load in metadata stored in Notion databases. Alternatively, load in your own metadata in separate dataframes for deployments, loggers, recordings, animals, and datasets. See examples here in the `metadata_snapshot.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "overwrite = True\n",
    "# Define the path to the metadata pickle file\n",
    "metadata_pickle_path = os.path.join(data_dir, \"00_Metadata/metadata_snapshot.pkl\")\n",
    "\n",
    "# Check if the metadata pickle file exists and is more than 5 days old\n",
    "if overwrite or not os.path.exists(metadata_pickle_path) or (datetime.now() - datetime.fromtimestamp(os.path.getmtime(metadata_pickle_path))) > timedelta(days=5):\n",
    "    \n",
    "    metadata = Metadata()\n",
    "\n",
    "    # Save database variables\n",
    "    deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "    logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "    recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "    animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "    dataset_db = metadata.get_metadata(\"dataset_DB\")\n",
    "    procedure_db = metadata.get_metadata(\"procedure_DB\")\n",
    "    observation_db = metadata.get_metadata(\"observation_DB\")\n",
    "    collaborator_db = metadata.get_metadata(\"collaborator_DB\")\n",
    "    location_db = metadata.get_metadata(\"location_DB\")\n",
    "    montage_db = metadata.get_metadata(\"montage_DB\")\n",
    "    sensor_db = metadata.get_metadata(\"sensor_DB\")\n",
    "    attachment_db = metadata.get_metadata(\"attachment_DB\")\n",
    "    originalchannel_db = metadata.get_metadata(\"originalchannel_DB\")\n",
    "    standardizedchannel_db = metadata.get_metadata(\"standardizedchannel_DB\")\n",
    "    derivedsignal_db = metadata.get_metadata(\"derivedsignal_DB\")\n",
    "    derivedchannel_db = metadata.get_metadata(\"derivedchannel_DB\")\n",
    "\n",
    "    # Get the relations map\n",
    "    relations_map = metadata.relations_map\n",
    "\n",
    "    # Define the path to save the relations map\n",
    "    relations_map_path = os.path.join(config['paths']['local_repo_path'], 'relations_map.json')\n",
    "\n",
    "    # Save the relations map as a JSON file\n",
    "    with open(relations_map_path, 'w') as file:\n",
    "        json.dump(relations_map, file, indent=4)\n",
    "\n",
    "    print(f\"Relations map saved at: {relations_map_path}\")\n",
    "\n",
    "    ## OPTIONAL: Save metadata snapshot as a pickle file\n",
    "    metadata.notion = None  # Temporarily remove the Notion client\n",
    "    # Save metadata snapshot as a pickle file\n",
    "    with open(metadata_pickle_path, \"wb\") as file:\n",
    "        pickle.dump(metadata, file)\n",
    "\n",
    "    print(f\"Metadata snapshot saved at: {metadata_pickle_path}\")\n",
    "else:\n",
    "    print(f\"Recent metadata snapshot loaded, already present at: {metadata_pickle_path}\")\n",
    "    \n",
    "    # Load the metadata snapshot from the pickle file\n",
    "    with open(metadata_pickle_path, \"rb\") as file:\n",
    "        metadata = pickle.load(file)\n",
    "    \n",
    "    # Save database variables\n",
    "    deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "    logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "    recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "    animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "    dataset_db = metadata.get_metadata(\"dataset_DB\")\n",
    "    procedure_db = metadata.get_metadata(\"procedure_DB\")\n",
    "    observation_db = metadata.get_metadata(\"observation_DB\")\n",
    "    collaborator_db = metadata.get_metadata(\"collaborator_DB\")\n",
    "    location_db = metadata.get_metadata(\"location_DB\")\n",
    "    montage_db = metadata.get_metadata(\"montage_DB\")\n",
    "    sensor_db = metadata.get_metadata(\"sensor_DB\")\n",
    "    attachment_db = metadata.get_metadata(\"attachment_DB\")\n",
    "    originalchannel_db = metadata.get_metadata(\"originalchannel_DB\")\n",
    "    standardizedchannel_db = metadata.get_metadata(\"standardizedchannel_DB\")\n",
    "    derivedsignal_db = metadata.get_metadata(\"derivedsignal_DB\")\n",
    "    derivedchannel_db = metadata.get_metadata(\"derivedchannel_DB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset folder\n",
    "dataset_folder = select_folder(data_dir, \"Select a dataset folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = select_folder(dataset_folder, \"Select a deployment folder:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deployment_id and animal_id from the folder name\n",
    "match = re.match(r\"(\\d{4}-\\d{2}-\\d{2}_[a-z]{4}-\\d{3})\", os.path.basename(deployment_folder), re.IGNORECASE)\n",
    "if match:\n",
    "    deployment_id = match.group(1)  # Extract YYYY-MM-DD_animalID\n",
    "    animal_id = deployment_id.split(\"_\")[1]  # Extract animal ID\n",
    "    print(f\"‚úÖ Extracted deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå Unable to extract deployment ID from folder: {deployment_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files in Deployment Folder\n",
    "\n",
    "Using datareader to load in files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print extracted values for debugging\n",
    "print(f\"üê≥ Deployment ID: {deployment_id}, Animal ID: {animal_id}\")\n",
    "\n",
    "deployment_info, loggers_used = metadata.extract_essential_metadata(deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize DataReader with dataset folder, deployment ID, and optional data subfolder\n",
    "data_pkl = DataReader(dataset_folder=dataset_folder, deployment_id=deployment_id, data_subfolder=\"01_raw-data\", montage_path=montage_path)\n",
    "# Step 5: Initialize config manager\n",
    "param_manager = ParamManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview montages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_channels = originalchannel_db[originalchannel_db['Montages'].str.contains('hr-montage_V2', na=False)]\n",
    "original_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_csv = False # üîÅ Set this to True if you want to load montage from CSVs\n",
    "csv_folder = \"/path/to/csvs\"  # Folder where your CSVs are stored\n",
    "\n",
    "montage_inputs = {}\n",
    "\n",
    "standardizedchannel_db = standardizedchannel_db.copy()\n",
    "standardizedchannel_db.loc[:, \"Color\"] = standardizedchannel_db[\"Color preview\"].str.extract(\n",
    "    r'\\\\color\\s*\\{\\s*#?(\\w+)\\s*\\}')\n",
    "\n",
    "for logger in loggers_used:\n",
    "    logger_id = logger[\"Logger ID\"]\n",
    "    montage_id = logger[\"Montage ID\"]\n",
    "\n",
    "    if use_csv:\n",
    "        csv_path = os.path.join(csv_folder, f\"{montage_id}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            montage_df = pd.read_csv(csv_path)\n",
    "            montage_inputs[logger_id] = montage_df\n",
    "            print(f\"üìÑ Loaded montage CSV for Logger: {logger_id} (Montage ID: {montage_id})\")\n",
    "        else:\n",
    "            print(f\"‚ùå CSV not found for montage ID: {montage_id} (Logger: {logger_id})\")\n",
    "        continue\n",
    "\n",
    "    # Otherwise, build from original + standardized DBs\n",
    "    original_channels = originalchannel_db[originalchannel_db['Montages'].str.contains(montage_id, na=False)]\n",
    "\n",
    "    if original_channels.empty:\n",
    "        print(f\"‚ö†Ô∏è No original channels found for montage ID: {montage_id} (Logger: {logger_id})\")\n",
    "        continue\n",
    "\n",
    "    montage_df = pd.merge(\n",
    "        standardizedchannel_db,\n",
    "        original_channels,\n",
    "        on='Standardized Channel ID',\n",
    "        suffixes=('', '_original')\n",
    "    )\n",
    "\n",
    "    montage_df.columns = montage_df.columns.str.lower().str.replace(' ', '_')\n",
    "    montage_df = montage_df[[\n",
    "        'original_channel_id', 'original_unit', 'manufacturer_sensor_name',\n",
    "        'standardized_channel_id', 'standardized_unit', 'standardized_sensor_type'\n",
    "    ]]\n",
    "\n",
    "    montage_inputs[logger_id] = montage_df\n",
    "    print(f\"‚úÖ Created montage_df for Logger: {logger_id} (Montage ID: {montage_id})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage_manager = MontageManager(montage_folder=os.path.dirname(montage_path))\n",
    "\n",
    "montages_metadata = montage_manager.add_missing_montages_per_logger(\n",
    "\tloggers_used=loggers_used,\n",
    "\tmontage_inputs=montage_inputs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
