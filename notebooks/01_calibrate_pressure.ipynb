{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero offset correction: calibrate pressure sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.config_manager import ConfigManager\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 01 IN PROGRESS.\"\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dives\n",
    "Involves a zero offset correction with `zoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?smooth_downsample_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the depth and temperature data\n",
    "depth_data = data_pkl.sensor_data['pressure']['pressure']\n",
    "depth_datetime = data_pkl.sensor_data['pressure']['datetime']\n",
    "depth_fs = data_pkl.sensor_info['pressure']['sampling_frequency']\n",
    "temp_data = data_pkl.sensor_data['temperature']['temp']\n",
    "temp_fs = data_pkl.sensor_info['temperature']['sampling_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and load configuration settings for dive detection\n",
    "dive_detection_params = {\n",
    "    \"first_deriv_threshold\": 0.1,          # Threshold for the first derivative of depth (meters/second)\n",
    "    \"min_duration\": 30,                   # Minimum duration for a surface interval (seconds)\n",
    "    \"depth_threshold\": 5,                 # Maximum depth to qualify as a surface interval (meters)\n",
    "    \"apply_temp_correction\": False,       # Apply temperature correction during zero offset correction (True/False)\n",
    "    \"min_depth_threshold\": 0.5,           # Minimum depth to start/end a dive (meters)\n",
    "    \"dive_duration_threshold\": 10,        # Minimum duration to qualify as a dive (seconds)\n",
    "    \"smoothing_window\": 5,                # Smoothing window size for dive detection (seconds)\n",
    "    \"downsampled_sampling_rate\": 1        # Target sampling rate after downsampling (Hz)\n",
    "}\n",
    "\n",
    "# Add dive detection settings to the configuration file\n",
    "config_manager.add_to_config(entries=dive_detection_params, section=\"dive_detection_settings\")\n",
    "\n",
    "# Retrieve dive detection parameters from the configuration\n",
    "dive_detection_settings = config_manager.get_from_config(\n",
    "    variable_names=list(dive_detection_params.keys()), \n",
    "    section=\"dive_detection_settings\"\n",
    ")\n",
    "\n",
    "# Print the loaded parameters\n",
    "print(\"Loaded Dive Detection Parameters:\")\n",
    "for key, value in dive_detection_settings.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Process depth data: downsample and calculate the first derivative\n",
    "depth_processing_params = {\n",
    "    \"original_sampling_rate\": depth_fs,\n",
    "    \"downsampled_sampling_rate\": dive_detection_settings[\"downsampled_sampling_rate\"]\n",
    "}\n",
    "first_derivative, downsampled_depth = smooth_downsample_derivative(depth_data, **depth_processing_params)\n",
    "depth_downsampled_datetime = depth_datetime[::int(depth_fs / dive_detection_settings[\"downsampled_sampling_rate\"])]\n",
    "\n",
    "# Detect flat chunks (potential surface intervals)\n",
    "flat_chunk_params = {\n",
    "    \"depth\": downsampled_depth,\n",
    "    \"datetime_data\": depth_downsampled_datetime,\n",
    "    \"first_derivative\": first_derivative,\n",
    "    \"threshold\": dive_detection_settings[\"first_deriv_threshold\"],\n",
    "    \"min_duration\": dive_detection_settings[\"min_duration\"],\n",
    "    \"depth_threshold\": dive_detection_settings[\"depth_threshold\"],\n",
    "    \"original_sampling_rate\": depth_fs,\n",
    "    \"downsampled_sampling_rate\": dive_detection_settings[\"downsampled_sampling_rate\"]\n",
    "}\n",
    "flat_chunks = detect_flat_chunks(**flat_chunk_params)\n",
    "\n",
    "# Apply zero offset correction\n",
    "zoc_params = {\n",
    "    \"depth\": downsampled_depth,\n",
    "    \"temp\": temp_data.values if temp_data is not None else None,\n",
    "    \"flat_chunks\": flat_chunks\n",
    "}\n",
    "corrected_depth_temp, corrected_depth_no_temp, depth_correction = apply_zero_offset_correction(**zoc_params)\n",
    "\n",
    "# Choose corrected depth based on temperature correction setting\n",
    "if dive_detection_settings[\"apply_temp_correction\"]:\n",
    "    corrected_depth = corrected_depth_temp\n",
    "else:\n",
    "    corrected_depth = corrected_depth_no_temp\n",
    "\n",
    "# Detect dives in the corrected depth data\n",
    "dive_detection_params = {\n",
    "    \"depth_series\": corrected_depth,\n",
    "    \"datetime_data\": depth_downsampled_datetime,\n",
    "    \"min_depth_threshold\": dive_detection_settings[\"min_depth_threshold\"],\n",
    "    \"sampling_rate\": dive_detection_settings[\"downsampled_sampling_rate\"],\n",
    "    \"duration_threshold\": dive_detection_settings[\"dive_duration_threshold\"],\n",
    "    \"smoothing_window\": dive_detection_settings[\"smoothing_window\"]\n",
    "}\n",
    "dives = find_dives(**dive_detection_params)\n",
    "# Calculate dive duration in seconds\n",
    "dives['dive_duration'] = (dives['end_time'] - dives['start_time']).dt.total_seconds()\n",
    "\n",
    "# Log transformations\n",
    "transformation_log = [\n",
    "    f\"downsampled_{dive_detection_settings['downsampled_sampling_rate']}Hz\",\n",
    "    f\"smoothed_{dive_detection_settings['smoothing_window']}s\",\n",
    "    f\"ZOC_settings__first_deriv_threshold_{dive_detection_settings['first_deriv_threshold']}mps__min_duration_{dive_detection_settings['min_duration']}s__depth_threshold_{dive_detection_settings['depth_threshold']}m\",\n",
    "    f\"DIVE_detection_settings__min_depth_threshold_{dive_detection_settings['min_depth_threshold']}m__dive_duration_threshold_{dive_detection_settings['dive_duration_threshold']}s__smoothing_window_{dive_detection_settings['smoothing_window']}\"\n",
    "]\n",
    "\n",
    "# Outputs\n",
    "print(f\"Number of potential surface intervals detected: {len(flat_chunks)}\")\n",
    "print(f\"Number of dives detected: {len(dives)}\")\n",
    "print(\"Transformation Log:\", transformation_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and update dive events\n",
    "data_pkl.event_data = create_state_event(\n",
    "    state_df=dives,\n",
    "    key='dive',\n",
    "    value_column='max_depth',\n",
    "    start_time_column='start_time',\n",
    "    duration_column='dive_duration', # in seconds\n",
    "    description='dive_start',\n",
    "    existing_events=data_pkl.event_data  # Pass existing events for overwrite and concatenation\n",
    ")\n",
    "\n",
    "# Update event_info with unique keys\n",
    "data_pkl.event_info = list(data_pkl.event_data['key'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the derived_from_sensors list\n",
    "derived_from_sensors = [\"pressure\"]\n",
    "original_name = 'Temp-corrected Depth (m)' if dive_detection_settings[\"apply_temp_correction\"] else 'Corrected Depth (m)'\n",
    "\n",
    "# Save the corrected depth back to the data structure\n",
    "depth_df = pd.DataFrame({\n",
    "    'datetime': depth_downsampled_datetime,\n",
    "    'depth': corrected_depth\n",
    "})\n",
    "derived_info = {\n",
    "    \"channels\": [\"depth\"],\n",
    "    \"metadata\": {\n",
    "            'depth': {'original_name': original_name,\n",
    "                    'unit': 'm',\n",
    "                    'sensor': 'pressure'}\n",
    "    },\n",
    "    \"derived_from_sensors\": derived_from_sensors.append(\"temperature\") if dive_detection_settings[\"apply_temp_correction\"] else derived_from_sensors,\n",
    "    \"transformation_log\": transformation_log.append(\"temperature_correction\") if dive_detection_settings[\"apply_temp_correction\"] else derived_from_sensors\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['depth'] = depth_df\n",
    "data_pkl.derived_info['depth'] = derived_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time range for plotting from the configuration file\n",
    "time_settings = config_manager.get_from_config(\n",
    "    variable_names=[\"earliest_common_start_time\", \"latest_common_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "# Convert the loaded time settings into datetime objects\n",
    "plot_start_time = pd.Timestamp(time_settings[\"earliest_common_start_time\"]).to_pydatetime()\n",
    "plot_end_time = pd.Timestamp(time_settings[\"latest_common_end_time\"]).to_pydatetime()\n",
    "\n",
    "# Ensure the loaded times are valid\n",
    "if not (plot_start_time and plot_end_time):\n",
    "    raise ValueError(\"Invalid time range loaded from configuration file.\")\n",
    "\n",
    "# Define the time range for the plot\n",
    "time_range = (plot_start_time, plot_end_time)\n",
    "\n",
    "# Define the sensors and derived data to plot\n",
    "sensors_to_plot = ['pressure']\n",
    "derived_data_to_plot = ['depth']\n",
    "\n",
    "# Define annotations or notes to include in the plot (optional, e.g., detected dives or flat intervals)\n",
    "note_annotations = {\n",
    "    'dive': {'signal': 'depth', 'symbol': 'triangle-down', 'color': 'blue'}  # Example for dive events\n",
    "}\n",
    "\n",
    "# Use plot_tag_data_interactive5 to create the interactive plot\n",
    "fig = plot_tag_data_interactive5(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=sensors_to_plot,\n",
    "    derived_data_signals=derived_data_to_plot,\n",
    "    time_range=time_range,\n",
    "    note_annotations=note_annotations,\n",
    "    color_mapping_path=color_mapping_path,  # Path to the color mapping JSON file\n",
    "    target_sampling_rate=1  # Adjust this if you want the data plotted at a specific resolution\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()  # If running in a Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.save_to_netcdf(os.path.join(deployment_folder, 'outputs/deployment_data_processed.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?DataReader.save_to_netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, 'outputs', 'deployment_data_processed.nc')\n",
    "\n",
    "# Open the NetCDF file\n",
    "data = xr.open_dataset(netcdf_path)\n",
    "\n",
    "# Display the contents of the NetCDF file\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically generate and print the statement\n",
    "print(f\"`{', '.join(derived_from_sensors)}` sensor data was transformed into derived data `depth` by applying these transformations: {', '.join(transformation_log)}\")\n",
    "current_processing_step = \"Processing Step 01. Calibration of pressure sensor complete and dives analyzed.\"\n",
    "print(current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or update the current_processing_step for the specified deployment\n",
    "print(current_processing_step)\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyologger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
