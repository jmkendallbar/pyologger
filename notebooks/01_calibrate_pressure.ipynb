{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero offset correction: calibrate pressure sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and dep_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(dep_db, data_dir)\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=True, save_parq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dives\n",
    "Involves a zero offset correction with `zoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?smooth_downsample_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the depth and temperature data\n",
    "depth_data = data_pkl.data['CC-96']['depth'].values\n",
    "temp_data = data_pkl.data['CC-96'].get('tempIMU')\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "datetime_data = pd.to_datetime(data_pkl.data['CC-96']['datetime'])\n",
    "\n",
    "# Sidebar for parameters\n",
    "threshold = 0.1 # meters\n",
    "min_duration = 30 # seconds\n",
    "depth_threshold = 5 # meters\n",
    "apply_temp_correction = False\n",
    "\n",
    "# Dive detection parameters\n",
    "min_depth_threshold = 1.0\n",
    "dive_duration_threshold = 10\n",
    "smoothing_window = value=5\n",
    "\n",
    "# Process depth data\n",
    "first_derivative, downsampled_depth = smooth_downsample_derivative(\n",
    "    depth_data, \n",
    "    original_sampling_rate=sampling_rate, \n",
    "    downsampled_sampling_rate=1)\n",
    "\n",
    "# Detect flat chunks (potential surface intervals)\n",
    "flat_chunks = detect_flat_chunks(\n",
    "    depth=downsampled_depth, \n",
    "    datetime_data=datetime_data[::int(sampling_rate)],  # Adjust datetime data to match downsampled depth\n",
    "    first_derivative=first_derivative, \n",
    "    threshold=threshold, \n",
    "    min_duration=min_duration, \n",
    "    depth_threshold=depth_threshold, \n",
    "    original_sampling_rate=400, \n",
    "    downsampled_sampling_rate=1\n",
    ")\n",
    "num_flat_chunks = len(flat_chunks)\n",
    "print(f\"Number of potential surface intervals detected: {num_flat_chunks}\")\n",
    "\n",
    "# Apply zero offset correction\n",
    "corrected_depth_temp, corrected_depth_no_temp, depth_correction = apply_zero_offset_correction(\n",
    "    depth=downsampled_depth, \n",
    "    temp=temp_data.values if temp_data is not None else None, \n",
    "    flat_chunks=flat_chunks\n",
    ")\n",
    "\n",
    "# Upsample and adjust the corrected depths to match original sampling rate\n",
    "upsampling_factor = int(sampling_rate / 1)\n",
    "repeated_corrected_depth_temp = upsample(corrected_depth_temp, upsampling_factor, len(depth_data))\n",
    "repeated_corrected_depth_no_temp = upsample(corrected_depth_no_temp, upsampling_factor, len(depth_data))\n",
    "\n",
    "# Detect dives in the corrected depth data\n",
    "dives = find_dives(\n",
    "    depth_series=repeated_corrected_depth_no_temp,\n",
    "    datetime_data=datetime_data,\n",
    "    min_depth_threshold=min_depth_threshold,\n",
    "    sampling_rate=sampling_rate,\n",
    "    duration_threshold=dive_duration_threshold,\n",
    "    smoothing_window=smoothing_window\n",
    ")\n",
    "num_dives = len(dives)\n",
    "print(f\"Number of dives detected: {num_dives}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot_depth_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "dec_factor = int(upsampling_factor)\n",
    "\n",
    "fig = plot_depth_correction(datetime_data, dec_factor, depth_data, first_derivative, \n",
    "                            repeated_corrected_depth_temp, repeated_corrected_depth_no_temp, \n",
    "                            depth_correction, dives, flat_chunks, temp_data, apply_temp_correction)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corrected depth back to the data structure\n",
    "if apply_temp_correction:\n",
    "    data_pkl.data['CC-96']['corrdepth'] = repeated_corrected_depth_temp\n",
    "else:\n",
    "    data_pkl.data['CC-96']['corrdepth'] = repeated_corrected_depth_no_temp\n",
    "\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
