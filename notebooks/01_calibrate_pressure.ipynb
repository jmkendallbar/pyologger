{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero offset correction: calibrate pressure sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(deployment_db, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dives\n",
    "Involves a zero offset correction with `zoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?smooth_downsample_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info['pressure']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.logger_data['CC-35']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_data['pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the depth and temperature data\n",
    "depth_data = data_pkl.sensor_data['pressure']['pressure']\n",
    "depth_datetime = data_pkl.sensor_data['pressure']['datetime']\n",
    "depth_fs = data_pkl.sensor_info['pressure']['sampling_frequency']\n",
    "temp_data = data_pkl.sensor_data['temperature']['temp']\n",
    "temp_fs = data_pkl.sensor_info['temperature']['sampling_frequency']\n",
    "\n",
    "# Sidebar for parameters\n",
    "first_deriv_threshold = 0.1 # meters\n",
    "min_duration = 30 # seconds\n",
    "depth_threshold = 5 # meters\n",
    "apply_temp_correction = False\n",
    "\n",
    "# Dive detection parameters\n",
    "min_depth_threshold = 1.0\n",
    "dive_duration_threshold = 10\n",
    "smoothing_window = 5\n",
    "downsampled_sampling_rate = 1\n",
    "\n",
    "# Process depth data\n",
    "first_derivative, downsampled_depth = smooth_downsample_derivative(\n",
    "    depth_data, \n",
    "    original_sampling_rate=depth_fs, \n",
    "    downsampled_sampling_rate=downsampled_sampling_rate)\n",
    "\n",
    "# Create the transformation log using values\n",
    "transformation_log = [\n",
    "    f\"downsampled_{downsampled_sampling_rate}Hz\",\n",
    "    f\"smoothed_{smoothing_window}s\"\n",
    "]\n",
    "\n",
    "# Detect flat chunks (potential surface intervals)\n",
    "flat_chunks = detect_flat_chunks(\n",
    "    depth=downsampled_depth, \n",
    "    datetime_data=depth_datetime[::int(depth_fs/downsampled_sampling_rate)],  # Adjust datetime data to match downsampled depth\n",
    "    first_derivative=first_derivative, # first derivative data\n",
    "    threshold=first_deriv_threshold, # first derivative threshold\n",
    "    min_duration=min_duration, # minimum surface duration\n",
    "    depth_threshold=depth_threshold, \n",
    "    original_sampling_rate=depth_fs, \n",
    "    downsampled_sampling_rate=downsampled_sampling_rate\n",
    ")\n",
    "num_flat_chunks = len(flat_chunks)\n",
    "print(f\"Number of potential surface intervals detected: {num_flat_chunks}\")\n",
    "\n",
    "# Apply zero offset correction\n",
    "corrected_depth_temp, corrected_depth_no_temp, depth_correction = apply_zero_offset_correction(\n",
    "    depth=downsampled_depth, \n",
    "    temp=temp_data.values if temp_data is not None else None, \n",
    "    flat_chunks=flat_chunks\n",
    ")\n",
    "\n",
    "# Upsample and adjust the corrected depths to match original sampling rate\n",
    "upsampling_factor = int(depth_fs / 1)\n",
    "repeated_corrected_depth_temp = upsample(corrected_depth_temp, upsampling_factor, len(depth_data))\n",
    "repeated_corrected_depth_no_temp = upsample(corrected_depth_no_temp, upsampling_factor, len(depth_data))\n",
    "\n",
    "# Detect dives in the corrected depth data\n",
    "dives = find_dives(\n",
    "    depth_series=repeated_corrected_depth_no_temp,\n",
    "    datetime_data=depth_datetime,\n",
    "    min_depth_threshold=min_depth_threshold,\n",
    "    sampling_rate=depth_fs,\n",
    "    duration_threshold=dive_duration_threshold,\n",
    "    smoothing_window=smoothing_window\n",
    ")\n",
    "num_dives = len(dives)\n",
    "print(f\"Number of dives detected: {num_dives}\")\n",
    "\n",
    "# Append the additional transformations\n",
    "transformation_log += [\n",
    "    f\"ZOC_settings__first_deriv_threshold_{first_deriv_threshold}mps__minimum_duration_for_zoc_{min_duration}s__max_depth_for_surface_interval_{depth_threshold}m\",\n",
    "    f\"DIVE_detection_settings__min_depth_threshold_{min_depth_threshold}m__dive_duration_threshol_{dive_duration_threshold}s__smoothing_window_{smoothing_window}\"\n",
    "]\n",
    "\n",
    "transformation_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot_depth_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "dec_factor = int(upsampling_factor)*10\n",
    "\n",
    "fig = plot_depth_correction(depth_datetime, dec_factor, depth_data, first_derivative, \n",
    "                            repeated_corrected_depth_temp, repeated_corrected_depth_no_temp, \n",
    "                            depth_correction, dives, flat_chunks, temp_data, apply_temp_correction)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dive duration in seconds\n",
    "dives['dive_duration'] = (dives['end_time'] - dives['start_time']).dt.total_seconds()\n",
    "event_data = data_pkl.event_data\n",
    "\n",
    "# Function to create the event DataFrame\n",
    "def create_dive_events(dives):\n",
    "    # Generate the event DataFrame in one go\n",
    "    events_df = pd.DataFrame({\n",
    "        'date': dives['start_time'].dt.floor('D').dt.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'time': dives['start_time'].dt.strftime('%H:%M:%S.%f').str[:-3],\n",
    "        'value': dives['max_depth'],\n",
    "        'type': 'state',\n",
    "        'key': 'dive',\n",
    "        'duration': dives['dive_duration'],\n",
    "        'short_description': 'dive_start',\n",
    "        'long_description': 'NaN',  # Optional field, left blank\n",
    "        'datetime': dives['start_time']\n",
    "    })\n",
    "    \n",
    "    return events_df\n",
    "\n",
    "# Generate the events from dives\n",
    "new_events = create_dive_events(dives)\n",
    "\n",
    "# Check if event_data is empty or not a DataFrame\n",
    "if isinstance(event_data, dict) or event_data.empty:\n",
    "    # Initialize event_data as new_events if empty\n",
    "    event_data = new_events\n",
    "else:\n",
    "    # Concatenate the new events with the existing event_data\n",
    "    event_data = pd.concat([event_data, new_events], ignore_index=True)\n",
    "\n",
    "event_data\n",
    "data_pkl.event_data = event_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_info = []\n",
    "unique_keys = event_data['key'].unique()\n",
    "\n",
    "# Append unique keys to event_info, avoiding duplicates\n",
    "for key in unique_keys:\n",
    "    if key not in event_info:\n",
    "        event_info.append(key)\n",
    "\n",
    "data_pkl.event_info = event_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the derived_from_sensors list\n",
    "derived_from_sensors = [\"pressure\"]\n",
    "\n",
    "# Save the corrected depth back to the data structure\n",
    "if apply_temp_correction:\n",
    "    depth_df = pd.DataFrame({\n",
    "        'datetime': depth_datetime,\n",
    "        'depth': repeated_corrected_depth_temp\n",
    "    })\n",
    "    derived_info = {\n",
    "        \"channels\": [\"depth\"],\n",
    "        \"metadata\": {\n",
    "             'depth': {'original_name': 'Temp-corrected Depth (m)',\n",
    "                       'unit': 'm',\n",
    "                       'sensor': 'pressure'}\n",
    "        },\n",
    "        \"derived_from_sensors\": derived_from_sensors.append(\"temperature\"),\n",
    "        \"transformation_log\": transformation_log.append(\"temperature_correction\")\n",
    "    }\n",
    "else:\n",
    "    depth_df = pd.DataFrame({\n",
    "        'datetime': depth_datetime,\n",
    "        'depth': repeated_corrected_depth_no_temp\n",
    "    })\n",
    "    derived_info = {\n",
    "        \"channels\": [\"depth\"],\n",
    "        \"metadata\": {\n",
    "             'depth': {'original_name': 'Depth (m)',\n",
    "                       'unit': 'm',\n",
    "                       'sensor': 'pressure'}\n",
    "        },\n",
    "        \"derived_from_sensors\": derived_from_sensors,\n",
    "        \"transformation_log\": transformation_log\n",
    "    }\n",
    "\n",
    "data_pkl.derived_data['depth'] = depth_df\n",
    "data_pkl.derived_info['depth'] = derived_info\n",
    "\n",
    "# Dynamically generate and print the statement\n",
    "print(f\"`{', '.join(derived_from_sensors)}` sensor data was transformed into derived data `depth` by applying these transformations: {', '.join(transformation_log)}\")\n",
    "\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.save_to_netcdf('data/2019-11-08_apfo-001a/outputs/deployment_data_processed.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
