{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero offset correction: calibrate pressure sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "deployment_id = \"2024-01-24_oror-001\"\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, config_manager = select_and_load_deployment(\n",
    "    data_dir, dataset_id=dataset_id, deployment_id=deployment_id\n",
    "    )\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 01 IN PROGRESS.\"\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.derived_data['heart_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dives\n",
    "Involves a zero offset correction with `zoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?smooth_downsample_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the depth and temperature data\n",
    "depth_data = data_pkl.sensor_data['pressure']['pressure']\n",
    "depth_datetime = data_pkl.sensor_data['pressure']['datetime']\n",
    "depth_fs = data_pkl.sensor_info['pressure']['sampling_frequency']\n",
    "temp_data = data_pkl.sensor_data['temperature']['temp']\n",
    "temp_fs = data_pkl.sensor_info['temperature']['sampling_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 2: Load Configuration Parameters**\n",
    "dive_detection_settings = config_manager.get_from_config(\n",
    "    variable_names=[\n",
    "        \"first_deriv_threshold\", \"min_duration\", \"depth_threshold\",\n",
    "        \"apply_temp_correction\", \"min_depth_threshold\", \"dive_duration_threshold\",\n",
    "        \"smoothing_window\", \"downsampled_sampling_rate\", \"baseline_adjust\"\n",
    "    ],\n",
    "    section=\"dive_detection_settings\"\n",
    ")\n",
    "\n",
    "# Default settings\n",
    "default_settings = {\n",
    "    \"first_deriv_threshold\": 0.1,       # Threshold for the first derivative of depth (meters/second)\n",
    "    \"min_duration\": 60,                 # Minimum duration for a surface interval (seconds)\n",
    "    \"depth_threshold\": 5,               # Maximum depth to qualify as a surface interval (meters)\n",
    "    \"apply_temp_correction\": False,     # Apply temperature correction during zero offset correction (True/False)\n",
    "    \"min_depth_threshold\": 0.5,         # Minimum depth to start/end a dive (meters)\n",
    "    \"dive_duration_threshold\": 10,      # Minimum duration for a dive (seconds)\n",
    "    \"smoothing_window\": 5,              # Smoothing window\n",
    "    \"downsampled_sampling_rate\": 1,     # Target sampling rate after downsampling (default 1Hz)\n",
    "    \"baseline_adjust\": 0.0              # Adjust baseline depth data if needs to be raised or lowered\n",
    "}\n",
    "\n",
    "# If settings are missing or None, initialize them\n",
    "if dive_detection_settings is None:\n",
    "    dive_detection_settings = default_settings\n",
    "elif any(v is None for v in dive_detection_settings.values()):  \n",
    "    # Fill in only missing/None values\n",
    "    dive_detection_settings = {k: v if dive_detection_settings.get(k) is not None else default_settings[k] for k, v in dive_detection_settings.items()}\n",
    "    # Print to confirm\n",
    "    print(f\"✅ Missing configurations were added.\")\n",
    "\n",
    "# Save if changes were made\n",
    "config_manager.add_to_config(entries=dive_detection_settings, section=\"dive_detection_settings\")\n",
    "\n",
    "# Step 6: Process depth data - Downsample, smooth, adjust baseline, and calculate first derivative\n",
    "depth_processing_params = {\n",
    "    \"original_sampling_rate\": depth_fs,\n",
    "    \"downsampled_sampling_rate\": int(dive_detection_settings[\"downsampled_sampling_rate\"]),\n",
    "    \"baseline_adjust\": dive_detection_settings[\"baseline_adjust\"]  # New parameter\n",
    "}\n",
    "\n",
    "first_derivative, downsampled_depth = smooth_downsample_derivative(depth_data, **depth_processing_params)\n",
    "\n",
    "# Adjust datetime indexing based on the new downsample rate\n",
    "depth_downsampled_datetime = depth_datetime.iloc[::int(depth_fs / dive_detection_settings[\"downsampled_sampling_rate\"])]\n",
    "\n",
    "# Ensure indexing does not go out of bounds\n",
    "if len(depth_downsampled_datetime) > len(downsampled_depth):\n",
    "    depth_downsampled_datetime = depth_downsampled_datetime[:len(downsampled_depth)]\n",
    "\n",
    "# Print summary of processing\n",
    "print(f\"✅ Depth processing complete: Downsampled to {dive_detection_settings['downsampled_sampling_rate']} Hz\")\n",
    "print(f\"✅ Baseline adjustment applied: {dive_detection_settings['baseline_adjust']} meters\")\n",
    "\n",
    "# Detect flat chunks (potential surface intervals)\n",
    "flat_chunk_params = {\n",
    "    \"depth\": downsampled_depth,\n",
    "    \"datetime_data\": depth_downsampled_datetime,\n",
    "    \"first_derivative\": first_derivative,\n",
    "    \"threshold\": dive_detection_settings[\"first_deriv_threshold\"],\n",
    "    \"min_duration\": dive_detection_settings[\"min_duration\"],\n",
    "    \"depth_threshold\": dive_detection_settings[\"depth_threshold\"],\n",
    "    \"original_sampling_rate\": depth_fs,\n",
    "    \"downsampled_sampling_rate\": dive_detection_settings[\"downsampled_sampling_rate\"]\n",
    "}\n",
    "flat_chunks = detect_flat_chunks(**flat_chunk_params)\n",
    "\n",
    "# Apply zero offset correction\n",
    "zoc_params = {\n",
    "    \"depth\": downsampled_depth,\n",
    "    \"temp\": temp_data.values if temp_data is not None else None,\n",
    "    \"flat_chunks\": flat_chunks\n",
    "}\n",
    "corrected_depth_temp, corrected_depth_no_temp, depth_correction = apply_zero_offset_correction(**zoc_params)\n",
    "\n",
    "# Choose corrected depth based on temperature correction setting\n",
    "if dive_detection_settings[\"apply_temp_correction\"]:\n",
    "    corrected_depth = corrected_depth_temp\n",
    "else:\n",
    "    corrected_depth = corrected_depth_no_temp\n",
    "\n",
    "# Detect dives in the corrected depth data\n",
    "dive_detection_params = {\n",
    "    \"depth_series\": corrected_depth,\n",
    "    \"datetime_data\": depth_downsampled_datetime,\n",
    "    \"min_depth_threshold\": dive_detection_settings[\"min_depth_threshold\"],\n",
    "    \"sampling_rate\": dive_detection_settings[\"downsampled_sampling_rate\"],\n",
    "    \"duration_threshold\": dive_detection_settings[\"dive_duration_threshold\"],\n",
    "    \"smoothing_window\": dive_detection_settings[\"smoothing_window\"]\n",
    "}\n",
    "\n",
    "flat_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_datetime = data_pkl.sensor_data['pressure']['datetime']\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['pressure', 'ecg'],\n",
    "    time_range=(pressure_datetime.min(), pressure_datetime.max()),\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives = find_dives(**dive_detection_params)\n",
    "corrected_depth = enforce_surface_before_after_dives(corrected_depth, depth_downsampled_datetime, dives)\n",
    "\n",
    "# Calculate dive duration in seconds\n",
    "dives['dive_duration'] = (dives['end_time'] - dives['start_time']).dt.total_seconds()\n",
    "\n",
    "# Log transformations\n",
    "transformation_log = [\n",
    "    f\"downsampled_{dive_detection_settings['downsampled_sampling_rate']}Hz\",\n",
    "    f\"smoothed_{dive_detection_settings['smoothing_window']}s\",\n",
    "    f\"ZOC_settings__first_deriv_threshold_{dive_detection_settings['first_deriv_threshold']}mps__min_duration_{dive_detection_settings['min_duration']}s__depth_threshold_{dive_detection_settings['depth_threshold']}m\",\n",
    "    f\"DIVE_detection_settings__min_depth_threshold_{dive_detection_settings['min_depth_threshold']}m__dive_duration_threshold_{dive_detection_settings['dive_duration_threshold']}s__smoothing_window_{dive_detection_settings['smoothing_window']}\"\n",
    "]\n",
    "\n",
    "# Outputs\n",
    "print(f\"✅ {len(flat_chunks)} surface intervals detected.\")\n",
    "print(f\"✅ {len(dives)} dives detected.\")\n",
    "print(\"📖 Transformation Log:\", transformation_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and update dive events\n",
    "data_pkl.event_data = create_state_event(\n",
    "    state_df=dives,\n",
    "    key='dive',\n",
    "    value_column='max_depth',\n",
    "    start_time_column='start_time',\n",
    "    duration_column='dive_duration', # in seconds\n",
    "    description='dive_start',\n",
    "    existing_events=data_pkl.event_data  # Pass existing events for overwrite and concatenation\n",
    ")\n",
    "\n",
    "# Update event_info with unique keys\n",
    "data_pkl.event_info = list(data_pkl.event_data['key'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the derived_from_sensors list\n",
    "derived_from_sensors = [\"pressure\"]\n",
    "original_name = 'Temp-corrected Depth (m)' if dive_detection_settings[\"apply_temp_correction\"] else 'Corrected Depth (m)'\n",
    "\n",
    "# Save the corrected depth back to the data structure\n",
    "depth_df = pd.DataFrame({\n",
    "    'datetime': depth_downsampled_datetime,\n",
    "    'depth': corrected_depth\n",
    "})\n",
    "derived_info = {\n",
    "    \"channels\": [\"depth\"],\n",
    "    \"metadata\": {\n",
    "            'depth': {'original_name': original_name,\n",
    "                    'unit': 'm',\n",
    "                    'sensor': 'pressure'}\n",
    "    },\n",
    "    \"derived_from_sensors\": derived_from_sensors.append(\"temperature\") if dive_detection_settings[\"apply_temp_correction\"] else derived_from_sensors,\n",
    "    \"transformation_log\": transformation_log.append(\"temperature_correction\") if dive_detection_settings[\"apply_temp_correction\"] else derived_from_sensors\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['depth'] = depth_df\n",
    "data_pkl.derived_info['depth'] = derived_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve necessary time settings from the settings section\n",
    "time_settings = config_manager.get_from_config(\n",
    "    [\"overlap_start_time\", \"overlap_end_time\", \"zoom_window_start_time\", \"zoom_window_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "OVERLAP_START_TIME = time_settings.get(\"overlap_start_time\")\n",
    "OVERLAP_END_TIME = time_settings.get(\"overlap_end_time\")\n",
    "ZOOM_START_TIME = time_settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = time_settings.get(\"zoom_window_end_time\")\n",
    "\n",
    "# Confirm the values or raise an error if any are missing\n",
    "if None in {OVERLAP_START_TIME, OVERLAP_END_TIME, ZOOM_START_TIME, ZOOM_END_TIME}:\n",
    "    raise ValueError(\"One or more required time values were not found in the config file.\")\n",
    "\n",
    "# Display the loaded values\n",
    "print(\"OVERLAP_START_TIME:\", OVERLAP_START_TIME)\n",
    "print(\"OVERLAP_END_TIME:\", OVERLAP_END_TIME)\n",
    "print(\"ZOOM_START_TIME:\", ZOOM_START_TIME)\n",
    "print(\"ZOOM_END_TIME:\", ZOOM_END_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['pressure'],\n",
    "    derived_data_signals=['depth'],\n",
    "    time_range=(depth_downsampled_datetime.min(), depth_downsampled_datetime.max()),\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "exporter = BaseExporter(data_pkl) # Create a BaseExporter instance using data pickle object\n",
    "netcdf_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step01.nc') # Define the export path\n",
    "exporter.save_to_netcdf(data_pkl, filepath=netcdf_file_path) # Save to NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step01.nc')\n",
    "\n",
    "# Open the NetCDF file\n",
    "data = xr.open_dataset(netcdf_path)\n",
    "\n",
    "# Display the contents of the NetCDF file\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically generate and print the statement\n",
    "print(f\"`{', '.join(derived_from_sensors)}` sensor data was transformed into derived data `depth` by applying these transformations: {', '.join(transformation_log)}\")\n",
    "current_processing_step = \"Processing Step 01. Calibration of pressure sensor complete and dives analyzed.\"\n",
    "print(current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or update the current_processing_step for the specified deployment\n",
    "print(current_processing_step)\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
