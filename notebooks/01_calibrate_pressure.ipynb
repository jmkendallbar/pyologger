{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero offset correction: calibrate pressure sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.io_operations.base_exporter import *\n",
    "from pyologger.analyze_data.find_segments import *\n",
    "\n",
    "dataset_id = \"nesc-adult-hi-monk-seal_dive-imu_SR-MB\"\n",
    "# deployment_id = \"2018-04-18_nesc-001\"\n",
    "deployment_id = \"2019-02-04_nesc-006\"\n",
    "# deployment_id = \"2018-08-31_nesc-005\"\n",
    "# deployment_id = \"2018-04-19_nesc-002\"\n",
    "\n",
    "# dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "# deployment_id = \"2023-10-26_oror-001\"\n",
    "\n",
    "# dataset_id = \"mian-juv-nese_sleep_lml-ano_JKB\"\n",
    "# deployment_id = \"2019-10-25_mian-001\"\n",
    "# deployment_id = \"2021-04-17_mian-011\"\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, param_manager = select_and_load_deployment(\n",
    "    data_dir, dataset_id=dataset_id, deployment_id=deployment_id\n",
    "    )\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 01 IN PROGRESS.\"\n",
    "param_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dives\n",
    "Involves a zero offset correction with `zoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?smooth_downsample_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that units are in meters or convert if necessary\n",
    "original_pressure_unit = data_pkl.sensor_info['pressure']['original_units']\n",
    "pressure_unit = data_pkl.sensor_info['pressure']['units']\n",
    "\n",
    "if original_pressure_unit == 'bar' and pressure_unit != 'm': # if bar to m and hasn't been converted yet\n",
    "    print(\"Converting pressure from bar to m\")\n",
    "    data_pkl.sensor_data['pressure']['pressure'] *= 10\n",
    "    data_pkl.sensor_info['pressure']['units'] = 'm'\n",
    "    print(\"âœ… Pressure unit changed from bar to m\")\n",
    "elif original_pressure_unit in ['m', '100bar_1', '30bar_1', 'msw']: # including CATS format weird 100bar_1 which seems to be m\n",
    "    print(\"âœ… Pressure unit already in m\")\n",
    "    data_pkl.sensor_info['pressure']['units'] = 'm'\n",
    "    pass\n",
    "else:\n",
    "    print(f\"Unknown pressure unit: {pressure_unit}\")\n",
    "    raise ValueError(f\"Unknown pressure unit: {pressure_unit}\")\n",
    "\n",
    "new_pressure_unit = data_pkl.sensor_info['pressure']['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check if logger is known to produce extreme pressure values\n",
    "if data_pkl.sensor_info['pressure']['logger_manufacturer'] == 'Evolocus':\n",
    "    # 2. Check if logger_restart events have already been added\n",
    "    if data_pkl.event_data is None or data_pkl.event_data.empty or not any(data_pkl.event_data['key'] == 'logger_restart'):\n",
    "        # 3. Identify bad segments based on unrealistic negative pressure\n",
    "        pressure_df = data_pkl.sensor_data['pressure'].copy()\n",
    "        restarts = find_segments(\n",
    "            data=pressure_df,\n",
    "            column='pressure',\n",
    "            criteria=lambda x: x < -500,\n",
    "            min_duration=None\n",
    "        )\n",
    "\n",
    "        # 4. Add restart events using standardized event creation\n",
    "        if not restarts.empty:\n",
    "            data_pkl.event_data = create_state_event(\n",
    "                state_df=restarts,\n",
    "                key=\"logger_restart\",\n",
    "                start_time_column=\"start_datetime\",\n",
    "                duration_column=\"duration\",\n",
    "                description=\"Detected logger restart from extreme pressure\",\n",
    "                long_description=\"Logger restart inferred from pressure values dropping below -500, typically inserted by logger hardware during reboot.\",\n",
    "                existing_events=data_pkl.event_data\n",
    "            )\n",
    "            print(f\"ðŸŸ  Added {len(restarts)} logger_restart event(s) to event_data.\")\n",
    "\n",
    "            # 5. Replace pressure values with NaN around each segment\n",
    "            pressure_series = data_pkl.sensor_data['pressure']\n",
    "            datetimes = pressure_series['datetime']\n",
    "            for _, row in restarts.iterrows():\n",
    "                start = row['start_datetime']\n",
    "                end = row['end_datetime']\n",
    "                mask = (datetimes >= start) & (datetimes <= end)\n",
    "                buffer_before = datetimes.shift(1)\n",
    "                buffer_after = datetimes.shift(-1)\n",
    "                buffer_mask = (buffer_before >= start) & (buffer_before <= end) | (buffer_after >= start) & (buffer_after <= end)\n",
    "                full_mask = mask | buffer_mask\n",
    "                data_pkl.sensor_data['pressure'].loc[full_mask, 'pressure'] = np.nan\n",
    "            print(\"âš ï¸ Replaced extreme pressure values (< -500) and surrounding buffer with NaN.\")\n",
    "        else:\n",
    "            print(\"âœ… No restart segments detected.\")\n",
    "    else:\n",
    "        print(\"âœ… Logger restart events already exist in event_data.\")\n",
    "\n",
    "    # 6. Check again in case any extreme values remain outside known segments\n",
    "    extreme_exists = (data_pkl.sensor_data['pressure']['pressure'] < -500).any()\n",
    "    if extreme_exists:\n",
    "        data_pkl.sensor_data['pressure'].loc[\n",
    "            data_pkl.sensor_data['pressure']['pressure'] < -500, 'pressure'\n",
    "        ] = np.nan\n",
    "        print(\"âš ï¸ Replaced residual extreme pressure values (< -500) with NaN.\")\n",
    "    else:\n",
    "        print(\"âœ… No extreme pressure values found.\")\n",
    "else:\n",
    "    print(\"âœ… No logger restart check needed for this logger.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_data['pressure']['pressure'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load depth and temperature data\n",
    "depth_data = data_pkl.sensor_data[\"pressure\"][\"pressure\"]\n",
    "depth_datetime = data_pkl.sensor_data[\"pressure\"][\"datetime\"]\n",
    "depth_fs = data_pkl.sensor_info[\"pressure\"][\"sampling_frequency\"]\n",
    "if 'temperature-ext' in data_pkl.sensor_data:\n",
    "    temp_data = data_pkl.sensor_data['temperature-ext']['temp-ext']\n",
    "    temp_fs = data_pkl.sensor_info['temperature-ext']['sampling_frequency']\n",
    "elif 'temperature-int' in data_pkl.sensor_data:\n",
    "    temp_data = data_pkl.sensor_data['temperature-int']['temp-int']\n",
    "    temp_fs = data_pkl.sensor_info['temperature-int']['sampling_frequency']\n",
    "else:\n",
    "    temp_data = None\n",
    "    temp_fs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.sensor_data['pressure']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(depth_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 2: Load Configuration Parameters**\n",
    "dive_detection_settings = param_manager.get_from_config(\n",
    "    variable_names=[\n",
    "        \"first_deriv_threshold\", \"min_duration\", \"depth_threshold\",\n",
    "        \"apply_temp_correction\", \"min_depth_threshold\", \"dive_duration_threshold\",\n",
    "        \"smoothing_window\", \"downsampled_sampling_rate\", \"baseline_adjust\"\n",
    "    ],\n",
    "    section=\"dive_detection_settings\"\n",
    ")\n",
    "\n",
    "# Default settings\n",
    "default_settings = {\n",
    "    \"first_deriv_threshold\": 0.1,       # Threshold for the first derivative of depth (meters/second)\n",
    "    \"min_duration\": 60,                 # Minimum duration for a surface interval (seconds)\n",
    "    \"depth_threshold\": 5,               # Maximum depth to qualify as a surface interval (meters)\n",
    "    \"apply_temp_correction\": False,     # Apply temperature correction during zero offset correction (True/False)\n",
    "    \"min_depth_threshold\": 0.5,         # Minimum depth to start/end a dive (meters)\n",
    "    \"dive_duration_threshold\": 10,      # Minimum duration for a dive (seconds)\n",
    "    \"smoothing_window\": 5,              # Smoothing window\n",
    "    \"downsampled_sampling_rate\": 1,     # Target sampling rate after downsampling (default 1Hz)\n",
    "    \"baseline_adjust\": 0.0              # Adjust baseline depth data if needs to be raised or lowered\n",
    "}\n",
    "\n",
    "# If settings are missing or None, initialize them\n",
    "if dive_detection_settings is None:\n",
    "    dive_detection_settings = default_settings\n",
    "elif any(v is None for v in dive_detection_settings.values()):  \n",
    "    # Fill in only missing/None values\n",
    "    dive_detection_settings = {k: v if dive_detection_settings.get(k) is not None else default_settings[k] for k, v in dive_detection_settings.items()}\n",
    "    # Print to confirm\n",
    "    print(f\"âœ… Missing configurations were added.\")\n",
    "\n",
    "# Save if changes were made\n",
    "param_manager.add_to_config(entries=dive_detection_settings, section=\"dive_detection_settings\")\n",
    "\n",
    "# Step 6: Process depth data - Downsample, smooth, adjust baseline, and calculate first derivative\n",
    "# Interpolate NaNs for processing\n",
    "interpolated_depth_data = depth_data.interpolate(limit_direction='both')\n",
    "\n",
    "depth_processing_params = {\n",
    "    \"original_sampling_rate\": depth_fs,\n",
    "    \"downsampled_sampling_rate\": int(dive_detection_settings[\"downsampled_sampling_rate\"]),\n",
    "    \"baseline_adjust\": dive_detection_settings[\"baseline_adjust\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivative, downsampled_depth = smooth_downsample_derivative(interpolated_depth_data, **depth_processing_params)\n",
    "# Adjust datetime indexing based on the new downsample rate\n",
    "depth_downsampled_datetime = depth_datetime.iloc[::int(depth_fs / dive_detection_settings[\"downsampled_sampling_rate\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure indexing does not go out of bounds\n",
    "if len(depth_downsampled_datetime) > len(downsampled_depth):\n",
    "    depth_downsampled_datetime = depth_downsampled_datetime[:len(downsampled_depth)]\n",
    "\n",
    "# Print summary of processing\n",
    "print(f\"âœ… Depth processing complete: Downsampled to {dive_detection_settings['downsampled_sampling_rate']} Hz\")\n",
    "print(f\"âœ… Baseline adjustment applied: {dive_detection_settings['baseline_adjust']} meters\")\n",
    "\n",
    "# Detect flat chunks (potential surface intervals)\n",
    "flat_chunk_params = {\n",
    "    \"depth\": downsampled_depth,\n",
    "    \"datetime_data\": depth_downsampled_datetime,\n",
    "    \"first_derivative\": first_derivative,\n",
    "    \"threshold\": dive_detection_settings[\"first_deriv_threshold\"],\n",
    "    \"min_duration\": dive_detection_settings[\"min_duration\"],\n",
    "    \"depth_threshold\": dive_detection_settings[\"depth_threshold\"],\n",
    "    \"original_sampling_rate\": depth_fs,\n",
    "    \"downsampled_sampling_rate\": dive_detection_settings[\"downsampled_sampling_rate\"]\n",
    "}\n",
    "flat_chunks = detect_flat_chunks(**flat_chunk_params)\n",
    "flat_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply zero offset correction\n",
    "zoc_params = {\n",
    "    \"depth\": downsampled_depth,\n",
    "    \"temp\": temp_data.values if temp_data is not None else None,\n",
    "    \"flat_chunks\": flat_chunks\n",
    "}\n",
    "corrected_depth_temp, corrected_depth_no_temp, depth_correction = apply_zero_offset_correction(**zoc_params)\n",
    "\n",
    "corrected_depth = corrected_depth_temp if dive_detection_settings[\"apply_temp_correction\"] else corrected_depth_no_temp\n",
    "\n",
    "# Detect dives using find_segments\n",
    "depth_df = pd.DataFrame({\n",
    "    'datetime': depth_downsampled_datetime,\n",
    "    'depth': corrected_depth\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(corrected_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives = find_segments(\n",
    "    data=depth_df,\n",
    "    column='depth',\n",
    "    criteria=lambda x: x > dive_detection_settings['min_depth_threshold'],\n",
    "    min_duration=dive_detection_settings['dive_duration_threshold'],\n",
    ")\n",
    "\n",
    "nan_mask = depth_data.isna().reindex(depth_downsampled_datetime.index, method='nearest')\n",
    "dives['has_nans'] = dives.apply(lambda row: nan_mask.loc[(depth_downsampled_datetime >= row['start_datetime']) & (depth_downsampled_datetime <= row['end_datetime'])].any(), axis=1)\n",
    "dives['short_description'] = dives['has_nans'].apply(lambda x: 'dive-with-nan_start' if x else 'dive_start')\n",
    "\n",
    "corrected_depth = enforce_surface_before_after_dives(corrected_depth, depth_downsampled_datetime, dives)\n",
    "\n",
    "dives['dive_duration'] = (dives['end_datetime'] - dives['start_datetime']).dt.total_seconds()\n",
    "\n",
    "transformation_log = [\n",
    "    f\"downsampled_{dive_detection_settings['downsampled_sampling_rate']}Hz\",\n",
    "    f\"smoothed_{dive_detection_settings['smoothing_window']}s\",\n",
    "    f\"ZOC_settings__first_deriv_threshold_{dive_detection_settings['first_deriv_threshold']}mps__min_duration_{dive_detection_settings['min_duration']}s__depth_threshold_{dive_detection_settings['depth_threshold']}m\",\n",
    "    f\"DIVE_detection_settings__min_depth_threshold_{dive_detection_settings['min_depth_threshold']}m__dive_duration_threshold_{dive_detection_settings['dive_duration_threshold']}s__smoothing_window_{dive_detection_settings['smoothing_window']}\"\n",
    "]\n",
    "\n",
    "print(f\"âœ… {len(flat_chunks)} surface intervals detected.\")\n",
    "print(f\"âœ… {len(dives)} dives detected.\")\n",
    "print(\"ðŸ“– Transformation Log:\", transformation_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.analyze_data.analyze_segments import *\n",
    "# Append max depth for each dive segment\n",
    "dives = append_stats(\n",
    "    data=depth_df, \n",
    "    segment_df=dives, \n",
    "    statistics=[(\"max\", \"depth\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and update dive events\n",
    "data_pkl.event_data = create_state_event(\n",
    "    state_df=dives,\n",
    "    key='dive',\n",
    "    value_column='depth_max',\n",
    "    start_time_column='start_datetime',\n",
    "    duration_column='dive_duration', # in seconds\n",
    "    description='dive_start',\n",
    "    existing_events=data_pkl.event_data  # Pass existing events for overwrite and concatenation\n",
    ")\n",
    "\n",
    "# Update event_info with unique keys\n",
    "data_pkl.event_info = list(data_pkl.event_data['key'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the derived_from_sensors list\n",
    "derived_from_sensors = [\"pressure\"]\n",
    "original_name = 'Temp-corrected Depth (m)' if dive_detection_settings[\"apply_temp_correction\"] else 'Corrected Depth (m)'\n",
    "\n",
    "# Save the corrected depth back to the data structure\n",
    "depth_df = pd.DataFrame({\n",
    "    'datetime': depth_downsampled_datetime,\n",
    "    'depth': corrected_depth\n",
    "})\n",
    "derived_info = {\n",
    "    \"channels\": [\"depth\"],\n",
    "    \"metadata\": {\n",
    "            'depth': {'original_name': original_name,\n",
    "                    'unit': 'm',\n",
    "                    'sensor': 'pressure'}\n",
    "    },\n",
    "    \"derived_from_sensors\": derived_from_sensors.append(\"temperature\") if dive_detection_settings[\"apply_temp_correction\"] else derived_from_sensors,\n",
    "    \"transformation_log\": transformation_log.append(\"temperature_correction\") if dive_detection_settings[\"apply_temp_correction\"] else derived_from_sensors\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['depth'] = depth_df\n",
    "data_pkl.derived_info['depth'] = derived_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve necessary time settings from the settings section\n",
    "time_settings = param_manager.get_from_config(\n",
    "    [\"overlap_start_time\", \"overlap_end_time\", \"zoom_window_start_time\", \"zoom_window_end_time\"],\n",
    "    section=\"settings\"\n",
    ")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "OVERLAP_START_TIME = time_settings.get(\"overlap_start_time\")\n",
    "OVERLAP_END_TIME = time_settings.get(\"overlap_end_time\")\n",
    "ZOOM_START_TIME = time_settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = time_settings.get(\"zoom_window_end_time\")\n",
    "\n",
    "# Confirm the values or raise an error if any are missing\n",
    "if None in {OVERLAP_START_TIME, OVERLAP_END_TIME, ZOOM_START_TIME, ZOOM_END_TIME}:\n",
    "    raise ValueError(\"One or more required time values were not found in the config file.\")\n",
    "\n",
    "# Display the loaded values\n",
    "print(\"OVERLAP_START_TIME:\", OVERLAP_START_TIME)\n",
    "print(\"OVERLAP_END_TIME:\", OVERLAP_END_TIME)\n",
    "print(\"ZOOM_START_TIME:\", ZOOM_START_TIME)\n",
    "print(\"ZOOM_END_TIME:\", ZOOM_END_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['pressure'],\n",
    "    derived_data_signals=['depth'],\n",
    "    time_range=(depth_downsampled_datetime.min(), depth_downsampled_datetime.max()),\n",
    "    note_annotations={\"dive\": {\"signal\": \"depth\", \"symbol\": \"triangle-down\", \"color\": \"blue\"}},\n",
    "    state_annotations={\"dive\": {\"signal\": \"depth\", \"color\": \"rgba(150, 150, 150, 0.3)\"}},\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=1\n",
    ")\n",
    "fig.show_dash(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "exporter = BaseExporter(data_pkl) # Create a BaseExporter instance using data pickle object\n",
    "netcdf_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step01.nc') # Define the export path\n",
    "exporter.save_to_netcdf(data_pkl, filepath=netcdf_file_path) # Save to NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step01.nc')\n",
    "\n",
    "# Open the NetCDF file\n",
    "data = xr.open_dataset(netcdf_path)\n",
    "\n",
    "# Display the contents of the NetCDF file\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically generate and print the statement\n",
    "print(f\"`{', '.join(derived_from_sensors)}` sensor data was transformed into derived data `depth` by applying these transformations: {', '.join(transformation_log)}\")\n",
    "current_processing_step = \"Processing Step 01. Calibration of pressure sensor complete and dives analyzed.\"\n",
    "print(current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or update the current_processing_step for the specified deployment\n",
    "print(current_processing_step)\n",
    "param_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
