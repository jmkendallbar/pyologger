{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag to animal frame: re-orienting tag to match animal's axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tutorial my friend Max developed to help with this headache: https://flukeandfeather.com/posts/2024-08-30-animal-orientation-with-imu-ta/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.io_operations.base_exporter import *\n",
    "from pyologger.utils.data_manager import *\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "\n",
    "dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "deployment_id = \"2023-10-26_oror-001\"\n",
    "\n",
    "# dataset_id = \"mian-juv-nese_sleep_lml-ano_JKB\"\n",
    "# deployment_id = \"2019-10-25_mian-001\"\n",
    "# deployment_id = \"2021-04-17_mian-011\"\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, param_manager = select_and_load_deployment(data_dir, dataset_id=dataset_id, deployment_id=deployment_id)\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 03 IN PROGRESS.\"\n",
    "param_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = data_pkl.derived_data['calibrated_acc']\n",
    "mag_df = data_pkl.derived_data['calibrated_mag']\n",
    "gyr_df = data_pkl.sensor_data['gyroscope']\n",
    "\n",
    "# Calculate and print sampling frequency for each dataframe\n",
    "acc_fs = calculate_sampling_frequency(acc_df['datetime'].head())\n",
    "print(f\"Accelerometer Sampling frequency: {acc_fs} Hz\")\n",
    "\n",
    "mag_fs = calculate_sampling_frequency(mag_df['datetime'].head())\n",
    "print(f\"Magnetometer Sampling frequency: {mag_fs} Hz\")\n",
    "\n",
    "gyr_fs = calculate_sampling_frequency(gyr_df['datetime'].head())\n",
    "print(f\"Gyroscope Sampling frequency: {gyr_fs} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = acc_df[['ax','ay','az']]\n",
    "mag_data = mag_df[['mx','my','mz']]\n",
    "gyr_data = gyr_df[['gx', 'gy', 'gz']]\n",
    "\n",
    "if gyr_fs < acc_fs:\n",
    "    upsampled_columns = []\n",
    "    for col in gyr_data.columns:\n",
    "        upsampled_col = upsample(gyr_data[col].values, acc_fs / gyr_fs, len(acc_data))  # Apply upsample to each column\n",
    "        upsampled_columns.append(upsampled_col)  # Append the upsampled column to the list\n",
    "\n",
    "    # Combine the upsampled columns back into a NumPy array\n",
    "    gyr_data_upsampled = np.column_stack(upsampled_columns)\n",
    "    gyr_data = gyr_data_upsampled\n",
    "    print(f\"Gyroscope data upsampled to match accelerometer length: gyr_data shape= {gyr_data.shape}\")\n",
    "else:\n",
    "    gyr_data = gyr_data.values\n",
    "    print(f\"Gyroscope data matches accelerometer length: gyr_data shape= {gyr_data.shape}\")\n",
    "\n",
    "acc_data = acc_data.values\n",
    "mag_data = mag_data.values\n",
    "\n",
    "# Assuming acc_data and mag_data_upsampled are NumPy arrays\n",
    "print(f\"Gyroscope matches accelerometer length: acc_data shape= {acc_data.shape}, upsampled gyr_data shape = {gyr_data.shape}\")\n",
    "sampling_rate = acc_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?orientation_and_heading_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve values from config\n",
    "variables = [\"calm_horizontal_start_time\", \"calm_horizontal_end_time\", \n",
    "             \"zoom_window_start_time\", \"zoom_window_end_time\", \n",
    "             \"overlap_start_time\", \"overlap_end_time\"]\n",
    "settings = param_manager.get_from_config(variables, section=\"settings\")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "CALM_HORIZONTAL_START_TIME = settings.get(\"calm_horizontal_start_time\")\n",
    "CALM_HORIZONTAL_END_TIME = settings.get(\"calm_horizontal_end_time\")\n",
    "ZOOM_START_TIME = settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = settings.get(\"zoom_window_end_time\")\n",
    "OVERLAP_START_TIME = settings.get(\"overlap_start_time\")\n",
    "OVERLAP_END_TIME = settings.get(\"overlap_end_time\")\n",
    "\n",
    "# Check if manual update is required for CALM_HORIZONTAL_START_TIME and CALM_HORIZONTAL_END_TIME\n",
    "requires_manual_update = False\n",
    "if not CALM_HORIZONTAL_START_TIME or CALM_HORIZONTAL_START_TIME == \"PLACEHOLDER\":\n",
    "    requires_manual_update = True\n",
    "if not CALM_HORIZONTAL_END_TIME or CALM_HORIZONTAL_END_TIME == \"PLACEHOLDER\":\n",
    "    requires_manual_update = True\n",
    "\n",
    "# Use ZOOM_WINDOW values as defaults if CALM_HORIZONTAL times are placeholders\n",
    "if requires_manual_update:\n",
    "    CALM_HORIZONTAL_START_TIME = CALM_HORIZONTAL_START_TIME or ZOOM_START_TIME\n",
    "    CALM_HORIZONTAL_END_TIME = CALM_HORIZONTAL_END_TIME or ZOOM_END_TIME\n",
    "\n",
    "# Display values to the user\n",
    "print(\"CALM_HORIZONTAL_START_TIME (current or default):\", CALM_HORIZONTAL_START_TIME)\n",
    "print(\"CALM_HORIZONTAL_END_TIME (current or default):\", CALM_HORIZONTAL_END_TIME)\n",
    "\n",
    "# Display a message based on whether manual update is needed\n",
    "if requires_manual_update:\n",
    "    print(\"Calm horizontal start and end times require manual update. Proceed to Cell 2 to set placeholders.\")\n",
    "else:\n",
    "    print(\"Calm horizontal start and end times are already set. No further action is required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve timezone from deployment info\n",
    "timezone = data_pkl.deployment_info['Time Zone']\n",
    "\n",
    "# Define placeholder timestamps for calm period in the retrieved timezone\n",
    "placeholder_start_time = pd.Timestamp(\"2024-01-16 09:58:10\").tz_localize(timezone)\n",
    "placeholder_end_time = pd.Timestamp(\"2024-01-16 09:58:24\").tz_localize(timezone)\n",
    "\n",
    "# Set this to True if we want to override the placeholders regardless of manual update status\n",
    "override_required = False\n",
    "\n",
    "# Only update if manual update is required or if override is enabled\n",
    "if requires_manual_update or override_required:\n",
    "    CALM_HORIZONTAL_START_TIME = str(placeholder_start_time)\n",
    "    CALM_HORIZONTAL_END_TIME = str(placeholder_end_time)\n",
    "    \n",
    "    # Use ParamManager to add placeholders to the config\n",
    "    param_manager.add_to_config(\"calm_horizontal_start_time\",\n",
    "        value=CALM_HORIZONTAL_START_TIME,\n",
    "        section=\"settings\"\n",
    "    )\n",
    "    param_manager.add_to_config(\"calm_horizontal_end_time\",\n",
    "        value=CALM_HORIZONTAL_END_TIME,\n",
    "        section=\"settings\"\n",
    "    )\n",
    "\n",
    "    print(\"Timestamps for calm horizontal start and end times have been set and saved.\")\n",
    "else:\n",
    "    print(\"Manual update not required. Placeholders were not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.derived_data['calibrated_acc']['datetime'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.derived_data['calibrated_acc']['datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = CALM_HORIZONTAL_START_TIME\n",
    "end_time   = CALM_HORIZONTAL_END_TIME\n",
    "\n",
    "# Filter the DataFrame to include only rows within the specified time range\n",
    "accelerometer_df = data_pkl.sensor_data['accelerometer']\n",
    "filtered_df = accelerometer_df[(accelerometer_df['datetime'] >= start_time) & (accelerometer_df['datetime'] <= end_time)]\n",
    "\n",
    "# Calculate the mean of ax, ay, and az columns within the filtered time range\n",
    "mean_values = filtered_df[['ax', 'ay', 'az']].mean()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Average values between {start_time} and {end_time}:\")\n",
    "print(mean_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?param_manager.add_to_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abar0 = [mean_values['ax'], mean_values['ay'], mean_values['az']]\n",
    "# abar0 = [0, 0, -9.8] # to override - use this or similar for orca and other standard CATS tags\n",
    "deploy_latitude = data_pkl.deployment_info[\"Deployment Latitude\"]\n",
    "deploy_longitude = data_pkl.deployment_info[\"Deployment Longitude\"]\n",
    "\n",
    "print(f\"Using location Lat: {deploy_latitude}, Lon: {deploy_longitude} and stationary readings of abar0: {str(abar0)} to orient tag.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to get corrected orientation and heading for the entire dataset\n",
    "pitch_deg, roll_deg, heading_deg, corrected_acc, corrected_mag, corrected_gyr = orientation_and_heading_correction(\n",
    "    abar0, \n",
    "    latitude= deploy_latitude,\n",
    "    longitude= deploy_longitude,\n",
    "    acc_data=acc_data, \n",
    "    mag_data=mag_data, \n",
    "    gyr_data=gyr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple key-value pairs to add under a section\n",
    "settings_to_add = {\n",
    "    \"declination_latitude\": deploy_latitude,\n",
    "    \"declination_longitude\": deploy_longitude,\n",
    "    \"abar0\": ', '.join(f\"{value:.3f}\" for value in mean_values)\n",
    "}\n",
    "\n",
    "# Add the settings under a specific section\n",
    "param_manager.add_to_config(entries=settings_to_add, section=\"03_tagtoanimal_settings\")\n",
    "\n",
    "print(f\"Tag to animal correction settings saved and added to config file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One datetime column from highest sampled data that was matched by other sensors\n",
    "datetime_data = data_pkl.sensor_data['accelerometer']['datetime']\n",
    "\n",
    "# Step 1: Create a DataFrame for pitch, roll, and heading\n",
    "prh_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'pitch': pitch_deg,\n",
    "    'roll': roll_deg,\n",
    "    'heading': heading_deg\n",
    "})\n",
    "\n",
    "# Store the 'prh' variable in derived_data\n",
    "data_pkl.derived_data['prh'] = prh_df\n",
    "data_pkl.derived_info['prh'] = {\n",
    "    \"channels\": [\"pitch\", \"roll\", \"heading\"],\n",
    "    \"metadata\": {\n",
    "        'pitch': {'original_name': 'Pitch (degrees)',\n",
    "                  'unit': 'degrees',\n",
    "                  'sensor': 'accelerometer'},\n",
    "        'roll': {'original_name': 'Roll (degrees)',\n",
    "                 'unit': 'degrees',\n",
    "                 'sensor': 'accelerometer'},\n",
    "        'heading': {'original_name': 'Heading (degrees)',\n",
    "                    'unit': 'degrees',\n",
    "                    'sensor': 'magnetometer'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"accelerometer\", \"magnetometer\"],\n",
    "    \"transformation_log\": [f\"calculated_pitch_roll_heading using abar0: {abar0} from calibration period with start time: {start_time} and end time: {end_time} at Deployment Latitude: {deploy_latitude} and Deployment Longitude: {deploy_longitude}.\"]\n",
    "}\n",
    "\n",
    "# Step 2: Create DataFrames for corrected accelerometer, magnetometer, and gyroscope data\n",
    "corrected_acc_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'ax': corrected_acc[:, 0],\n",
    "    'ay': corrected_acc[:, 1],\n",
    "    'az': corrected_acc[:, 2]\n",
    "})\n",
    "\n",
    "corrected_mag_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'mx': corrected_mag[:, 0],\n",
    "    'my': corrected_mag[:, 1],\n",
    "    'mz': corrected_mag[:, 2]\n",
    "})\n",
    "\n",
    "corrected_gyr_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'gx': corrected_gyr[:, 0],\n",
    "    'gy': corrected_gyr[:, 1],\n",
    "    'gz': corrected_gyr[:, 2]\n",
    "})\n",
    "\n",
    "# Step 3: Store the corrected accelerometer, magnetometer, and gyroscope data into derived_data\n",
    "data_pkl.derived_data['corrected_acc'] = corrected_acc_df\n",
    "data_pkl.derived_info['corrected_acc'] = {\n",
    "    \"channels\": [\"ax\", \"ay\", \"az\"],\n",
    "    \"metadata\": {\n",
    "        'ax': {'original_name': 'Acceleration X (m/s^2)',\n",
    "               'unit': 'm/s^2',\n",
    "               'sensor': 'accelerometer'},\n",
    "        'ay': {'original_name': 'Acceleration Y (m/s^2)',\n",
    "               'unit': 'm/s^2',\n",
    "               'sensor': 'accelerometer'},\n",
    "        'az': {'original_name': 'Acceleration Z (m/s^2)',\n",
    "               'unit': 'm/s^2',\n",
    "               'sensor': 'accelerometer'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"accelerometer\"],\n",
    "    \"transformation_log\": [\"corrected_orientation\"]\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['corrected_mag'] = corrected_mag_df\n",
    "data_pkl.derived_info['corrected_mag'] = {\n",
    "    \"channels\": [\"mx\", \"my\", \"mz\"],\n",
    "    \"metadata\": {\n",
    "        'mx': {'original_name': 'Magnetometer X (µT)',\n",
    "               'unit': 'µT',\n",
    "               'sensor': 'magnetometer'},\n",
    "        'my': {'original_name': 'Magnetometer Y (µT)',\n",
    "               'unit': 'µT',\n",
    "               'sensor': 'magnetometer'},\n",
    "        'mz': {'original_name': 'Magnetometer Z (µT)',\n",
    "               'unit': 'µT',\n",
    "               'sensor': 'magnetometer'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"magnetometer\"],\n",
    "    \"transformation_log\": [\"corrected_orientation\"]\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['corrected_gyr'] = corrected_gyr_df\n",
    "data_pkl.derived_info['corrected_gyr'] = {\n",
    "    \"channels\": [\"gx\", \"gy\", \"gz\"],\n",
    "    \"metadata\": {\n",
    "        'gx': {'original_name': 'Gyroscope X (mrad/s)',\n",
    "               'unit': 'mrad/s',\n",
    "               'sensor': 'gyroscope'},\n",
    "        'gy': {'original_name': 'Gyroscope Y (mrad/s)',\n",
    "               'unit': 'mrad/s',\n",
    "               'sensor': 'gyroscope'},\n",
    "        'gz': {'original_name': 'Gyroscope Z (mrad/s)',\n",
    "               'unit': 'mrad/s',\n",
    "               'sensor': 'gyroscope'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"gyroscope\"],\n",
    "    \"transformation_log\": [\"corrected_orientation\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 1\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'accelerometer', 'magnetometer'],\n",
    "    derived_data_signals=['depth', 'corrected_acc', 'corrected_mag', 'prh'],\n",
    "    channels={},\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "fig.show_dash(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_pkl.derived_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = ['calibrated_acc','calibrated_mag']\n",
    "\n",
    "# Clear the specified keys\n",
    "clear_intermediate_signals(data_pkl, remove_keys=keys_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "exporter = BaseExporter(data_pkl) # Create a BaseExporter instance using data pickle object\n",
    "netcdf_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step03.nc') # Define the export path\n",
    "exporter.save_to_netcdf(data_pkl, filepath=netcdf_file_path) # Save to NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 03. Tag frame to animal frame transformation complete.\"\n",
    "print(current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or update the current_processing_step for the specified deployment\n",
    "print(current_processing_step)\n",
    "param_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "# Open the NetCDF file\n",
    "nc_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step03.nc')\n",
    "dataset = xarray.open_dataset(nc_file_path)\n",
    "\n",
    "# Display the dataset\n",
    "display(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
