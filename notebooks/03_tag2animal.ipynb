{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag to animal frame: re-orienting tag to match animal's axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tutorial my friend Max developed to help with this headache: https://flukeandfeather.com/posts/2024-08-30-animal-orientation-with-imu-ta/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(deployment_db, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = data_pkl.derived_data['calibrated_acc']\n",
    "mag_df = data_pkl.derived_data['calibrated_mag']\n",
    "gyr_df = data_pkl.sensor_data['gyroscope']\n",
    "\n",
    "# Calculate and print sampling frequency for each dataframe\n",
    "acc_fs = calculate_sampling_frequency(acc_df['datetime'])\n",
    "print(f\"Accelerometer Sampling frequency: {acc_fs} Hz\")\n",
    "\n",
    "mag_fs = calculate_sampling_frequency(mag_df['datetime'])\n",
    "print(f\"Magnetometer Sampling frequency: {mag_fs} Hz\")\n",
    "\n",
    "gyr_fs = calculate_sampling_frequency(gyr_df['datetime'])\n",
    "print(f\"Gyroscope Sampling frequency: {gyr_fs} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_data = data_pkl.sensor_data['accelerometer'][['ax','ay','az']]\n",
    "# mag_data = data_pkl.sensor_data['magnetometer'][['mx', 'my', 'mz']]\n",
    "\n",
    "acc_data = acc_df[['ax','ay','az']]\n",
    "mag_data = mag_df[['mx','my','mz']]\n",
    "gyr_data = gyr_df[['gx', 'gy', 'gz']]\n",
    "\n",
    "upsampled_columns = []\n",
    "for col in gyr_data.columns:\n",
    "    upsampled_col = upsample(gyr_data[col].values, acc_fs / gyr_fs, len(acc_data))  # Apply upsample to each column\n",
    "    upsampled_columns.append(upsampled_col)  # Append the upsampled column to the list\n",
    "\n",
    "# Combine the upsampled columns back into a NumPy array\n",
    "gyr_data_upsampled = np.column_stack(upsampled_columns)\n",
    "gyr_data = gyr_data_upsampled\n",
    "\n",
    "acc_data = acc_data.values\n",
    "mag_data = mag_data.values\n",
    "\n",
    "# Assuming acc_data and mag_data_upsampled are NumPy arrays\n",
    "print(f\"Gyroscope upsampled to match accelerometer length: acc_data shape= {acc_data.shape}, upsampled gyr_data shape = {gyr_data.shape}\")\n",
    "sampling_rate = acc_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_pkl.deployment_info[\"Deployment Latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?orientation_and_heading_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the start and end datetimes with the correct timezone\n",
    "timezone = acc_df['datetime'].dt.tz  # Get the timezone of the datetime column\n",
    "\n",
    "# Find start and end time of calm period that tag is near horizontal\n",
    "start_time = pd.Timestamp(\"2019-11-08 09:18:33\").tz_localize(timezone) \n",
    "end_time = pd.Timestamp(\"2019-11-08 09:18:58\").tz_localize(timezone)\n",
    "\n",
    "# Filter the DataFrame to include only rows within the specified time range\n",
    "accelerometer_df = data_pkl.sensor_data['accelerometer']\n",
    "filtered_df = accelerometer_df[(accelerometer_df['datetime'] >= start_time) & (accelerometer_df['datetime'] <= end_time)]\n",
    "\n",
    "# Calculate the mean of ax, ay, and az columns within the filtered time range\n",
    "mean_values = filtered_df[['ax', 'ay', 'az']].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Average values between 09:18:33 and 09:18:58 on Nov 8, 2019:\")\n",
    "print(mean_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "abar0 = [-2.124303, -0.699504, -10.225013]\n",
    "# abar0 = [0,0, -9.8]\n",
    "deploy_latitude = data_pkl.deployment_info[\"Deployment Latitude\"]\n",
    "deploy_longitude = data_pkl.deployment_info[\"Deployment Longitude\"]\n",
    "\n",
    "# latitude= 32.764655 # Seaworld\n",
    "# longitude= -117.228585 # Seaworld\n",
    "# abar0 = [0, 0, -9.8] # FOR KILLER WHALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to get corrected orientation and heading for the entire dataset\n",
    "pitch_deg, roll_deg, heading_deg, corrected_acc, corrected_mag, corrected_gyr = orientation_and_heading_correction(\n",
    "    abar0, \n",
    "    latitude= deploy_latitude,\n",
    "    longitude= deploy_longitude,\n",
    "    acc_data=acc_data, \n",
    "    mag_data=mag_data, \n",
    "    gyr_data=gyr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One datetime column from highest sampled data that was matched by other sensors\n",
    "datetime_data = data_pkl.sensor_data['accelerometer']['datetime']\n",
    "\n",
    "# Step 1: Create a DataFrame for pitch, roll, and heading\n",
    "prh_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'pitch': pitch_deg,\n",
    "    'roll': roll_deg,\n",
    "    'heading': heading_deg\n",
    "})\n",
    "\n",
    "# Store the 'prh' variable in derived_data\n",
    "data_pkl.derived_data['prh'] = prh_df\n",
    "data_pkl.derived_info['prh'] = {\n",
    "    \"channels\": [\"pitch\", \"roll\", \"heading\"],\n",
    "    \"metadata\": {\n",
    "        'pitch': {'original_name': 'Pitch (degrees)',\n",
    "                  'unit': 'degrees',\n",
    "                  'sensor': 'accelerometer'},\n",
    "        'roll': {'original_name': 'Roll (degrees)',\n",
    "                 'unit': 'degrees',\n",
    "                 'sensor': 'accelerometer'},\n",
    "        'heading': {'original_name': 'Heading (degrees)',\n",
    "                    'unit': 'degrees',\n",
    "                    'sensor': 'magnetometer'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"accelerometer\", \"magnetometer\"],\n",
    "    \"transformation_log\": [\"calculated_pitch_roll_heading\"]\n",
    "}\n",
    "\n",
    "# Step 2: Create DataFrames for corrected accelerometer, magnetometer, and gyroscope data\n",
    "corrected_acc_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'ax': corrected_acc[:, 0],\n",
    "    'ay': corrected_acc[:, 1],\n",
    "    'az': corrected_acc[:, 2]\n",
    "})\n",
    "\n",
    "corrected_mag_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'mx': corrected_mag[:, 0],\n",
    "    'my': corrected_mag[:, 1],\n",
    "    'mz': corrected_mag[:, 2]\n",
    "})\n",
    "\n",
    "corrected_gyr_df = pd.DataFrame({\n",
    "    'datetime': datetime_data,\n",
    "    'gx': corrected_gyr[:, 0],\n",
    "    'gy': corrected_gyr[:, 1],\n",
    "    'gz': corrected_gyr[:, 2]\n",
    "})\n",
    "\n",
    "# Step 3: Store the corrected accelerometer, magnetometer, and gyroscope data into derived_data\n",
    "data_pkl.derived_data['corrected_acc'] = corrected_acc_df\n",
    "data_pkl.derived_info['corrected_acc'] = {\n",
    "    \"channels\": [\"ax\", \"ay\", \"az\"],\n",
    "    \"metadata\": {\n",
    "        'ax': {'original_name': 'Acceleration X (m/s^2)',\n",
    "               'unit': 'm/s^2',\n",
    "               'sensor': 'accelerometer'},\n",
    "        'ay': {'original_name': 'Acceleration Y (m/s^2)',\n",
    "               'unit': 'm/s^2',\n",
    "               'sensor': 'accelerometer'},\n",
    "        'az': {'original_name': 'Acceleration Z (m/s^2)',\n",
    "               'unit': 'm/s^2',\n",
    "               'sensor': 'accelerometer'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"accelerometer\"],\n",
    "    \"transformation_log\": [\"corrected_orientation\"]\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['corrected_mag'] = corrected_mag_df\n",
    "data_pkl.derived_info['corrected_mag'] = {\n",
    "    \"channels\": [\"mx\", \"my\", \"mz\"],\n",
    "    \"metadata\": {\n",
    "        'mx': {'original_name': 'Magnetometer X (µT)',\n",
    "               'unit': 'µT',\n",
    "               'sensor': 'magnetometer'},\n",
    "        'my': {'original_name': 'Magnetometer Y (µT)',\n",
    "               'unit': 'µT',\n",
    "               'sensor': 'magnetometer'},\n",
    "        'mz': {'original_name': 'Magnetometer Z (µT)',\n",
    "               'unit': 'µT',\n",
    "               'sensor': 'magnetometer'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"magnetometer\"],\n",
    "    \"transformation_log\": [\"corrected_orientation\"]\n",
    "}\n",
    "\n",
    "data_pkl.derived_data['corrected_gyr'] = corrected_gyr_df\n",
    "data_pkl.derived_info['corrected_gyr'] = {\n",
    "    \"channels\": [\"gx\", \"gy\", \"gz\"],\n",
    "    \"metadata\": {\n",
    "        'gx': {'original_name': 'Gyroscope X (deg/s)',\n",
    "               'unit': 'deg/s',\n",
    "               'sensor': 'gyroscope'},\n",
    "        'gy': {'original_name': 'Gyroscope Y (deg/s)',\n",
    "               'unit': 'deg/s',\n",
    "               'sensor': 'gyroscope'},\n",
    "        'gz': {'original_name': 'Gyroscope Z (deg/s)',\n",
    "               'unit': 'deg/s',\n",
    "               'sensor': 'gyroscope'}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"gyroscope\"],\n",
    "    \"transformation_log\": [\"corrected_orientation\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERLAP_START_TIME = '2019-11-08 09:00:00'\n",
    "# OVERLAP_END_TIME = '2019-11-08 11:30:00'\n",
    "# ZOOM_START_TIME = '2019-11-08 09:15:00'\n",
    "# ZOOM_END_TIME = '2019-11-08 09:30:00'\n",
    "\n",
    "\n",
    "OVERLAP_START_TIME = '2024-01-16 09:30:00'  # Start time for plotting\n",
    "OVERLAP_END_TIME = '2024-01-16 10:30:00'  # End time for plotting\n",
    "ZOOM_START_TIME = '2024-01-16 10:00:00'  # Start time for zooming\n",
    "ZOOM_END_TIME = '2024-01-16 10:02:30'  # End time for zooming\n",
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'sensor': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'sensor': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'sensor': 'ecg', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive5(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'accelerometer', 'magnetometer'],\n",
    "    derived_data_signals=['depth', 'corrected_acc', 'corrected_mag', 'prh'],\n",
    "    channels={},\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?DataReader.save_to_netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.save_to_netcdf('data/2019-11-08_apfo-001a/outputs/deployment_data_processed.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
