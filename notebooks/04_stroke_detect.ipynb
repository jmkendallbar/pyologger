{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.config_manager import ConfigManager\n",
    "from pyologger.utils.data_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.process_data.peak_detect import *\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 04 IN PROGRESS.\"\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "print(f\"Loaded pickle file from deployment ID: {deployment_id}\")\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve values from config\n",
    "variables = [\"calm_horizontal_start_time\", \"calm_horizontal_end_time\", \n",
    "             \"zoom_window_start_time\", \"zoom_window_end_time\", \n",
    "             \"earliest_common_start_time\", \"latest_common_end_time\"]\n",
    "settings = config_manager.get_from_config(variables, section=\"settings\")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "CALM_HORIZONTAL_START_TIME = settings.get(\"calm_horizontal_start_time\")\n",
    "CALM_HORIZONTAL_END_TIME = settings.get(\"calm_horizontal_end_time\")\n",
    "ZOOM_START_TIME = settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = settings.get(\"zoom_window_end_time\")\n",
    "OVERLAP_START_TIME = settings.get(\"earliest_common_start_time\")\n",
    "OVERLAP_END_TIME = settings.get(\"latest_common_end_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find time chunk when stroking is dominant activity\n",
    "Use interactive plot to locate a start time and end time when stroking is the dominant activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive5(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg'],\n",
    "    derived_data_signals=['depth', 'corrected_acc', 'corrected_gyr', 'prh'],\n",
    "    channels={},\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve timezone from deployment info\n",
    "timezone = data_pkl.deployment_info['Time Zone']\n",
    "\n",
    "# Define placeholder timestamps for calm period in the retrieved timezone\n",
    "stroking_start_time = pd.Timestamp(\"2024-01-16 10:03:10\").tz_localize(timezone)\n",
    "stroking_end_time = pd.Timestamp(\"2024-01-16 10:03:40\").tz_localize(timezone)\n",
    "\n",
    "# Use ConfigManager to add both stroking start and end times to the config in the desired section\n",
    "config_manager.add_to_config(\n",
    "    entries={\n",
    "        \"stroking_start_time\": str(stroking_start_time),\n",
    "        \"stroking_end_time\": str(stroking_end_time)\n",
    "    },\n",
    "    section=\"settings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE AS NEEDED\n",
    "\n",
    "detection_mode=\"stroke_rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parent signal options\n",
    "parent_signal_options = list(data_pkl.sensor_data.keys()) + list(data_pkl.derived_data.keys())\n",
    "default_parent_signal = \"ecg\" if detection_mode == \"heart_rate\" else \"corrected_gyr\"\n",
    "\n",
    "# User input for parent signal\n",
    "print(f\"Available parent signals: {parent_signal_options}\")\n",
    "parent_signal = input(f\"Choose parent signal (default: {default_parent_signal}): \").strip()\n",
    "if not parent_signal or parent_signal not in parent_signal_options:\n",
    "    parent_signal = default_parent_signal\n",
    "\n",
    "# Get available channels\n",
    "if parent_signal in data_pkl.sensor_data:\n",
    "    available_channels = list(data_pkl.sensor_data[parent_signal].columns)\n",
    "elif parent_signal in data_pkl.derived_data:\n",
    "    available_channels = list(data_pkl.derived_data[parent_signal].columns)\n",
    "else:\n",
    "    available_channels = []\n",
    "\n",
    "# Default channel\n",
    "default_channel = \"ecg\" if detection_mode == \"heart_rate\" else \"gy\"\n",
    "\n",
    "# User input for channel\n",
    "print(f\"Available channels: {available_channels}\")\n",
    "channel = input(f\"Choose channel (default: {default_channel}): \").strip()\n",
    "if not channel or channel not in available_channels:\n",
    "    channel = default_channel\n",
    "\n",
    "# Configure signals\n",
    "signal_df = data_pkl.sensor_data[parent_signal] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal]\n",
    "signal = data_pkl.sensor_data[parent_signal][channel] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal][channel]\n",
    "datetime_signal = data_pkl.sensor_data[parent_signal]['datetime'] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal]['datetime']\n",
    "sampling_rate = data_pkl.sensor_info.get(parent_signal, {}).get('sampling_frequency', calculate_sampling_frequency(datetime_signal))\n",
    "\n",
    "# Define the default time range based on the signal's datetime column\n",
    "signal_start = datetime_signal.min()\n",
    "signal_end = datetime_signal.max()\n",
    "\n",
    "# User input for time range\n",
    "print(f\"Signal time range: {signal_start} to {signal_end}\")\n",
    "start_time_input = input(f\"Enter start time (default: {signal_start}): \").strip()\n",
    "end_time_input = input(f\"Enter end time (default: {signal_end}): \").strip()\n",
    "\n",
    "# Determine time range based on user input\n",
    "start_datetime = pd.Timestamp(start_time_input) if start_time_input else signal_start\n",
    "end_datetime = pd.Timestamp(end_time_input) if end_time_input else signal_end\n",
    "\n",
    "# Filter signal based on the selected time range\n",
    "time_mask = (datetime_signal >= start_datetime) & (datetime_signal <= end_datetime)\n",
    "signal_subset = signal[time_mask]\n",
    "datetime_subset = datetime_signal[time_mask]\n",
    "signal_subset_df = signal_df[\n",
    "    (signal_df['datetime'] >= start_datetime) & \n",
    "    (signal_df['datetime'] <= end_datetime)\n",
    "]\n",
    "\n",
    "# Output the results\n",
    "print(f\"Time range selected: {start_datetime} to {end_datetime}\")\n",
    "print(f\"Signal subset size: {len(signal_subset)}\")\n",
    "\n",
    "# Retrieve parameters for peak detection\n",
    "params = config_manager.get_from_config(\n",
    "    variable_names=[\n",
    "        \"BROAD_LOW_CUTOFF\", \"BROAD_HIGH_CUTOFF\", \"NARROW_LOW_CUTOFF\", \"NARROW_HIGH_CUTOFF\",\n",
    "        \"FILTER_ORDER\", \"SPIKE_THRESHOLD\", \"SMOOTH_SEC_MULTIPLIER\", \"WINDOW_SIZE_MULTIPLIER\",\n",
    "        \"NORMALIZATION_NOISE\", \"PEAK_HEIGHT\", \"PEAK_DISTANCE_SEC\", \"SEARCH_RADIUS_SEC\",\n",
    "        \"MIN_PEAK_HEIGHT\", \"MAX_PEAK_HEIGHT\", \"enable_bandpass\", \"enable_spike_removal\",\n",
    "        \"enable_absolute\", \"enable_smoothing\", \"enable_normalization\", \"enable_refinement\"\n",
    "    ],\n",
    "    section=\"hr_peak_detection_settings\" if detection_mode == \"Heart Rate\" else \"stroke_peak_detection_settings\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite=False # If needed, change to true and rewrite settings here\n",
    "\n",
    "# Add parameters to the config file (if not already present)\n",
    "if overwrite:\n",
    "    # Define default parameters for peak detection with simple structure (no descriptions here)\n",
    "    params = {\n",
    "        \"BROAD_LOW_CUTOFF\": 0.05,  # Hz, lower cutoff for the broad bandpass filter\n",
    "        \"BROAD_HIGH_CUTOFF\": 10,  # Hz, upper cutoff for the broad bandpass filter\n",
    "        \"NARROW_LOW_CUTOFF\": 0.1,  # Hz, lower cutoff for the narrow bandpass filter\n",
    "        \"NARROW_HIGH_CUTOFF\": 2,  # Hz, upper cutoff for the narrow bandpass filter\n",
    "        \"FILTER_ORDER\": 2,  # Order of the bandpass filter, affects sharpness\n",
    "        \"SPIKE_THRESHOLD\": 400,  # Threshold for removing large spikes (e.g., noise or artifacts)\n",
    "        \"SMOOTH_SEC_MULTIPLIER\": 3,  # Multiplier for calculating the smoothing window size (3 for HR)\n",
    "        \"WINDOW_SIZE_MULTIPLIER\": 3,  # Multiplier for calculating sliding window size (if this is too big it will lump all strokes into a plateau)\n",
    "        \"NORMALIZATION_NOISE\": 1e-10,  # Small constant to avoid division by zero in normalization\n",
    "        \"PEAK_HEIGHT\": -0.9,  # Minimum amplitude (height) for peak detection\n",
    "        \"PEAK_DISTANCE_SEC\": 0.5,  # Minimum time between detected peaks (in seconds)\n",
    "        \"SEARCH_RADIUS_SEC\": 2,  # Time range for refining the peak location (in seconds)\n",
    "        \"MIN_PEAK_HEIGHT\": 500,  # Minimum acceptable amplitude for detected peaks; original units\n",
    "        \"MAX_PEAK_HEIGHT\": 1000000,  # Maximum acceptable amplitude for detected peaks; original units \n",
    "        \"enable_bandpass\": True,  # Enable/disable bandpass filtering\n",
    "        \"enable_spike_removal\": False,  # Enable/disable spike removal\n",
    "        \"enable_absolute\": False,  # Enable/disable abs() transformation of signal (only use if HR, not for stroke rate)\n",
    "        \"enable_smoothing\": True,  # Enable/disable smoothing\n",
    "        \"enable_normalization\": True,  # Enable/disable sliding window normalization\n",
    "        \"enable_refinement\": True,  # Enable/disable peak refinement\n",
    "    }\n",
    "    config_manager.add_to_config(entries=params, section=\"stroke_peak_detection_settings\")\n",
    "else:\n",
    "    print(\"Settings loaded from config file, not overwritten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run peak detection\n",
    "results = peak_detect(\n",
    "    signal=signal_subset,\n",
    "    sampling_rate=sampling_rate,\n",
    "    datetime_series=datetime_subset,\n",
    "    broad_lowcut=params[\"BROAD_LOW_CUTOFF\"],\n",
    "    broad_highcut=params[\"BROAD_HIGH_CUTOFF\"],\n",
    "    narrow_lowcut=params[\"NARROW_LOW_CUTOFF\"],\n",
    "    narrow_highcut=params[\"NARROW_HIGH_CUTOFF\"],\n",
    "    filter_order=params[\"FILTER_ORDER\"],\n",
    "    spike_threshold=params[\"SPIKE_THRESHOLD\"],\n",
    "    smooth_sec_multiplier=params[\"SMOOTH_SEC_MULTIPLIER\"],\n",
    "    window_size_multiplier=params[\"WINDOW_SIZE_MULTIPLIER\"],\n",
    "    normalization_noise=params[\"NORMALIZATION_NOISE\"],\n",
    "    peak_height=params[\"PEAK_HEIGHT\"],\n",
    "    peak_distance_sec=params[\"PEAK_DISTANCE_SEC\"],\n",
    "    search_radius_sec=params[\"SEARCH_RADIUS_SEC\"],\n",
    "    min_peak_height=params[\"MIN_PEAK_HEIGHT\"],\n",
    "    max_peak_height=params[\"MAX_PEAK_HEIGHT\"],\n",
    "    enable_bandpass=params[\"enable_bandpass\"],\n",
    "    enable_spike_removal=params[\"enable_spike_removal\"],\n",
    "    enable_absolute=params[\"enable_absolute\"],\n",
    "    enable_smoothing=params[\"enable_smoothing\"],\n",
    "    enable_normalization=params[\"enable_normalization\"],\n",
    "    enable_refinement=params[\"enable_refinement\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_rate(data_pkl, results, signal_subset_df, parent_signal,\n",
    "             params, sampling_rate, detection_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['peak_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use streamlit app to refine parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define peak detection parameters for stroke rate detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate stroke rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'red'},\n",
    "    'strokebeat_auto_detect_accepted': {'signal': 'sr_narrow_bandpass', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'strokebeat_auto_detect_rejected': {'signal': 'sr_narrow_bandpass', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'gyroscope'],\n",
    "    derived_data_signals=['depth', 'corrected_gyr', 'prh','stroke_rate', 'sr_broad_bandpass',\n",
    "                          'sr_narrow_bandpass', 'sr_smoothed',\n",
    "                          'sr_normalized'],\n",
    "    channels={}, #'corrected_gyr': ['broad_bandpassed_signal']\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=stroking_start_time,\n",
    "    zoom_end_time=stroking_end_time,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_pkl.derived_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the specified keys\n",
    "keys_to_remove = ['sr_broad_bandpass','sr_narrow_bandpass', 'sr_normalized']\n",
    "clear_intermediate_signals(data_pkl, remove_keys=keys_to_remove)\n",
    "\n",
    "initial_event_count = len(data_pkl.event_data)\n",
    "# Remove events with keys ending in '_rejected'\n",
    "data_pkl.event_data = data_pkl.event_data[~data_pkl.event_data['key'].str.endswith('_rejected', na=False)]\n",
    "# Get the final count of events\n",
    "final_event_count = len(data_pkl.event_data)\n",
    "# Print the number of removed events\n",
    "removed_event_count = initial_event_count - final_event_count\n",
    "print(f\"Removed {removed_event_count} events with keys ending in '_rejected'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get excerpt with stroking only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import firwin, filtfilt\n",
    "\n",
    "def compute_odba(data, fs, method='vedba', n=None, fh=None):\n",
    "    \"\"\"\n",
    "    Compute Overall Dynamic Body Acceleration (ODBA) or VeDBA (Vectorial Dynamic Body Acceleration).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        DataFrame with columns ['datetime', 'ax', 'ay', 'az'].\n",
    "    fs : float\n",
    "        Sampling rate in Hz.\n",
    "    method : str, optional\n",
    "        'wilson' for 1-norm ODBA or 'vedba' for 2-norm VeDBA. Default is 'vedba'.\n",
    "    n : int, optional\n",
    "        Length of the rectangular window (samples) for high-pass filtering. Required if `method` is 'wilson' or 'vedba'.\n",
    "    fh : float, optional\n",
    "        High-pass filter cutoff frequency in Hz. Required if using FIR filtering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns ['datetime', 'odba'], where 'odba' is the computed ODBA or VeDBA.\n",
    "    \"\"\"\n",
    "    if not all(col in data.columns for col in ['datetime', 'ax', 'ay', 'az']):\n",
    "        raise ValueError(\"Input data must have columns: ['datetime', 'ax', 'ay', 'az']\")\n",
    "\n",
    "    acc_data = data[['ax', 'ay', 'az']].to_numpy()\n",
    "\n",
    "    if method in ['wilson', 'vedba']:\n",
    "        if n is None:\n",
    "            raise ValueError(\"Window size 'n' must be provided for 'wilson' or 'vedba' methods.\")\n",
    "        n = 2 * (n // 2) + 1  # Ensure n is odd\n",
    "        nz = n // 2\n",
    "        h = np.concatenate([np.zeros(nz), [1], np.zeros(nz)]) - np.ones(n) / n\n",
    "\n",
    "        # Apply high-pass filter using a moving average method\n",
    "        padded_acc = np.vstack([np.tile(acc_data[0, :], (nz, 1)), acc_data, np.tile(acc_data[-1, :], (nz, 1))])\n",
    "        Ah = np.array([np.convolve(padded_acc[:, i], h, mode='valid') for i in range(acc_data.shape[1])]).T\n",
    "\n",
    "        if method == 'vedba':\n",
    "            odba_values = np.sqrt(np.sum(Ah**2, axis=1))  # VeDBA: Use 2-norm\n",
    "        elif method == 'wilson':\n",
    "            odba_values = np.sum(np.abs(Ah), axis=1)      # Wilson: Use 1-norm\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    elif fh is not None:\n",
    "        if acc_data.shape[0] <= 2 * (fs / fh):\n",
    "            raise ValueError(f\"Data needs at least {2 * (fs / fh) + 1} rows to compute FIR filter.\")\n",
    "        # Design FIR high-pass filter\n",
    "        n = 4 * int(fs / fh)\n",
    "        b = firwin(n, cutoff=fh / (fs / 2), pass_zero=False)\n",
    "        Ah = np.array([filtfilt(b, 1, acc_data[:, i]) for i in range(acc_data.shape[1])]).T\n",
    "        odba_values = np.sqrt(np.sum(Ah**2, axis=1))  # Use 2-norm\n",
    "    else:\n",
    "        raise ValueError(\"Either 'fh' or 'n' must be provided to calculate ODBA.\")\n",
    "\n",
    "    # Combine datetime with ODBA values into a DataFrame\n",
    "    odba_df = pd.DataFrame({\n",
    "        'datetime': data['datetime'],\n",
    "        'odba': odba_values\n",
    "    })\n",
    "\n",
    "    return odba_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the corrected accelerometer data is stored in data_pkl.derived_data['corrected_acc']\n",
    "corrected_acc = data_pkl.derived_data['corrected_acc']\n",
    "acc_sampling_rate = calculate_sampling_frequency(data_pkl.derived_data['corrected_acc']['datetime'])\n",
    "acc_sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_rate_subset = data_pkl.derived_data['stroke_rate'][\n",
    "        (data_pkl.derived_data['stroke_rate']['datetime'] >= stroking_start_time) &\n",
    "        (data_pkl.derived_data['stroke_rate']['datetime'] <= stroking_end_time)\n",
    "    ]\n",
    "\n",
    "# Calculate mean stroke rate\n",
    "mean_stroke_rate = stroke_rate_subset['stroke_rate'].mean()\n",
    "stroke_hz = mean_stroke_rate/60\n",
    "print(f'Stroke rate in Hz: {stroke_hz} Hz.')\n",
    "\n",
    "fh = stroke_hz / 2\n",
    "n = 4 * round(acc_sampling_rate / fh)\n",
    "\n",
    "# Calculate ODBA using VeDBA method with a window size of 5 samples\n",
    "odba_df = compute_odba(corrected_acc, fs=acc_sampling_rate, method='wilson', n=n)\n",
    "\n",
    "# Print the first few rows of the resulting ODBA DataFrame\n",
    "print(odba_df.head())\n",
    "\n",
    "# Optionally store it in derived_data\n",
    "data_pkl.derived_data['odba'] = odba_df\n",
    "data_pkl.derived_info['odba'] = {\n",
    "    \"channels\": [\"odba\"],\n",
    "    \"metadata\": {\n",
    "        \"odba\": {\"original_name\": \"Overall Dynamic Body Acceleration (VeDBA)\", \"unit\": \"g\"}\n",
    "    },\n",
    "    \"derived_from_sensors\": [\"corrected_acc\"],\n",
    "    \"transformation_log\": [\"VeDBA calculated with n=5\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'signal': 'ecg', 'symbol': 'triangle-up', 'color': 'red'},\n",
    "    'strokebeat_auto_detect_accepted': {'signal': 'sr_smoothed', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'strokebeat_auto_detect_rejected': {'signal': 'sr_smoothed', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive5(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'gyroscope'],\n",
    "    derived_data_signals=['depth', 'corrected_gyr', 'prh', 'stroke_rate', 'sr_smoothed','odba'],\n",
    "    channels={}, #'corrected_gyr': ['broad_bandpassed_signal']\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=stroking_start_time,\n",
    "    zoom_end_time=stroking_end_time,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "exporter = BaseExporter(data_pkl) # Create a BaseExporter instance using data pickle object\n",
    "netcdf_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step04.nc') # Define the export path\n",
    "exporter.save_to_netcdf(datareader, filepath=netcdf_file_path) # Save to NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 04. Stroke rate and ODBA calculation complete.\"\n",
    "print(current_processing_step)\n",
    "\n",
    "# Add or update the current_processing_step for the specified deployment\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "# Open the NetCDF file\n",
    "nc_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step04.nc')\n",
    "dataset = xarray.open_dataset(nc_file_path)\n",
    "\n",
    "# Display the dataset\n",
    "display(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyologger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
