{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data\n",
    "Load pickle file and inspect contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import *\n",
    "from pyologger.process_data.cropping import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.calibrate_data.calibrate_acc_mag import *\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and dep_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(dep_db, data_dir)\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=True, save_parq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change out preferred source of IMU or ephys data depending on your deployment\n",
    "imu_logger = 'CC-96'\n",
    "ephys_logger = 'UF-01'\n",
    "\n",
    "if imu_logger != None:\n",
    "    imu_fs = int(data_pkl.info[imu_logger]['datetime_metadata']['fs'])\n",
    "    print(f\"IMU Logger {imu_logger} sampled at: {imu_fs} Hz\")\n",
    "if ephys_logger != None: \n",
    "    ephys_fs = int(data_pkl.info[ephys_logger]['datetime_metadata']['fs'])\n",
    "    print(f\"ePhys Logger {ephys_logger} sampled at: {ephys_fs} Hz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find time chunk when stroking is dominant activity\n",
    "Use interactive plot to locate a start time and end time when stroking is the dominant activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_channels_to_plot = ['depth', 'corr_accX', 'corr_accY', 'corr_accZ', 'pitch', 'roll', 'heading']\n",
    "ephys_channels_to_plot = []\n",
    "\n",
    "imu_df = data_pkl.data[imu_logger]\n",
    "ephys_df = data_pkl.data[ephys_logger]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'exhalation_breath': 'depth'\n",
    "}\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=10, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger, ephys_logger=ephys_logger, note_annotations= notes_to_plot,\n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_start_time = pd.to_datetime('2024-01-16 10:06:30')\n",
    "new_end_time = pd.to_datetime('2024-01-16 10:07:00')\n",
    "\n",
    "# Get the time zone from the selected deployment\n",
    "time_zone_str = data_pkl.selected_deployment['Time Zone']\n",
    "time_zone = pytz.timezone(time_zone_str)\n",
    "\n",
    "# Localize start_time and end_time to the specified time zone\n",
    "new_start_time = time_zone.localize(new_start_time)\n",
    "new_end_time = time_zone.localize(new_end_time)\n",
    "\n",
    "data_crop = crop_data(data_pkl, imu_logger=imu_logger, ephys_logger=ephys_logger, start_time=new_start_time, end_time=new_end_time)\n",
    "\n",
    "# peek at cropped data\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=10, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger, ephys_logger=ephys_logger, note_annotations= notes_to_plot,\n",
    "                          color_mapping_path=color_mapping_path) #time_range=(new_start_time, new_end_time), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from scipy.signal import welch\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "def dsf(A, sampling_rate=None, fc=2.5, Nfft=None, channels=None):\n",
    "    \"\"\"\n",
    "    Estimate the dominant stroke frequency from triaxial sensor data (accelerometer, gyroscope, magnetometer).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : dict\n",
    "        A dictionary where each key is the name of the sensor (e.g., 'acc', 'gyro', 'mag'), \n",
    "        and each value is an nx3 matrix with columns [x, y, z].\n",
    "    sampling_rate : float, optional\n",
    "        The sampling rate of the sensor data in Hz (samples per second). Required if A is not a dictionary.\n",
    "    fc : float, optional\n",
    "        The cut-off frequency in Hz for a low-pass filter to apply to A before computing the spectra. Default is 2.5 Hz.\n",
    "    Nfft : int, optional\n",
    "        The FFT length and therefore the frequency resolution. Default is the power of two closest to 20 * sampling_rate.\n",
    "    channels : list, optional\n",
    "        List of channel names (e.g., ['acc', 'gyro', 'mag']) to include in the analysis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "        - 'fpk': The dominant stroke frequency in Hz.\n",
    "        - 'q': The quality of the peak measured by the peak power divided by the mean power of the spectra.\n",
    "    \"\"\"\n",
    "    \n",
    "    if channels is None:\n",
    "        channels = ['acc']  # Default to accelerometer only if no channels are specified\n",
    "    \n",
    "    # Handle the input data structure\n",
    "    if sampling_rate is None:\n",
    "        raise ValueError(\"sampling_rate is a required input.\")\n",
    "\n",
    "    # Default FFT length if not provided\n",
    "    if Nfft is None:\n",
    "        Nfft = int(20 * sampling_rate)\n",
    "        \n",
    "    # Force Nfft to the nearest power of 2\n",
    "    Nfft = 2 ** int(np.round(np.log2(Nfft)))\n",
    "\n",
    "    results = {}\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over each channel (sensor type)\n",
    "    for channel in channels:\n",
    "        data = A[channel]\n",
    "\n",
    "        # Apply low-pass filter if cutoff frequency is valid\n",
    "        if fc is not None and fc < (sampling_rate / 2):\n",
    "            data = low_pass_filter(data, cutoff=fc, fs=sampling_rate)\n",
    "        \n",
    "        # Compute the differential (difference between consecutive samples)\n",
    "        data_diff = np.diff(data, axis=0)\n",
    "        \n",
    "        # Calculate power spectral density using Welch's method\n",
    "        f, Pxx = welch(data_diff, fs=sampling_rate, nperseg=Nfft, axis=0)\n",
    "        \n",
    "        # Sum spectral power in the three axes\n",
    "        v = np.sum(Pxx, axis=1)\n",
    "        \n",
    "        # Identify the frequency with maximum power\n",
    "        m = np.max(v)\n",
    "        n = np.argmax(v)\n",
    "        \n",
    "        # Use the frequency corresponding to the maximum power directly\n",
    "        fpk = f[n]\n",
    "\n",
    "        # # Quadratic interpolation to refine peak frequency estimate - currently giving less reliable results, bring back if necessary\n",
    "        # if 1 < n < len(f) - 1:\n",
    "        #     p = poly.polyfit(f[n-1:n+2], v[n-1:n+2], 2)\n",
    "        #     fpk = -p[1] / (2 * p[0])\n",
    "        # else:\n",
    "        #     fpk = f[n]\n",
    "        \n",
    "        # Calculate the quality of the peak\n",
    "        q = m / np.mean(v)\n",
    "        \n",
    "        # Store results\n",
    "        results[channel] = {'fpk': fpk, 'q': q}\n",
    "\n",
    "        # Plot the power density spectrum on a logarithmic scale using Plotly\n",
    "        fig.add_trace(go.Scatter(x=f, y=10 * np.log10(Pxx[:, 0]), mode='lines', name=f'{channel.upper()} X', line=dict(color='red')))\n",
    "        fig.add_trace(go.Scatter(x=f, y=10 * np.log10(Pxx[:, 1]), mode='lines', name=f'{channel.upper()} Y', line=dict(color='green')))\n",
    "        fig.add_trace(go.Scatter(x=f, y=10 * np.log10(Pxx[:, 2]), mode='lines', name=f'{channel.upper()} Z', line=dict(color='blue')))\n",
    "        fig.add_vline(x=fpk, line_dash=\"dash\", line_color=\"black\", annotation_text=f'DSF: {fpk:.2f} Hz', annotation_position=\"top right\")\n",
    "\n",
    "    # Update layout for logarithmic x-axis\n",
    "    fig.update_layout(\n",
    "        xaxis_type=\"log\",\n",
    "        title=\"Power Density Spectrum\",\n",
    "        xaxis_title=\"Frequency (Hz)\",\n",
    "        yaxis_title=\"Power/Frequency (dB/Hz)\",\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def low_pass_filter(data, cutoff, fs, order=4):\n",
    "    \"\"\"\n",
    "    Apply a low-pass filter to the data to extract the static component.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input data to filter.\n",
    "    cutoff : float\n",
    "        Cutoff frequency for the low-pass filter.\n",
    "    fs : float\n",
    "        Sampling rate in Hz.\n",
    "    order : int, optional\n",
    "        The order of the filter. Default is 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Filtered data.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to calculate ODBA\n",
    "def calculate_odba(accX, accY, accZ, cutoff=0.1, fs=10):\n",
    "    # Apply low-pass filter to get the static acceleration\n",
    "    accX_static = low_pass_filter(accX, cutoff, fs)\n",
    "    accY_static = low_pass_filter(accY, cutoff, fs)\n",
    "    accZ_static = low_pass_filter(accZ, cutoff, fs)\n",
    "\n",
    "    # Subtract the static component to get the dynamic acceleration\n",
    "    accX_dynamic = accX - accX_static\n",
    "    accY_dynamic = accY - accY_static\n",
    "    accZ_dynamic = accZ - accZ_static\n",
    "\n",
    "    # Calculate ODBA\n",
    "    odba = np.abs(accX_dynamic) + np.abs(accY_dynamic) + np.abs(accZ_dynamic)\n",
    "    \n",
    "    return odba\n",
    "\n",
    "import scipy.signal as sp_sig\n",
    "\n",
    "def bandpass_filter(signal, lowcut=1, highcut=20, fs=500, order=5):\n",
    "    \"\"\"\n",
    "    Apply a bandpass filter to the signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : array-like\n",
    "        The input signal to filter.\n",
    "    lowcut : float, optional\n",
    "        The low cut-off frequency of the bandpass filter (default is 1 Hz).\n",
    "    highcut : float, optional\n",
    "        The high cut-off frequency of the bandpass filter (default is 20 Hz).\n",
    "    fs : float, optional\n",
    "        The sampling rate of the signal (default is 500 Hz).\n",
    "    order : int, optional\n",
    "        The order of the Butterworth filter (default is 5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy.ndarray\n",
    "        The filtered signal.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    low = lowcut / nyquist  # Normalize lowcut frequency\n",
    "    high = highcut / nyquist  # Normalize highcut frequency\n",
    "    b, a = sp_sig.butter(order, [low, high], btype='band')  # Design bandpass filter\n",
    "    y = sp_sig.filtfilt(b, a, signal)  # Apply filter forward and backward\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract accelerometer, gyroscope, and magnetometer data from the cropped dataset\n",
    "accX = data_crop.data[imu_logger]['corr_accX'].values\n",
    "accY = data_crop.data[imu_logger]['corr_accY'].values\n",
    "accZ = data_crop.data[imu_logger]['corr_accZ'].values\n",
    "\n",
    "gyrX = data_crop.data[imu_logger]['corr_gyrX'].values\n",
    "gyrY = data_crop.data[imu_logger]['corr_gyrY'].values\n",
    "gyrZ = data_crop.data[imu_logger]['corr_gyrZ'].values\n",
    "\n",
    "magX = data_crop.data[imu_logger]['corr_magX'].values\n",
    "magY = data_crop.data[imu_logger]['corr_magY'].values\n",
    "magZ = data_crop.data[imu_logger]['corr_magZ'].values\n",
    "\n",
    "# Stack data into nx3 matrices for accelerometer, gyroscope, and magnetometer\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "gyr_data = np.vstack((gyrX, gyrY, gyrZ)).T\n",
    "mag_data = np.vstack((magX, magY, magZ)).T\n",
    "\n",
    "# Pack the data into a dictionary as required by the dsf function\n",
    "sensor_data = {\n",
    "    'acc': acc_data,\n",
    "    'gyr': gyr_data,\n",
    "    'mag': mag_data\n",
    "}\n",
    "\n",
    "# Call the dsf function, specifying the sensor data and sampling rate\n",
    "result = dsf(sensor_data, sampling_rate=imu_fs, channels=['acc', 'gyr', 'mag'])\n",
    "\n",
    "# Print the dominant stroke frequency and quality for each sensor type\n",
    "print(f\"Accelerometer - Dominant Stroke Frequency: {result['acc']['fpk']} Hz\")\n",
    "print(f\"Accelerometer - Quality: {result['acc']['q']}\")\n",
    "\n",
    "print(f\"Gyroscope - Dominant Stroke Frequency: {result['gyr']['fpk']} Hz\")\n",
    "print(f\"Gyroscope - Quality: {result['gyr']['q']}\")\n",
    "\n",
    "print(f\"Magnetometer - Dominant Stroke Frequency: {result['mag']['fpk']} Hz\")\n",
    "print(f\"Magnetometer - Quality: {result['mag']['q']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Low-Pass and High-Pass Cut-Off Frequencies\n",
    "\n",
    "This should be 70% of your dominant stroking frequency, but anywhere between 50% and 70% could be good options. The terms \"high-pass\" and \"low-pass\" can sometimes be confusingâ€”a high-pass filter lets everything higher than the cut-off frequency pass, and a low-pass filter lets everything lower than the cut-off frequency pass. So, the high-pass filter is the lower cut-off frequency (below your dominant stroking frequency, `dsf`), and your low-pass filter is your higher cut-off frequency (above your `dsf`). We recommend:\n",
    "\n",
    "$$\n",
    "\\text{High-Pass Filter} = \\text{Low Cut-Off Frequency} = 0.70 \\times \\text{dsf}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Low-Pass Filter} = \\text{High Cut-Off Frequency} = 2 \\times \\text{dsf}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sp_sig\n",
    "\n",
    "# Bandpass filter function\n",
    "def bandpass_filter(signal, lowcut=1, highcut=20, fs=500, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = sp_sig.butter(order, [low, high], btype='band')\n",
    "    y = sp_sig.filtfilt(b, a, signal)\n",
    "    return y\n",
    "\n",
    "# Example usage with your data\n",
    "accX = data_pkl.data[imu_logger]['corr_accX'].values\n",
    "\n",
    "# Check for NaNs or Infs in the input data\n",
    "print(f\"NaNs in signal: {np.isnan(accX).sum()}, Infs in signal: {np.isinf(accX).sum()}\")\n",
    "\n",
    "# Define cut-off frequencies and sampling rate\n",
    "lowcut = 0.70 * result['acc']['fpk']\n",
    "highcut = 2 * result['acc']['fpk']\n",
    "imu_fs\n",
    "\n",
    "# Validate cutoff frequencies\n",
    "nyquist = 0.5 * imu_fs\n",
    "print(f\"Low cutoff: {lowcut}, High cutoff: {highcut}, Nyquist: {nyquist}\")\n",
    "if lowcut >= nyquist or highcut >= nyquist or lowcut <= 0 or highcut <= 0:\n",
    "    raise ValueError(\"Invalid cutoff frequencies\")\n",
    "\n",
    "# Apply the bandpass filter\n",
    "filtered_data = bandpass_filter(signal=accX, lowcut=lowcut, highcut=highcut, fs=imu_fs, order=4)\n",
    "\n",
    "# Check if filtered data is all NaNs\n",
    "print(f\"Filtered data contains NaNs: {np.isnan(filtered_data).sum()}\")\n",
    "\n",
    "# Create time vector based on sampling rate and length of data\n",
    "time = np.arange(len(accX)) / imu_fs\n",
    "\n",
    "# Plot original and filtered data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time, accX, label='Original Data', alpha=0.5)\n",
    "plt.plot(time, filtered_data, label='Filtered Data', linewidth=2)\n",
    "plt.title('Original and Bandpass Filtered Accelerometer Data')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Acceleration (m/s^2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with your data\n",
    "accX = data_pkl.data[imu_logger]['corr_accX'].values\n",
    "accY = data_pkl.data[imu_logger]['corr_accY'].values\n",
    "accZ = data_pkl.data[imu_logger]['corr_accZ'].values\n",
    "\n",
    "odba = calculate_odba(accX, accY, accZ)\n",
    "\n",
    "data_pkl.data[imu_logger]['odba'] = odba\n",
    "\n",
    "imu_channels_to_plot = ['depth', 'corr_accX', 'corr_accY', 'corr_accZ', 'odba', 'pitch', 'roll', 'heading']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = imu_logger\n",
    "ephys_logger_to_use = ephys_logger\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "# Define notes to plot\n",
    "notes_to_plot = {\n",
    "    'exhalation_breath': 'depth'\n",
    "}\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, imu_sampling_rate=10, ephys_channels=ephys_channels_to_plot, \n",
    "                          imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, note_annotations= notes_to_plot,\n",
    "                          time_range=(start_time, end_time), color_mapping_path=color_mapping_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
