{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart beat detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.io_operations.base_exporter import *\n",
    "from pyologger.utils.data_manager import *\n",
    "from pyologger.process_data.peak_detect import *\n",
    "\n",
    "dataset_id = \"nesc-adult-hi-monk-seal_dive-imu_SR-MB\"\n",
    "deployment_id = \"2018-04-18_nesc-001\"\n",
    "\n",
    "# dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "# deployment_id = \"2023-10-26_oror-001\"\n",
    "\n",
    "# dataset_id = \"mian-juv-nese_sleep_lml-ano_JKB\"\n",
    "# deployment_id = \"2019-10-25_mian-001\"\n",
    "# deployment_id = \"2021-04-17_mian-011\"\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, param_manager = select_and_load_deployment(data_dir, dataset_id=dataset_id, deployment_id=deployment_id)\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 05 IN PROGRESS.\"\n",
    "param_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve values from config\n",
    "variables = [\"calm_horizontal_start_time\", \"calm_horizontal_end_time\", \n",
    "             \"zoom_window_start_time\", \"zoom_window_end_time\", \n",
    "             \"overlap_start_time\", \"overlap_end_time\",\n",
    "             \"analysis_start_time\", \"analysis_end_time\"]\n",
    "settings = param_manager.get_from_config(variables, section=\"settings\")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "CALM_HORIZONTAL_START_TIME = settings.get(\"calm_horizontal_start_time\")\n",
    "CALM_HORIZONTAL_END_TIME = settings.get(\"calm_horizontal_end_time\")\n",
    "ZOOM_START_TIME = settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = settings.get(\"zoom_window_end_time\")\n",
    "OVERLAP_START_TIME = settings.get(\"overlap_start_time\")\n",
    "OVERLAP_END_TIME = settings.get(\"overlap_end_time\")\n",
    "ANALYSIS_START_TIME = settings.get(\"analysis_start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE AS NEEDED\n",
    "\n",
    "detection_mode=\"heart_rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "# Define parent signal options\n",
    "# parent_signal_options = list(data_pkl.sensor_data.keys()) + list(data_pkl.derived_data.keys())\n",
    "\n",
    "# Handle default signal based on availability\n",
    "if detection_mode == \"heart_rate\":\n",
    "    if \"ecg\" in data_pkl.sensor_data:\n",
    "        default_parent_signal = \"ecg\"\n",
    "        default_channel = \"ecg\"\n",
    "    else:\n",
    "        print(\"⚠️ ECG not found. Using accelerometer (ax) as fallback for peak detection.\")\n",
    "        default_parent_signal = \"accelerometer\" if \"accelerometer\" in data_pkl.sensor_data else parent_signal_options[0]\n",
    "        default_channel = \"ax\" if \"ax\" in data_pkl.sensor_data.get(default_parent_signal, {}).columns else None\n",
    "else:\n",
    "    default_parent_signal = \"corrected_gyr\"\n",
    "    default_channel = \"gy\"\n",
    "\n",
    "parent_signal = default_parent_signal\n",
    "channel = default_channel\n",
    "\n",
    "# Configure signals\n",
    "signal_df = data_pkl.sensor_data[parent_signal] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal]\n",
    "signal = data_pkl.sensor_data[parent_signal][channel] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal][channel]\n",
    "datetime_signal = data_pkl.sensor_data[parent_signal]['datetime'] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal]['datetime']\n",
    "sampling_rate = calculate_sampling_frequency(datetime_signal.head())\n",
    "\n",
    "# Define the default time range based on the signal's datetime column\n",
    "signal_start = datetime_signal.min()\n",
    "signal_end = datetime_signal.max()\n",
    "\n",
    "# User input for time range\n",
    "print(f\"Signal time range: {signal_start} to {signal_end}\")\n",
    "start_time_input = input(f\"Enter start time (default: {signal_start}): \").strip()\n",
    "end_time_input = input(f\"Enter end time (default: {signal_end}): \").strip()\n",
    "\n",
    "# Determine time range based on user input\n",
    "start_datetime = pd.Timestamp(start_time_input) if start_time_input else signal_start\n",
    "end_datetime = pd.Timestamp(end_time_input) if end_time_input else signal_end\n",
    "\n",
    "# Filter signal based on the selected time range\n",
    "time_mask = (datetime_signal >= start_datetime) & (datetime_signal <= end_datetime)\n",
    "signal_subset = signal[time_mask]\n",
    "datetime_subset = datetime_signal[time_mask]\n",
    "signal_subset_df = signal_df[\n",
    "    (signal_df['datetime'] >= start_datetime) & \n",
    "    (signal_df['datetime'] <= end_datetime)\n",
    "]\n",
    "\n",
    "# Output the results\n",
    "print(f\"Time range selected: {start_datetime} to {end_datetime}\")\n",
    "print(f\"Signal subset size: {len(signal_subset)}\")\n",
    "\n",
    "# Retrieve parameters for peak detection\n",
    "params = param_manager.get_from_config(\n",
    "    variable_names=[\n",
    "        \"BROAD_LOW_CUTOFF\", \"BROAD_HIGH_CUTOFF\", \"NARROW_LOW_CUTOFF\", \"NARROW_HIGH_CUTOFF\",\n",
    "        \"FILTER_ORDER\", \"SPIKE_THRESHOLD\", \"SMOOTH_SEC_MULTIPLIER\", \"WINDOW_SIZE_MULTIPLIER\",\n",
    "        \"NORMALIZATION_NOISE\", \"PEAK_HEIGHT\", \"PEAK_DISTANCE_SEC\", \"SEARCH_RADIUS_SEC\",\n",
    "        \"MIN_PEAK_HEIGHT\", \"MAX_PEAK_HEIGHT\", \"enable_bandpass\", \"enable_spike_removal\",\n",
    "        \"enable_absolute\", \"enable_smoothing\", \"enable_normalization\", \"enable_refinement\"\n",
    "    ],\n",
    "    section=\"hr_peak_detection_settings\" if detection_mode == \"heart_rate\" else \"stroke_peak_detection_settings\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = calculate_sampling_frequency(datetime_signal.head())\n",
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite=False # If needed, change to true and rewrite settings here\n",
    "\n",
    "# Add parameters to the config file (if not already present)\n",
    "if overwrite | any(value is None for value in params.values()):\n",
    "    # Define parameters for peak detection with updated values\n",
    "    params = {\n",
    "        \"BROAD_LOW_CUTOFF\": 1.0,  # Hz, lower cutoff for the broad bandpass filter\n",
    "        \"BROAD_HIGH_CUTOFF\": 35.0,  # Hz, upper cutoff for the broad bandpass filter\n",
    "        \"NARROW_LOW_CUTOFF\": 5.0,  # Hz, lower cutoff for the narrow bandpass filter\n",
    "        \"NARROW_HIGH_CUTOFF\": 20.0,  # Hz, upper cutoff for the narrow bandpass filter\n",
    "        \"FILTER_ORDER\": 2,  # Order of the bandpass filter, affects sharpness\n",
    "        \"SPIKE_THRESHOLD\": 400,  # Threshold for removing large spikes (e.g., noise or artifacts)\n",
    "        \"SMOOTH_SEC_MULTIPLIER\": 0.36,  # Multiplier for calculating the smoothing window size\n",
    "        \"WINDOW_SIZE_MULTIPLIER\": 6.35,  # Multiplier for calculating sliding window size\n",
    "        \"NORMALIZATION_NOISE\": 1e-10,  # Small constant to avoid division by zero in normalization\n",
    "        \"PEAK_HEIGHT\": -0.4,  # Minimum amplitude (height) for peak detection\n",
    "        \"PEAK_DISTANCE_SEC\": 0.71,  # Minimum time between detected peaks (in seconds)\n",
    "        \"SEARCH_RADIUS_SEC\": 0.35,  # Time range for refining the peak location (in seconds)\n",
    "        \"MIN_PEAK_HEIGHT\": 70,  # Minimum acceptable amplitude for detected peaks\n",
    "        \"MAX_PEAK_HEIGHT\": 12000,  # Maximum acceptable amplitude for detected peaks\n",
    "        \"enable_bandpass\": True,  # Enable/disable bandpass filtering\n",
    "        \"enable_spike_removal\": True,  # Enable/disable spike removal\n",
    "        \"enable_absolute\": True,  # Enable/disable abs() transformation of signal (only use if HR, not for stroke rate)\n",
    "        \"enable_smoothing\": True,  # Enable/disable smoothing\n",
    "        \"enable_normalization\": True,  # Enable/disable sliding window normalization\n",
    "        \"enable_refinement\": True,  # Enable/disable peak refinement\n",
    "    }\n",
    "    # Add updated parameters to the config file\n",
    "    param_manager.add_to_config(entries=params, section=\"hr_peak_detection_settings\")\n",
    "else:\n",
    "    print(\"Settings loaded from config file, not overwritten.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the updated parameters in peak detection\n",
    "results = peak_detect(\n",
    "    signal=signal_subset,\n",
    "    sampling_rate=sampling_rate,\n",
    "    datetime_series=datetime_subset,\n",
    "    broad_lowcut=params[\"BROAD_LOW_CUTOFF\"],\n",
    "    broad_highcut=params[\"BROAD_HIGH_CUTOFF\"],\n",
    "    narrow_lowcut=params[\"NARROW_LOW_CUTOFF\"],\n",
    "    narrow_highcut=params[\"NARROW_HIGH_CUTOFF\"],\n",
    "    filter_order=params[\"FILTER_ORDER\"],\n",
    "    spike_threshold=params[\"SPIKE_THRESHOLD\"],\n",
    "    smooth_sec_multiplier=params[\"SMOOTH_SEC_MULTIPLIER\"],\n",
    "    window_size_multiplier=params[\"WINDOW_SIZE_MULTIPLIER\"],\n",
    "    normalization_noise=params[\"NORMALIZATION_NOISE\"],\n",
    "    peak_height=params[\"PEAK_HEIGHT\"],\n",
    "    peak_distance_sec=params[\"PEAK_DISTANCE_SEC\"],\n",
    "    search_radius_sec=params[\"SEARCH_RADIUS_SEC\"],\n",
    "    min_peak_height=params[\"MIN_PEAK_HEIGHT\"],\n",
    "    max_peak_height=params[\"MAX_PEAK_HEIGHT\"],\n",
    "    enable_bandpass=params[\"enable_bandpass\"],\n",
    "    enable_spike_removal=params[\"enable_spike_removal\"],\n",
    "    enable_absolute=params[\"enable_absolute\"],\n",
    "    enable_smoothing=params[\"enable_smoothing\"],\n",
    "    enable_normalization=params[\"enable_normalization\"],\n",
    "    enable_refinement=params[\"enable_refinement\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_rate(data_pkl, results, signal_subset_df, parent_signal,\n",
    "             params, sampling_rate, detection_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['peak_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'hr_normalized', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'hr_normalized', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'signal': 'hr_normalized', 'symbol': 'triangle-up', 'color': 'red'},\n",
    "    'strokebeat_auto_detect_accepted': {'signal': 'sr_smoothed', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'hr_broad_bandpass', 'hr_narrow_bandpass','hr_smoothed', 'hr_normalized'],\n",
    "    derived_data_signals=['depth', 'prh', 'stroke_rate', 'heart_rate','sr_smoothed'],\n",
    "    channels={}, #'corrected_gyr': ['broad_bandpassed_signal']\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "# fig.show()\n",
    "fig.show_dash(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the specified keys\n",
    "keys_to_remove = ['hr_broad_bandpass','hr_narrow_bandpass', 'hr_smoothed'] # KEEPING 'hr_normalized' because it is clearest\n",
    "clear_intermediate_signals(data_pkl, remove_keys=keys_to_remove)\n",
    "\n",
    "initial_event_count = len(data_pkl.event_data)\n",
    "# Remove events with keys ending in '_rejected'\n",
    "data_pkl.event_data = data_pkl.event_data[~data_pkl.event_data['key'].str.endswith('_rejected', na=False)]\n",
    "# Get the final count of events\n",
    "final_event_count = len(data_pkl.event_data)\n",
    "# Print the number of removed events\n",
    "removed_event_count = initial_event_count - final_event_count\n",
    "print(f\"Removed {removed_event_count} events with keys ending in '_rejected'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 05. Heart rate calculation complete.\"\n",
    "print(current_processing_step)\n",
    "\n",
    "# Add or update the current_processing_step for the specified deployment\n",
    "param_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "# Open the NetCDF file\n",
    "nc_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step05.nc')\n",
    "dataset = xarray.open_dataset(nc_file_path)\n",
    "\n",
    "# Display the dataset\n",
    "display(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
