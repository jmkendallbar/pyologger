{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart beat detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.config_manager import ConfigManager\n",
    "from pyologger.utils.data_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.process_data.peak_detect import *\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 05 IN PROGRESS.\"\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve values from config\n",
    "variables = [\"calm_horizontal_start_time\", \"calm_horizontal_end_time\", \n",
    "             \"zoom_window_start_time\", \"zoom_window_end_time\", \n",
    "             \"earliest_common_start_time\", \"latest_common_end_time\"]\n",
    "settings = config_manager.get_from_config(variables, section=\"settings\")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "CALM_HORIZONTAL_START_TIME = settings.get(\"calm_horizontal_start_time\")\n",
    "CALM_HORIZONTAL_END_TIME = settings.get(\"calm_horizontal_end_time\")\n",
    "ZOOM_START_TIME = settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = settings.get(\"zoom_window_end_time\")\n",
    "OVERLAP_START_TIME = settings.get(\"earliest_common_start_time\")\n",
    "OVERLAP_END_TIME = settings.get(\"latest_common_end_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE AS NEEDED\n",
    "\n",
    "detection_mode=\"heart_rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parent signal options\n",
    "parent_signal_options = list(data_pkl.sensor_data.keys()) + list(data_pkl.derived_data.keys())\n",
    "default_parent_signal = \"ecg\" if detection_mode == \"heart_rate\" else \"corrected_gyr\"\n",
    "\n",
    "# User input for parent signal\n",
    "print(f\"Available parent signals: {parent_signal_options}\")\n",
    "parent_signal = input(f\"Choose parent signal (default: {default_parent_signal}): \").strip()\n",
    "if not parent_signal or parent_signal not in parent_signal_options:\n",
    "    parent_signal = default_parent_signal\n",
    "\n",
    "# Get available channels\n",
    "if parent_signal in data_pkl.sensor_data:\n",
    "    available_channels = list(data_pkl.sensor_data[parent_signal].columns)\n",
    "elif parent_signal in data_pkl.derived_data:\n",
    "    available_channels = list(data_pkl.derived_data[parent_signal].columns)\n",
    "else:\n",
    "    available_channels = []\n",
    "\n",
    "# Default channel\n",
    "default_channel = \"ecg\" if detection_mode == \"heart_rate\" else \"gy\"\n",
    "\n",
    "# User input for channel\n",
    "print(f\"Available channels: {available_channels}\")\n",
    "channel = input(f\"Choose channel (default: {default_channel}): \").strip()\n",
    "if not channel or channel not in available_channels:\n",
    "    channel = default_channel\n",
    "\n",
    "# Configure signals\n",
    "signal_df = data_pkl.sensor_data[parent_signal] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal]\n",
    "signal = data_pkl.sensor_data[parent_signal][channel] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal][channel]\n",
    "datetime_signal = data_pkl.sensor_data[parent_signal]['datetime'] if parent_signal in data_pkl.sensor_data else data_pkl.derived_data[parent_signal]['datetime']\n",
    "sampling_rate = data_pkl.sensor_info.get(parent_signal, {}).get('sampling_frequency', calculate_sampling_frequency(datetime_signal))\n",
    "\n",
    "# Define the default time range based on the signal's datetime column\n",
    "signal_start = datetime_signal.min()\n",
    "signal_end = datetime_signal.max()\n",
    "\n",
    "# User input for time range\n",
    "print(f\"Signal time range: {signal_start} to {signal_end}\")\n",
    "start_time_input = input(f\"Enter start time (default: {signal_start}): \").strip()\n",
    "end_time_input = input(f\"Enter end time (default: {signal_end}): \").strip()\n",
    "\n",
    "# Determine time range based on user input\n",
    "start_datetime = pd.Timestamp(start_time_input) if start_time_input else signal_start\n",
    "end_datetime = pd.Timestamp(end_time_input) if end_time_input else signal_end\n",
    "\n",
    "# Filter signal based on the selected time range\n",
    "time_mask = (datetime_signal >= start_datetime) & (datetime_signal <= end_datetime)\n",
    "signal_subset = signal[time_mask]\n",
    "datetime_subset = datetime_signal[time_mask]\n",
    "signal_subset_df = signal_df[\n",
    "    (signal_df['datetime'] >= start_datetime) & \n",
    "    (signal_df['datetime'] <= end_datetime)\n",
    "]\n",
    "\n",
    "# Output the results\n",
    "print(f\"Time range selected: {start_datetime} to {end_datetime}\")\n",
    "print(f\"Signal subset size: {len(signal_subset)}\")\n",
    "\n",
    "# Retrieve parameters for peak detection\n",
    "params = config_manager.get_from_config(\n",
    "    variable_names=[\n",
    "        \"BROAD_LOW_CUTOFF\", \"BROAD_HIGH_CUTOFF\", \"NARROW_LOW_CUTOFF\", \"NARROW_HIGH_CUTOFF\",\n",
    "        \"FILTER_ORDER\", \"SPIKE_THRESHOLD\", \"SMOOTH_SEC_MULTIPLIER\", \"WINDOW_SIZE_MULTIPLIER\",\n",
    "        \"NORMALIZATION_NOISE\", \"PEAK_HEIGHT\", \"PEAK_DISTANCE_SEC\", \"SEARCH_RADIUS_SEC\",\n",
    "        \"MIN_PEAK_HEIGHT\", \"MAX_PEAK_HEIGHT\", \"enable_bandpass\", \"enable_spike_removal\",\n",
    "        \"enable_absolute\", \"enable_smoothing\", \"enable_normalization\", \"enable_refinement\"\n",
    "    ],\n",
    "    section=\"hr_peak_detection_settings\" if detection_mode == \"Heart Rate\" else \"stroke_peak_detection_settings\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite=False # If needed, change to true and rewrite settings here\n",
    "\n",
    "# Add parameters to the config file (if not already present)\n",
    "if overwrite:\n",
    "    # Define parameters for peak detection with updated values\n",
    "    params = {\n",
    "        \"BROAD_LOW_CUTOFF\": 1.0,  # Hz, lower cutoff for the broad bandpass filter\n",
    "        \"BROAD_HIGH_CUTOFF\": 35.0,  # Hz, upper cutoff for the broad bandpass filter\n",
    "        \"NARROW_LOW_CUTOFF\": 5.0,  # Hz, lower cutoff for the narrow bandpass filter\n",
    "        \"NARROW_HIGH_CUTOFF\": 20.0,  # Hz, upper cutoff for the narrow bandpass filter\n",
    "        \"FILTER_ORDER\": 2,  # Order of the bandpass filter, affects sharpness\n",
    "        \"SPIKE_THRESHOLD\": 400,  # Threshold for removing large spikes (e.g., noise or artifacts)\n",
    "        \"SMOOTH_SEC_MULTIPLIER\": 0.36,  # Multiplier for calculating the smoothing window size\n",
    "        \"WINDOW_SIZE_MULTIPLIER\": 6.35,  # Multiplier for calculating sliding window size\n",
    "        \"NORMALIZATION_NOISE\": 1e-10,  # Small constant to avoid division by zero in normalization\n",
    "        \"PEAK_HEIGHT\": -0.4,  # Minimum amplitude (height) for peak detection\n",
    "        \"PEAK_DISTANCE_SEC\": 0.71,  # Minimum time between detected peaks (in seconds)\n",
    "        \"SEARCH_RADIUS_SEC\": 0.35,  # Time range for refining the peak location (in seconds)\n",
    "        \"MIN_PEAK_HEIGHT\": 70,  # Minimum acceptable amplitude for detected peaks\n",
    "        \"MAX_PEAK_HEIGHT\": 12000,  # Maximum acceptable amplitude for detected peaks\n",
    "        \"enable_bandpass\": True,  # Enable/disable bandpass filtering\n",
    "        \"enable_spike_removal\": True,  # Enable/disable spike removal\n",
    "        \"enable_absolute\": True,  # Enable/disable abs() transformation of signal (only use if HR, not for stroke rate)\n",
    "        \"enable_smoothing\": True,  # Enable/disable smoothing\n",
    "        \"enable_normalization\": True,  # Enable/disable sliding window normalization\n",
    "        \"enable_refinement\": True,  # Enable/disable peak refinement\n",
    "    }\n",
    "    # Add updated parameters to the config file\n",
    "    config_manager.add_to_config(entries=params, section=\"hr_peak_detection_settings\")\n",
    "else:\n",
    "    print(\"Settings loaded from config file, not overwritten.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the updated parameters in peak detection\n",
    "results = peak_detect(\n",
    "    signal=signal_subset,\n",
    "    sampling_rate=sampling_rate,\n",
    "    datetime_series=datetime_subset,\n",
    "    broad_lowcut=params[\"BROAD_LOW_CUTOFF\"],\n",
    "    broad_highcut=params[\"BROAD_HIGH_CUTOFF\"],\n",
    "    narrow_lowcut=params[\"NARROW_LOW_CUTOFF\"],\n",
    "    narrow_highcut=params[\"NARROW_HIGH_CUTOFF\"],\n",
    "    filter_order=params[\"FILTER_ORDER\"],\n",
    "    spike_threshold=params[\"SPIKE_THRESHOLD\"],\n",
    "    smooth_sec_multiplier=params[\"SMOOTH_SEC_MULTIPLIER\"],\n",
    "    window_size_multiplier=params[\"WINDOW_SIZE_MULTIPLIER\"],\n",
    "    normalization_noise=params[\"NORMALIZATION_NOISE\"],\n",
    "    peak_height=params[\"PEAK_HEIGHT\"],\n",
    "    peak_distance_sec=params[\"PEAK_DISTANCE_SEC\"],\n",
    "    search_radius_sec=params[\"SEARCH_RADIUS_SEC\"],\n",
    "    min_peak_height=params[\"MIN_PEAK_HEIGHT\"],\n",
    "    max_peak_height=params[\"MAX_PEAK_HEIGHT\"],\n",
    "    enable_bandpass=params[\"enable_bandpass\"],\n",
    "    enable_spike_removal=params[\"enable_spike_removal\"],\n",
    "    enable_absolute=params[\"enable_absolute\"],\n",
    "    enable_smoothing=params[\"enable_smoothing\"],\n",
    "    enable_normalization=params[\"enable_normalization\"],\n",
    "    enable_refinement=params[\"enable_refinement\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_rate(data_pkl, results, signal_subset_df, parent_signal,\n",
    "             params, sampling_rate, detection_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['peak_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'hr_normalized', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'hr_normalized', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'signal': 'hr_normalized', 'symbol': 'triangle-up', 'color': 'red'},\n",
    "    'strokebeat_auto_detect_accepted': {'signal': 'sr_smoothed', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'hr_broad_bandpass', 'hr_narrow_bandpass','hr_smoothed', 'hr_normalized'],\n",
    "    derived_data_signals=['depth', 'prh', 'stroke_rate', 'heart_rate','sr_smoothed'],\n",
    "    channels={}, #'corrected_gyr': ['broad_bandpassed_signal']\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate heart rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the specified keys\n",
    "keys_to_remove = ['hr_broad_bandpass','hr_narrow_bandpass', 'hr_smoothed'] # KEEPING 'hr_normalized' because it is clearest\n",
    "clear_intermediate_signals(data_pkl, remove_keys=keys_to_remove)\n",
    "\n",
    "initial_event_count = len(data_pkl.event_data)\n",
    "# Remove events with keys ending in '_rejected'\n",
    "data_pkl.event_data = data_pkl.event_data[~data_pkl.event_data['key'].str.endswith('_rejected', na=False)]\n",
    "# Get the final count of events\n",
    "final_event_count = len(data_pkl.event_data)\n",
    "# Print the number of removed events\n",
    "removed_event_count = initial_event_count - final_event_count\n",
    "print(f\"Removed {removed_event_count} events with keys ending in '_rejected'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.io_operations.base_exporter import *\n",
    "\n",
    "exporter = BaseExporter(data_pkl) # Create a BaseExporter instance using data pickle object\n",
    "netcdf_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step05.nc') # Define the export path\n",
    "exporter.save_to_netcdf(datareader, filepath=netcdf_file_path) # Save to NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_processing_step = \"Processing Step 05. Heart rate calculation complete.\"\n",
    "print(current_processing_step)\n",
    "\n",
    "# Add or update the current_processing_step for the specified deployment\n",
    "config_manager.add_to_config(\"current_processing_step\", current_processing_step)\n",
    "\n",
    "# Optional: save new pickle file\n",
    "with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(data_pkl, file)\n",
    "print(\"Pickle file updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.logger_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "# Open the NetCDF file\n",
    "nc_file_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_step05.nc')\n",
    "dataset = xarray.open_dataset(nc_file_path)\n",
    "\n",
    "# Display the dataset\n",
    "display(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyologger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
