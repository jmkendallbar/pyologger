{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.calibrate_acc_mag import *\n",
    "#from pyologger.process_data.feature_generation_utils import get_heart_rate\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load color mappings\n",
    "color_mapping_path = os.path.join(root_dir, 'color_mappings.json')\n",
    "\n",
    "# Streamlit sidebar for time range selection\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Define the overlapping time range\n",
    "imu_df = data_pkl.logger_data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.logger_data[ephys_logger_to_use]\n",
    "OVERLAP_START_TIME = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "OVERLAP_END_TIME = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.signal.windows import bartlett\n",
    "from scipy import stats\n",
    "from wfdb import processing\n",
    "\n",
    "# Bandpass Filter Function\n",
    "def bandpass_filter(signal, lowcut, highcut, fs, order=2):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "# Spike Removal Function\n",
    "def remove_spikes(signal, threshold=400):\n",
    "    median = np.median(signal)\n",
    "    mad = np.median(np.abs(signal - median))\n",
    "    return np.where(np.abs(signal - median) > threshold * mad, median, signal)\n",
    "\n",
    "# Signal Smoothing Function\n",
    "def smooth_signal(signal, smooth_sec, fs):\n",
    "    window = int(smooth_sec * fs)\n",
    "    return np.convolve(np.abs(signal), bartlett(window), mode='same')\n",
    "\n",
    "# Sliding Window Normalization Function\n",
    "def sliding_window_normalization(signal, window_size, noise=1e-10):\n",
    "    half_window = window_size // 2\n",
    "    normalized_signal = np.array([(signal[i] - np.mean(signal[max(0, i - half_window):min(len(signal), i + half_window)])) / \n",
    "                     (np.std(signal[max(0, i - half_window):min(len(signal), i + half_window)]) + noise) \n",
    "                     for i in range(len(signal))])\n",
    "    return normalized_signal\n",
    "\n",
    "# Peak Refinement with WFDB\n",
    "def refine_peaks_with_wfdb(cleaned_signal, rpeaks, fs, search_radius=0.5, sample_rate=1000, peak_dir=\"compare\"):\n",
    "    return processing.correct_peaks(cleaned_signal, rpeaks, smooth_window_size = int(0.5 * fs), search_radius=int(search_radius * sample_rate), peak_dir=peak_dir)\n",
    "\n",
    "# Absolute Maxima Search Function\n",
    "def absolute_maxima_search(refined_peaks, original_signal, QRS_width, sample_rate):\n",
    "    QRS_samples = int(QRS_width * sample_rate)\n",
    "    return [np.argmax(original_signal[max(0, peak - QRS_samples):peak + 1]) + max(0, peak - QRS_samples) for peak in refined_peaks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.peak_detect import *\n",
    "\n",
    "# Configuration Section\n",
    "BROAD_LOW_CUTOFF = 1  # Hz for bandpass filter\n",
    "BROAD_HIGH_CUTOFF = 35  # Hz for bandpass filter\n",
    "NARROW_LOW_CUTOFF = 5  # Hz for bandpass filter\n",
    "NARROW_HIGH_CUTOFF = 20  # Hz for bandpass filter\n",
    "FILTER_ORDER = 2  # Order of the bandpass filter\n",
    "SPIKE_THRESHOLD = 400  # Threshold for spike removal\n",
    "SMOOTH_SEC_MULTIPLIER = 3  # Multiplier for smoothing window size\n",
    "WINDOW_SIZE_MULTIPLIER = 5  # Multiplier for sliding window normalization\n",
    "NORMALIZATION_NOISE = 1e-10  # Noise level for sliding window normalization\n",
    "PEAK_HEIGHT = -0.4  # Minimum peak height for detection\n",
    "PEAK_DISTANCE_SEC = 0.2  # Minimum distance between peaks in seconds\n",
    "SEARCH_RADIUS_SEC = 0.5  # Search radius for peak refinement in seconds\n",
    "MIN_PEAK_HEIGHT = 500  # Minimum acceptable peak height\n",
    "MAX_PEAK_HEIGHT = 12000  # Maximum acceptable peak height\n",
    "OVERLAP_START_TIME = '2024-01-16 09:30:00'  # Start time for plotting\n",
    "OVERLAP_END_TIME = '2024-01-16 10:30:00'  # End time for plotting\n",
    "ZOOM_START_TIME = '2024-01-16 10:00:00'  # Start time for zooming\n",
    "ZOOM_END_TIME = '2024-01-16 10:02:30'  # End time for zooming\n",
    "TARGET_SAMPLING_RATE = 25  # Target sampling rate for plotting\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of peak_detect function\n",
    "results = peak_detect(\n",
    "    signal=data_pkl.sensor_data['ecg']['ecg'], \n",
    "    sampling_rate=data_pkl.sensor_info['ecg']['sampling_frequency']\n",
    ")\n",
    "\n",
    "peak_df = results['peak_df']\n",
    "\n",
    "# Add intermediate results to the sensor data for visualization\n",
    "data_pkl.sensor_data['ecg']['broad_bandpassed_signal'] = results.get('broad_bandpassed_signal', None)-2000\n",
    "data_pkl.sensor_data['ecg']['narrow_bandpassed_signal'] = results.get('narrow_bandpassed_signal', None)\n",
    "data_pkl.sensor_data['ecg']['spike_removed_signal'] = results.get('spike_removed_signal', None)\n",
    "data_pkl.sensor_data['ecg']['smoothed_signal'] = results.get('smoothed_signal', None) / 10\n",
    "data_pkl.sensor_data['ecg']['normalized_signal'] = results.get('normalized_signal', None)*250 + 4000\n",
    "\n",
    "# Calculate RR intervals (in seconds) between successive peaks\n",
    "if len(peak_df) > 1:\n",
    "    rr_intervals = np.diff(peak_df['refined_index']) / data_pkl.sensor_info['ecg']['sampling_frequency']\n",
    "    # Calculate heart rate (in bpm) from RR intervals\n",
    "    heart_rate = 60 / rr_intervals\n",
    "else:\n",
    "    rr_intervals = np.array([])\n",
    "    heart_rate = np.array([])\n",
    "\n",
    "# Create an array for heart rate data, initialized with NaNs\n",
    "hr_data = np.full(len(data_pkl.sensor_data['ecg']['ecg']), np.nan)\n",
    "\n",
    "# Fill in the heart rate data between detected peaks\n",
    "for i in range(len(heart_rate)):\n",
    "    start_idx = peak_df['refined_index'].iloc[i]\n",
    "    end_idx = peak_df['refined_index'].iloc[i + 1]\n",
    "    hr_data[start_idx:end_idx] = heart_rate[i]\n",
    "\n",
    "# Optionally, fill in the remaining segment with the last heart rate value\n",
    "if len(peak_df) > 0 and len(heart_rate) > 0:\n",
    "    hr_data[peak_df['refined_index'].iloc[-1]:] = heart_rate[-1]\n",
    "\n",
    "# Assign hr_data to the sensor data\n",
    "data_pkl.sensor_data['ecg']['hr_data'] = hr_data\n",
    "\n",
    "# Convert refined indices to datetime for event annotations\n",
    "matching_datetimes = data_pkl.sensor_data['ecg'].loc[peak_df['refined_index'], 'datetime'].values\n",
    "utc_datetimes = pd.to_datetime(matching_datetimes).tz_localize('UTC')\n",
    "local_timezone = data_pkl.sensor_info['ecg']['sensor_start_datetime'].tz\n",
    "peak_df['datetime'] = utc_datetimes.tz_convert(local_timezone)\n",
    "\n",
    "# Append detected peaks to the event data with heart rate values at each peak\n",
    "hr_values = hr_data[peak_df['refined_index']]\n",
    "\n",
    "hr_events = pd.DataFrame({\n",
    "    'datetime': peak_df['datetime'],\n",
    "    'key': peak_df['key'],\n",
    "    'short_description': 'calculated heart rate from detected peaks',\n",
    "    'type': 'point',\n",
    "    'value': hr_values  # Use the interpolated heart rate values here\n",
    "})\n",
    "\n",
    "# Clear any existing events with keys that start with 'heartbeat_auto_detect'\n",
    "data_pkl.event_data['key'] = data_pkl.event_data['key'].astype(str)  # Ensure 'key' is string type\n",
    "data_pkl.event_data = data_pkl.event_data[~data_pkl.event_data['key'].str.startswith('heartbeat_auto_detect', na=False)]\n",
    "\n",
    "# Concatenate with hr_events\n",
    "data_pkl.event_data = pd.concat([data_pkl.event_data, hr_events], ignore_index=True)\n",
    "\n",
    "# Visualization: Define Notes and Plot the Results\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'sensor': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'sensor': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'sensor': 'ecg', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "events_to_plot = ['heartbeat_manual_ok', 'heartbeat_auto_detect_accepted', 'heartbeat_auto_detect_rejected']\n",
    "\n",
    "fig = plot_tag_data_interactive4(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg'],\n",
    "    channels={'ecg': ['broad_bandpassed_signal','narrow_bandpassed_signal', 'spike_removed_signal', 'smoothed_signal', 'normalized_signal']},\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='ecg',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "# Add horizontal lines for min and max peak height\n",
    "min_peak_height_line = MIN_PEAK_HEIGHT / 10\n",
    "max_peak_height_line = MAX_PEAK_HEIGHT / 10\n",
    "min_peak_height_line2 = 4000 + (250 * PEAK_HEIGHT)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[OVERLAP_START_TIME, OVERLAP_END_TIME],\n",
    "    y=[min_peak_height_line, min_peak_height_line],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"gray\", dash=\"dot\"),\n",
    "    name=\"Min Peak Height\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[OVERLAP_START_TIME, OVERLAP_END_TIME],\n",
    "    y=[min_peak_height_line2, min_peak_height_line2],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"gray\", dash=\"dot\"),\n",
    "    name=\"Min Peak Height Peak detect\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[OVERLAP_START_TIME, OVERLAP_END_TIME],\n",
    "    y=[max_peak_height_line, max_peak_height_line],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"gray\", dash=\"dot\"),\n",
    "    name=\"Max Peak Height\"\n",
    "))\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "\n",
    "# Specify the file path to save the HTML file\n",
    "html_file_path = os.path.join(deployment_folder,'hr_data.html')\n",
    "\n",
    "# Save the fig plot as an HTML file\n",
    "pyo.plot(fig, filename=html_file_path, auto_open=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
