{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jessiekb/Documents/GitHub/pyologger\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.calibrate_acc_mag import *\n",
    "#from pyologger.process_data.feature_generation_utils import get_heart_rate\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Notion secret token.\n",
      "Loaded database ID for deployment_DB.\n",
      "Loaded database ID for recording_DB.\n",
      "Loaded database ID for logger_DB.\n",
      "Loaded database ID for animal_DB.\n",
      "Loaded database ID for dataset_DB.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "metadata.find_relations(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dataset_db = metadata.get_metadata(\"dataset_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Displaying deployments to help you select one.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m channel_mapping_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_mapping.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m datareader \u001b[38;5;241m=\u001b[39m DataReader(deployment_folder_path\u001b[38;5;241m=\u001b[39mdata_dir)\n\u001b[0;32m----> 6\u001b[0m deployment_folder \u001b[38;5;241m=\u001b[39m \u001b[43mdatareader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_deployment_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m edf_filename_template \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datareader\u001b[38;5;241m.\u001b[39mfiles_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeployment_folder_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medf_test_\u001b[39m\u001b[38;5;132;01m{sensor}\u001b[39;00m\u001b[38;5;124m.edf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_folder:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/finescale_env/lib/python3.12/site-packages/pyologger/load_data/datareader.py:478\u001b[0m, in \u001b[0;36mDataReader.check_deployment_folder\u001b[0;34m(self, deployment_db, data_dir)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks the deployment folder and allows the user to select a deployment.\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 1: Displaying deployments to help you select one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdeployment_db\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDeployment ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNotes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    480\u001b[0m selected_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the index of the deployment you want to work with: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m selected_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(deployment_db):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Define the path to your custom mapping file\n",
    "channel_mapping_path = os.path.join(root_dir, 'channel_mapping.json')\n",
    "\n",
    "datareader = DataReader(deployment_folder_path=data_dir)\n",
    "\n",
    "deployment_folder = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "edf_filename_template = os.path.join(datareader.files_info['deployment_folder_path'], 'outputs', 'edf_test_{sensor}.edf')\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=False, save_parq=False, save_edf=False, \n",
    "                          custom_mapping_path=channel_mapping_path, save_netcdf=False,\n",
    "                          edf_filename_template=edf_filename_template, edf_save_from='sensor_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = '/Users/jessiekb/Documents/GitHub/pyologger/data/2024-01-16_oror-002a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency for CC-96: 400 Hz\n",
      "Sampling frequency for UF-01: 100 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load color mappings\n",
    "color_mapping_path = os.path.join(root_dir, 'color_mappings.json')\n",
    "\n",
    "# Streamlit sidebar for time range selection\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Define the overlapping time range\n",
    "imu_df = data_pkl.logger_data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.logger_data[ephys_logger_to_use]\n",
    "OVERLAP_START_TIME = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "OVERLAP_END_TIME = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bartlett' from 'scipy.signal' (/Users/jessiekb/opt/anaconda3/envs/finescale_env/lib/python3.12/site-packages/scipy/signal/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m butter, filtfilt, bartlett\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwfdb\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Bandpass Filter Function\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'bartlett' from 'scipy.signal' (/Users/jessiekb/opt/anaconda3/envs/finescale_env/lib/python3.12/site-packages/scipy/signal/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, bartlett\n",
    "import wfdb\n",
    "\n",
    "# Bandpass Filter Function\n",
    "def bandpass_filter(signal, lowcut, highcut, fs, order=2):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "# Spike Removal Function\n",
    "def remove_spikes(signal, threshold=400):\n",
    "    median = np.median(signal)\n",
    "    mad = np.median(np.abs(signal - median))\n",
    "    return np.where(np.abs(signal - median) > threshold * mad, median, signal)\n",
    "\n",
    "# Signal Smoothing Function\n",
    "def smooth_signal(signal, smooth_sec, fs):\n",
    "    window = int(smooth_sec * fs)\n",
    "    return np.convolve(np.abs(signal), bartlett(window), mode='same')\n",
    "\n",
    "# Sliding Window Normalization Function\n",
    "def sliding_window_normalization(signal, window_size, noise=1e-10):\n",
    "    half_window = window_size // 2\n",
    "    normalized_signal = np.array([(signal[i] - np.mean(signal[max(0, i - half_window):min(len(signal), i + half_window)])) / \n",
    "                     (np.std(signal[max(0, i - half_window):min(len(signal), i + half_window)]) + noise) \n",
    "                     for i in range(len(signal))])\n",
    "    return normalized_signal\n",
    "\n",
    "# Peak Refinement with WFDB\n",
    "def refine_peaks_with_wfdb(cleaned_signal, rpeaks, fs, search_radius=0.5, sample_rate=1000, peak_dir=\"compare\"):\n",
    "    return wfdb.processing.correct_peaks(cleaned_signal, rpeaks, smooth_window_size = int(0.5 * fs), search_radius=int(search_radius * sample_rate), peak_dir=peak_dir)\n",
    "\n",
    "# Absolute Maxima Search Function\n",
    "def absolute_maxima_search(refined_peaks, original_signal, QRS_width, sample_rate):\n",
    "    QRS_samples = int(QRS_width * sample_rate)\n",
    "    return [np.argmax(original_signal[max(0, peak - QRS_samples):peak + 1]) + max(0, peak - QRS_samples) for peak in refined_peaks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, bartlett, find_peaks\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Configuration Section\n",
    "BROAD_LOW_CUTOFF = 1  # Hz for bandpass filter\n",
    "BROAD_HIGH_CUTOFF = 35  # Hz for bandpass filter\n",
    "NARROW_LOW_CUTOFF = 5  # Hz for bandpass filter\n",
    "NARROW_HIGH_CUTOFF = 20  # Hz for bandpass filter\n",
    "FILTER_ORDER = 2  # Order of the bandpass filter\n",
    "SPIKE_THRESHOLD = 400  # Threshold for spike removal\n",
    "SMOOTH_SEC_MULTIPLIER = 3  # Multiplier for smoothing window size\n",
    "WINDOW_SIZE_MULTIPLIER = 5  # Multiplier for sliding window normalization\n",
    "NORMALIZATION_NOISE = 1e-10  # Noise level for sliding window normalization\n",
    "PEAK_HEIGHT = -0.4  # Minimum peak height for detection\n",
    "PEAK_DISTANCE_SEC = 0.2  # Minimum distance between peaks in seconds\n",
    "SEARCH_RADIUS_SEC = 0.5  # Search radius for peak refinement in seconds\n",
    "MIN_PEAK_HEIGHT = 500  # Minimum acceptable peak height\n",
    "MAX_PEAK_HEIGHT = 12000  # Maximum acceptable peak height\n",
    "OVERLAP_START_TIME = '2024-01-16 09:30:00'  # Start time for plotting\n",
    "OVERLAP_END_TIME = '2024-01-16 10:30:00'  # End time for plotting\n",
    "ZOOM_START_TIME = '2024-01-16 10:00:00'  # Start time for zooming\n",
    "ZOOM_END_TIME = '2024-01-16 10:02:30'  # End time for zooming\n",
    "TARGET_SAMPLING_RATE = 25  # Target sampling rate for plotting\n",
    "\n",
    "# Combined Peak Detection Function\n",
    "def peak_detect(signal, sampling_rate, QRS_width=0.200, \n",
    "                broad_lowcut= BROAD_LOW_CUTOFF, broad_highcut = BROAD_HIGH_CUTOFF,\n",
    "                narrow_lowcut=NARROW_LOW_CUTOFF, narrow_highcut=NARROW_HIGH_CUTOFF, \n",
    "                filter_order=FILTER_ORDER,\n",
    "                spike_threshold=SPIKE_THRESHOLD, smooth_sec_multiplier=SMOOTH_SEC_MULTIPLIER,\n",
    "                window_size_multiplier=WINDOW_SIZE_MULTIPLIER, normalization_noise=NORMALIZATION_NOISE,\n",
    "                peak_height=PEAK_HEIGHT, peak_distance_sec=PEAK_DISTANCE_SEC, search_radius_sec=SEARCH_RADIUS_SEC,\n",
    "                min_peak_height=MIN_PEAK_HEIGHT, max_peak_height=MAX_PEAK_HEIGHT,\n",
    "                enable_bandpass=True, enable_spike_removal=True, enable_smoothing=True, \n",
    "                enable_normalization=True, enable_refinement=True):\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    # Bandpass filter\n",
    "    if enable_bandpass:\n",
    "        broad_bandpassed_signal = bandpass_filter(signal, lowcut=broad_lowcut, highcut=broad_highcut, fs=sampling_rate, order=filter_order)\n",
    "        results['broad_bandpassed_signal'] = broad_bandpassed_signal\n",
    "        narrow_bandpassed_signal = bandpass_filter(signal, lowcut=narrow_lowcut, highcut=narrow_highcut, fs=sampling_rate, order=filter_order)\n",
    "        results['narrow_bandpassed_signal'] = narrow_bandpassed_signal\n",
    "    else:\n",
    "        narrow_bandpassed_signal = signal\n",
    "\n",
    "    # Spike removal\n",
    "    if enable_spike_removal:\n",
    "        spike_removed_signal = remove_spikes(narrow_bandpassed_signal, threshold=spike_threshold)\n",
    "        results['spike_removed_signal'] = spike_removed_signal\n",
    "    else:\n",
    "        spike_removed_signal = narrow_bandpassed_signal\n",
    "\n",
    "    # Smoothing\n",
    "    if enable_smoothing:\n",
    "        smoothed_signal = smooth_signal(spike_removed_signal, smooth_sec=QRS_width * smooth_sec_multiplier, fs=sampling_rate)\n",
    "        results['smoothed_signal'] = smoothed_signal\n",
    "    else:\n",
    "        smoothed_signal = spike_removed_signal\n",
    "\n",
    "    # Normalization\n",
    "    if enable_normalization:\n",
    "        normalized_signal = sliding_window_normalization(smoothed_signal, int(window_size_multiplier * sampling_rate), noise=normalization_noise)\n",
    "        results['normalized_signal'] = normalized_signal\n",
    "    else:\n",
    "        normalized_signal = smoothed_signal\n",
    "\n",
    "    # Peak Detection\n",
    "    detected_peaks = find_peaks(normalized_signal, height=peak_height, distance=int(peak_distance_sec * sampling_rate))[0]\n",
    "    results['detected_peaks'] = detected_peaks\n",
    "\n",
    "    # Peak Refinement\n",
    "    if enable_refinement:\n",
    "        refined_peaks = refine_peaks_with_wfdb(spike_removed_signal, detected_peaks, fs = sampling_rate, search_radius=search_radius_sec, sample_rate=sampling_rate,\n",
    "                                               peak_dir = \"both\")\n",
    "        results['refined_peaks'] = refined_peaks\n",
    "        refined_indices = refined_peaks\n",
    "        #refined_indices = absolute_maxima_search(refined_peaks, spike_removed_signal, QRS_width * 2, sampling_rate)\n",
    "    else:\n",
    "        refined_indices = detected_peaks\n",
    "\n",
    "    # Remove duplicate refined_indices while keeping corresponding heights aligned\n",
    "    unique_refined_indices, index_positions = np.unique(refined_indices, return_index=True)\n",
    "    results['unique_refined_indices'] = unique_refined_indices\n",
    "\n",
    "    # Align heights with unique refined indices\n",
    "    height_original = smoothed_signal[unique_refined_indices]\n",
    "    height_normalized = normalized_signal[refined_peaks[index_positions]]\n",
    "    results['height_original'] = height_original\n",
    "    results['height_normalized'] = height_normalized\n",
    "\n",
    "    # Filter out peaks that are too close to each other\n",
    "    min_distance_samples = int(peak_distance_sec * sampling_rate)\n",
    "    filtered_indices = [unique_refined_indices[0]] if len(unique_refined_indices) > 0 else []  # Always include the first peak if available\n",
    "    for i in range(1, len(unique_refined_indices)):\n",
    "        if unique_refined_indices[i] - filtered_indices[-1] >= min_distance_samples:\n",
    "            filtered_indices.append(unique_refined_indices[i])\n",
    "    \n",
    "    # Update the DataFrame with the filtered indices\n",
    "    peak_df = pd.DataFrame({\n",
    "        'refined_index': filtered_indices,\n",
    "        'height_original': smoothed_signal[filtered_indices],\n",
    "        'height_normalized': normalized_signal[filtered_indices]\n",
    "    })\n",
    "\n",
    "    # Filter out peaks based on both minimum and maximum peak height\n",
    "    filtered_peak_df = peak_df[(peak_df['height_original'] >= min_peak_height) & \n",
    "                               (peak_df['height_original'] <= max_peak_height)].reset_index(drop=True)\n",
    "    results['filtered_peak_df'] = filtered_peak_df\n",
    "\n",
    "    # Label peaks as accepted or rejected\n",
    "    peak_df['key'] = np.where(peak_df['refined_index'].isin(filtered_peak_df['refined_index']), \n",
    "                              'heartbeat_auto_detect_accepted', \n",
    "                              'heartbeat_auto_detect_rejected')\n",
    "    # Ensure no NaN values in 'key'\n",
    "    peak_df['key'].fillna('heartbeat_auto_detect_unknown', inplace=True)\n",
    "    results['peak_df'] = peak_df\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage of peak_detect function\n",
    "results = peak_detect(\n",
    "    signal=data_pkl.sensor_data['ecg']['ecg'], \n",
    "    sampling_rate=data_pkl.sensor_info['ecg']['sampling_frequency']\n",
    ")\n",
    "\n",
    "peak_df = results['peak_df']\n",
    "\n",
    "# Add intermediate results to the sensor data for visualization\n",
    "data_pkl.sensor_data['ecg']['broad_bandpassed_signal'] = results.get('broad_bandpassed_signal', None)-2000\n",
    "data_pkl.sensor_data['ecg']['narrow_bandpassed_signal'] = results.get('narrow_bandpassed_signal', None)\n",
    "data_pkl.sensor_data['ecg']['spike_removed_signal'] = results.get('spike_removed_signal', None)\n",
    "data_pkl.sensor_data['ecg']['smoothed_signal'] = results.get('smoothed_signal', None) / 10\n",
    "data_pkl.sensor_data['ecg']['normalized_signal'] = results.get('normalized_signal', None)*250 + 4000\n",
    "\n",
    "# Calculate RR intervals (in seconds) between successive peaks\n",
    "if len(peak_df) > 1:\n",
    "    rr_intervals = np.diff(peak_df['refined_index']) / data_pkl.sensor_info['ecg']['sampling_frequency']\n",
    "    # Calculate heart rate (in bpm) from RR intervals\n",
    "    heart_rate = 60 / rr_intervals\n",
    "else:\n",
    "    rr_intervals = np.array([])\n",
    "    heart_rate = np.array([])\n",
    "\n",
    "# Create an array for heart rate data, initialized with NaNs\n",
    "hr_data = np.full(len(data_pkl.sensor_data['ecg']['ecg']), np.nan)\n",
    "\n",
    "# Fill in the heart rate data between detected peaks\n",
    "for i in range(len(heart_rate)):\n",
    "    start_idx = peak_df['refined_index'].iloc[i]\n",
    "    end_idx = peak_df['refined_index'].iloc[i + 1]\n",
    "    hr_data[start_idx:end_idx] = heart_rate[i]\n",
    "\n",
    "# Optionally, fill in the remaining segment with the last heart rate value\n",
    "if len(peak_df) > 0 and len(heart_rate) > 0:\n",
    "    hr_data[peak_df['refined_index'].iloc[-1]:] = heart_rate[-1]\n",
    "\n",
    "# Assign hr_data to the sensor data\n",
    "data_pkl.sensor_data['ecg']['hr_data'] = hr_data\n",
    "\n",
    "# Convert refined indices to datetime for event annotations\n",
    "matching_datetimes = data_pkl.sensor_data['ecg'].loc[peak_df['refined_index'], 'datetime'].values\n",
    "utc_datetimes = pd.to_datetime(matching_datetimes).tz_localize('UTC')\n",
    "local_timezone = data_pkl.sensor_info['ecg']['sensor_start_datetime'].tz\n",
    "peak_df['datetime'] = utc_datetimes.tz_convert(local_timezone)\n",
    "\n",
    "# Append detected peaks to the event data with heart rate values at each peak\n",
    "hr_values = hr_data[peak_df['refined_index']]\n",
    "\n",
    "hr_events = pd.DataFrame({\n",
    "    'datetime': peak_df['datetime'],\n",
    "    'key': peak_df['key'],\n",
    "    'short_description': 'calculated heart rate from detected peaks',\n",
    "    'type': 'point',\n",
    "    'value': hr_values  # Use the interpolated heart rate values here\n",
    "})\n",
    "\n",
    "# Clear any existing events with keys that start with 'heartbeat_auto_detect'\n",
    "data_pkl.event_data['key'] = data_pkl.event_data['key'].astype(str)  # Ensure 'key' is string type\n",
    "data_pkl.event_data = data_pkl.event_data[~data_pkl.event_data['key'].str.startswith('heartbeat_auto_detect', na=False)]\n",
    "\n",
    "# Concatenate with hr_events\n",
    "data_pkl.event_data = pd.concat([data_pkl.event_data, hr_events], ignore_index=True)\n",
    "\n",
    "# Visualization: Define Notes and Plot the Results\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'sensor': 'ecg', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'sensor': 'ecg', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'heartbeat_auto_detect_rejected': {'sensor': 'ecg', 'symbol': 'triangle-up', 'color': 'red'}\n",
    "}\n",
    "events_to_plot = ['heartbeat_manual_ok', 'heartbeat_auto_detect_accepted', 'heartbeat_auto_detect_rejected']\n",
    "\n",
    "fig = plot_tag_data_interactive4(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg'],\n",
    "    channels={'ecg': ['broad_bandpassed_signal','narrow_bandpassed_signal', 'spike_removed_signal', 'smoothed_signal', 'normalized_signal']},\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='ecg',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "# Add horizontal lines for min and max peak height\n",
    "min_peak_height_line = MIN_PEAK_HEIGHT / 10\n",
    "max_peak_height_line = MAX_PEAK_HEIGHT / 10\n",
    "min_peak_height_line2 = 4000 + (250 * PEAK_HEIGHT)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[OVERLAP_START_TIME, OVERLAP_END_TIME],\n",
    "    y=[min_peak_height_line, min_peak_height_line],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"gray\", dash=\"dot\"),\n",
    "    name=\"Min Peak Height\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[OVERLAP_START_TIME, OVERLAP_END_TIME],\n",
    "    y=[min_peak_height_line2, min_peak_height_line2],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"gray\", dash=\"dot\"),\n",
    "    name=\"Min Peak Height Peak detect\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[OVERLAP_START_TIME, OVERLAP_END_TIME],\n",
    "    y=[max_peak_height_line, max_peak_height_line],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"gray\", dash=\"dot\"),\n",
    "    name=\"Max Peak Height\"\n",
    "))\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "\n",
    "# Specify the file path to save the HTML file\n",
    "html_file_path = os.path.join(deployment_folder,'hr_data.html')\n",
    "\n",
    "# Save the fig plot as an HTML file\n",
    "pyo.plot(fig, filename=html_file_path, auto_open=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
