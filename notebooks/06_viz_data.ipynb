{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data with 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.utils.event_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.io_operations.base_exporter import *\n",
    "from pyologger.utils.data_manager import *\n",
    "from pyologger.process_data.peak_detect import *\n",
    "\n",
    "dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "deployment_id = \"2024-01-16_oror-002\"\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, config_manager = select_and_load_deployment(data_dir, dataset_id=dataset_id, deployment_id=deployment_id)\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_path = os.path.join(config['paths']['local_private_media'], dataset_id, deployment_id, 'media', '02_processed-video')\n",
    "media_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vidformer as vf\n",
    "import vidformer.cv2 as cv2\n",
    "\n",
    "server = vf.Server(\"http://localhost:8080\", api_key = \"VF_IGNI_API_KEY\")\n",
    "\n",
    "cap = cv2.VideoCapture(media_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(\"my_output.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "                        fps, (width, height))\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    cv2.putText(frame, \"Hello, World!\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1, (255, 0, 0), 1)\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.logger_info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve values from config\n",
    "variables = [\"calm_horizontal_start_time\", \"calm_horizontal_end_time\", \n",
    "             \"zoom_window_start_time\", \"zoom_window_end_time\", \n",
    "             \"stroking_start_time\", \"stroking_end_time\",\n",
    "             \"overlap_start_time\", \"overlap_end_time\"]\n",
    "settings = config_manager.get_from_config(variables, section=\"settings\")\n",
    "\n",
    "# Assign retrieved values to variables\n",
    "CALM_HORIZONTAL_START_TIME = settings.get(\"calm_horizontal_start_time\")\n",
    "CALM_HORIZONTAL_END_TIME = settings.get(\"calm_horizontal_end_time\")\n",
    "ZOOM_START_TIME = settings.get(\"zoom_window_start_time\")\n",
    "ZOOM_END_TIME = settings.get(\"zoom_window_end_time\")\n",
    "OVERLAP_START_TIME = settings.get(\"overlap_start_time\")\n",
    "OVERLAP_END_TIME = settings.get(\"overlap_end_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 10\n",
    "\n",
    "notes_to_plot = {\n",
    "    'heartbeat_manual_ok': {'signal': 'hr_normalized', 'symbol': 'triangle-down', 'color': 'blue'},\n",
    "    'heartbeat_auto_detect_accepted': {'signal': 'hr_normalized', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "    'strokebeat_auto_detect_accepted': {'signal': 'sr_smoothed', 'symbol': 'triangle-up', 'color': 'green'},\n",
    "}\n",
    "\n",
    "fig = plot_tag_data_interactive(\n",
    "    data_pkl=data_pkl,\n",
    "    sensors=['ecg', 'hr_normalized'],\n",
    "    derived_data_signals=['depth', 'prh', 'stroke_rate', 'heart_rate','sr_smoothed', 'odba'],\n",
    "    channels={}, #'corrected_gyr': ['broad_bandpassed_signal']\n",
    "    time_range=(OVERLAP_START_TIME, OVERLAP_END_TIME),\n",
    "    note_annotations=notes_to_plot,\n",
    "    color_mapping_path=color_mapping_path,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    zoom_start_time=ZOOM_START_TIME,\n",
    "    zoom_end_time=ZOOM_END_TIME,\n",
    "    zoom_range_selector_channel='depth',\n",
    "    plot_event_values=[],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve timezone from deployment info\n",
    "timezone = data_pkl.deployment_info['Time Zone']\n",
    "\n",
    "VIDEO_FILENAME = \"2024-01-16_oror-002a_CC-96_095653-101244.mp4\"\n",
    "\n",
    "# Define placeholder timestamps for calm period in the retrieved timezone\n",
    "VIDEO_START_TIME = pd.Timestamp(\"2024-01-16 09:56:53\").tz_localize(timezone)\n",
    "VIDEO_END_TIME = pd.Timestamp(\"2024-01-16 10:12:44\").tz_localize(timezone)\n",
    "\n",
    "# Use ConfigManager to add both stroking start and end times to the config in the desired section\n",
    "config_manager.add_to_config(\n",
    "    entries={\n",
    "        \"video_filename\": str(VIDEO_FILENAME),\n",
    "        \"video_start_time\": str(VIDEO_START_TIME),\n",
    "        \"video_end_time\": str(VIDEO_END_TIME)\n",
    "    },\n",
    "    section=\"settings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_START_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_END_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_all = data_pkl.derived_data['prh']\n",
    "\n",
    "# Convert to UTC\n",
    "#dff[\"datetime\"] = dff[\"datetime\"].dt.tz_localize(\"UTC\")\n",
    "# convert the datetime in dff to timezone\n",
    "#dff[\"datetime\"] = dff[\"datetime\"] + pd.Timedelta(hours=-8)\n",
    "print(f'')\n",
    "\n",
    "dff = df_all[(df_all['datetime'] >= VIDEO_START_TIME) & (df_all['datetime'] <= VIDEO_END_TIME)]\n",
    "dff.reset_index(drop=True, inplace=True)\n",
    "# Convert datetime to timestamp (seconds since epoch) for slider control\n",
    "dff[\"timestamp\"] = dff[\"datetime\"].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process video data\n",
    "import cv2\n",
    "import pytesseract\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to Tesseract executable (modify as per your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def extract_timestamps(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Crop bottom right corner (adjust these values to match the location in your video)\n",
    "        h, w, _ = frame.shape\n",
    "        cropped = frame[int(h*0.85):h, int(w*0.7):w]\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform OCR\n",
    "        text = pytesseract.image_to_string(gray, config='--psm 6')\n",
    "        text = text.strip()\n",
    "\n",
    "        try:\n",
    "            # Parse timestamp (adjust format as needed)\n",
    "            timestamp = datetime.strptime(text, '%d%b%y %H:%M:%S.%f')\n",
    "            timestamps.append((frame_count, timestamp))\n",
    "        except ValueError:\n",
    "            pass  # Skip if OCR output is not a valid timestamp\n",
    "\n",
    "    cap.release()\n",
    "    return timestamps\n",
    "\n",
    "# Usage\n",
    "video_path = 'your_video.mp4'\n",
    "timestamps = extract_timestamps(video_path)\n",
    "print(timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "\n",
    "# Specify the file path to save the HTML file\n",
    "html_file_path = os.path.join(deployment_folder,'hr_data.html')\n",
    "\n",
    "# Save the fig plot as an HTML file\n",
    "pyo.plot(fig, filename=html_file_path, auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from pythreejs import *\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the path to the OBJ file\n",
    "model_path = os.path.join(data_dir, '/assets/', '6_killerWhale_v017_LP.obj')\n",
    "\n",
    "# Load the OBJ file using Trimesh\n",
    "mesh = trimesh.load(model_path)\n",
    "\n",
    "# Extract vertices and faces from the mesh\n",
    "vertices = mesh.vertices\n",
    "faces = mesh.faces.astype(np.uint32)  # Ensure the indices are unsigned integers\n",
    "\n",
    "# Create BufferGeometry for PyThreeJS\n",
    "geometry = BufferGeometry(\n",
    "    attributes={\n",
    "        'position': BufferAttribute(vertices, normalized=False),\n",
    "        'index': BufferAttribute(faces.flatten(), normalized=False)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a material\n",
    "material = MeshLambertMaterial(color='red') # , roughness=0.9, metalness=0.2\n",
    "key_light = DirectionalLight(color='white', position=[3, 5, 1], intensity=0.75)\n",
    "\n",
    "# Create the Mesh\n",
    "mesh = Mesh(geometry=geometry, material=material, position=[0, 0, 200], rotation= [0, math.pi, 0, 'XYZ'])\n",
    "\n",
    "# Move (translate) the mesh\n",
    "mesh.position = [0, 0, 200]  # Move the mesh up by 1 unit on the Y-axis\n",
    "\n",
    "# Rotate the mesh\n",
    "#mesh.rotation = [0.5, 0.5, 0, 0]  # Rotate the mesh around the X and Y axes (in radians)\n",
    "\n",
    "# Scale the mesh\n",
    "mesh.scale = [1.5, 1.5, 1.5]  # Scale the mesh uniformly by 1.5 on all axes\n",
    "\n",
    "# Create a scene, camera, and renderer\n",
    "scene = Scene(children=[mesh, AmbientLight(color='#ffffff', intensity=0.5)])\n",
    "camera = PerspectiveCamera(position=[0, 0, 5], up=[0, 1, 0], children=[key_light])\n",
    "renderer = Renderer(camera=camera, scene=scene, controls=[OrbitControls(controlling=camera)], width=400, height=400)\n",
    "\n",
    "# Display the model in the notebook\n",
    "display(renderer)\n",
    "\n",
    "ball = Mesh(geometry=SphereGeometry(), \n",
    "            material=MeshLambertMaterial(color='red'))\n",
    "\n",
    "\n",
    "c = PerspectiveCamera(position=[0, 5, 5], up=[0, 1, 0], children=[key_light])\n",
    "\n",
    "scene = Scene(children=[ball, c, AmbientLight(color='#777777')], background=None)\n",
    "\n",
    "renderer = Renderer(camera=c, \n",
    "                    scene=scene,\n",
    "                    alpha=True,\n",
    "                    clearOpacity=0,\n",
    "                    controls=[OrbitControls(controlling=c)])\n",
    "display(renderer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
