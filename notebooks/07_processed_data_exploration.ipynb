{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from pyologger.utils.folder_manager import load_configuration\n",
    "from pyologger.utils.folder_manager import select_and_load_deployment\n",
    "\n",
    "def list_valid_deployments(dataset_folder):\n",
    "    dataset_path = Path(dataset_folder)\n",
    "    deployment_folders = [\n",
    "        p for p in dataset_path.iterdir()\n",
    "        if p.is_dir() and (p / 'outputs' / 'data.pkl').exists()\n",
    "    ]\n",
    "    return deployment_folders\n",
    "\n",
    "# --- Config & Paths ---\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "dataset_folder = os.path.join(data_dir, dataset_id)\n",
    "\n",
    "# --- Discover valid deployments ---\n",
    "deployment_paths = list_valid_deployments(dataset_folder)\n",
    "print(f\"Found {len(deployment_paths)} deployments in {dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare storage ---\n",
    "all_event_data = []\n",
    "all_sensor_data = defaultdict(list)\n",
    "all_derived_data = defaultdict(list)\n",
    "metadata = {\n",
    "    'deployment_ids': [],\n",
    "    'sensor_info': {},\n",
    "    'derived_info': {},\n",
    "    'logger_info': {},\n",
    "    'animal_info': {},\n",
    "    'deployment_info': {},\n",
    "}\n",
    "\n",
    "# --- Process deployments ---\n",
    "for deployment_path in deployment_paths:\n",
    "    deployment_id = deployment_path.name\n",
    "    pkl_path = deployment_path / 'outputs' / 'data.pkl'\n",
    "\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data_pkl = pickle.load(f)\n",
    "\n",
    "    print(f\"→ Processing {deployment_id}\")\n",
    "    metadata['deployment_ids'].append(deployment_id)\n",
    "\n",
    "    # Event data\n",
    "    if hasattr(data_pkl, 'event_data') and data_pkl.event_data is not None:\n",
    "        df = data_pkl.event_data.copy()\n",
    "        df['deployment_id'] = deployment_id\n",
    "        all_event_data.append(df)\n",
    "\n",
    "    # Sensor data\n",
    "    for sensor, df in data_pkl.sensor_data.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['deployment_id'] = deployment_id\n",
    "        all_sensor_data[sensor].append(df_copy)\n",
    "\n",
    "    # Derived data\n",
    "    for signal, df in data_pkl.derived_data.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['deployment_id'] = deployment_id\n",
    "        all_derived_data[signal].append(df_copy)\n",
    "\n",
    "    # Metadata\n",
    "    metadata['sensor_info'][deployment_id] = data_pkl.sensor_info\n",
    "    metadata['derived_info'][deployment_id] = data_pkl.derived_info\n",
    "    metadata['logger_info'][deployment_id] = data_pkl.logger_info\n",
    "    metadata['animal_info'][deployment_id] = data_pkl.animal_info\n",
    "    metadata['deployment_info'][deployment_id] = data_pkl.deployment_info\n",
    "\n",
    "# --- Build MEGA PICKLE ---\n",
    "megadata_pkl = {\n",
    "    'event_data': pd.concat(all_event_data, ignore_index=True) if all_event_data else None,\n",
    "    'sensor_data': {k: pd.concat(v, ignore_index=True) for k, v in all_sensor_data.items()},\n",
    "    'derived_data': {k: pd.concat(v, ignore_index=True) for k, v in all_derived_data.items()},\n",
    "    'metadata': metadata\n",
    "}\n",
    "\n",
    "# --- Save output ---\n",
    "output_path = os.path.join(dataset_folder, 'megadata_pkl.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(megadata_pkl, f)\n",
    "\n",
    "print(f\"✅ MEGA PICKLE saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megadata_pkl['event_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megadata_pkl['derived_data']['prh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.sampling import calculate_sampling_frequency\n",
    "import pandas as pd\n",
    "\n",
    "def truncate_to_seconds(dt_series):\n",
    "    return dt_series.dt.floor('s')\n",
    "\n",
    "def downsample_and_merge_derived_data(megadata_pkl, keys, target_freq=1):\n",
    "    derived = megadata_pkl['derived_data']\n",
    "    dfs = {}\n",
    "\n",
    "    # Step 1: Drop NA and collect sampling frequencies\n",
    "    sampling_frequencies = {}\n",
    "\n",
    "    for key in keys:\n",
    "        df = derived[key].dropna().copy()\n",
    "        unique_deployments = df['deployment_id'].unique()\n",
    "\n",
    "        freqs = []\n",
    "        for deployment_id in unique_deployments:\n",
    "            sub = df[df['deployment_id'] == deployment_id]\n",
    "            fs = calculate_sampling_frequency(sub['datetime'])\n",
    "            freqs.append(fs)\n",
    "\n",
    "        if len(set(freqs)) != 1:\n",
    "            print(f\"❌ Inconsistent sampling frequency in '{key}':\")\n",
    "            for i, deployment_id in enumerate(unique_deployments):\n",
    "                print(f\"   {deployment_id}: {freqs[i]:.2f} Hz\")\n",
    "            return None\n",
    "\n",
    "        fs = freqs[0]\n",
    "        print(f\"✅ '{key}' has consistent sampling frequency: {fs:.2f} Hz\")\n",
    "        sampling_frequencies[key] = fs\n",
    "        dfs[key] = df\n",
    "\n",
    "    # Step 2: Downsample and truncate datetime\n",
    "    downsampled = {}\n",
    "    for key, df in dfs.items():\n",
    "        step = int(sampling_frequencies[key] // target_freq)\n",
    "        df = df.iloc[::step].reset_index(drop=True)\n",
    "        df['datetime'] = truncate_to_seconds(df['datetime'])\n",
    "        downsampled[key] = df\n",
    "\n",
    "    # Step 3: Merge on 'datetime'\n",
    "    merged_df = None\n",
    "    for i, (key, df) in enumerate(downsampled.items()):\n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='datetime', how='inner', suffixes=('', f'_{key}'))\n",
    "\n",
    "    # Step 4: Check deployment_id consistency row-wise\n",
    "    deployment_cols = [col for col in merged_df.columns if col.startswith('deployment_id')]\n",
    "    inconsistent_rows = merged_df[deployment_cols].nunique(axis=1) > 1\n",
    "    if inconsistent_rows.any():\n",
    "        print(\"❌ Inconsistent deployment IDs found in merged rows:\")\n",
    "        print(merged_df.loc[inconsistent_rows, ['datetime'] + deployment_cols])\n",
    "        return None\n",
    "\n",
    "    # Step 5: Collapse to one consistent deployment_id column (but keep it)\n",
    "    merged_df['deployment_id'] = merged_df[deployment_cols[0]]\n",
    "    # Optionally: Keep original columns or drop them\n",
    "    # merged_df = merged_df.drop(columns=deployment_cols[1:])  # keep just one, drop others\n",
    "\n",
    "    # Save result\n",
    "    result_key = f\"merged_{'_'.join(keys)}\"\n",
    "    megadata_pkl['derived_data'][result_key] = merged_df\n",
    "    print(f\"✅ Merged DataFrame saved as megadata_pkl['derived_data']['{result_key}']\")\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = downsample_and_merge_derived_data(\n",
    "    megadata_pkl,\n",
    "    keys=['heart_rate', 'stroke_rate', 'prh', 'depth'],\n",
    "    target_freq=1  # Hz\n",
    ")\n",
    "merged_df\n",
    "\n",
    "merged_df['roll'] = abs(merged_df['roll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Make sure the category dtype is correct\n",
    "merged_df['deployment_id'] = merged_df['deployment_id'].astype('category')\n",
    "\n",
    "# Original scatter plot with proper categorical coloring\n",
    "fig = px.scatter(\n",
    "    merged_df,\n",
    "    x='stroke_rate',\n",
    "    y='heart_rate',\n",
    "    color='deployment_id',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2,  # optional, helps make sure colors are distinct\n",
    "    title='Heart Rate vs Stroke Rate',\n",
    "    labels={'stroke_rate': 'depth (m)', 'heart_rate': 'Heart Rate (bpm)', 'deployment_id': 'Deployment ID'},\n",
    "    opacity=0.3\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(\n",
    "    xaxis_title='depth (m)',\n",
    "    yaxis_title='Heart Rate (bpm)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Add a single regression line (not per deployment)\n",
    "trend_fig = px.scatter(\n",
    "    merged_df,\n",
    "    x='stroke_rate',\n",
    "    y='heart_rate',\n",
    "    trendline='ols'\n",
    ")\n",
    "\n",
    "# Extract just the trendline (usually the 1st or 2nd trace depending on how many groups)\n",
    "trendline_trace = [trace for trace in trend_fig.data if trace.mode == 'lines'][0]\n",
    "\n",
    "# Add the regression line to your original colorful scatter plot\n",
    "fig.add_trace(trendline_trace)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Make sure the category dtype is correct\n",
    "merged_df['deployment_id'] = merged_df['deployment_id'].astype('category')\n",
    "\n",
    "# Original scatter plot with proper categorical coloring\n",
    "fig = px.scatter(\n",
    "    merged_df,\n",
    "    x='depth',\n",
    "    y='heart_rate',\n",
    "    color='deployment_id',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2,  # optional, helps make sure colors are distinct\n",
    "    title='Heart Rate vs Depth',\n",
    "    labels={'depth': 'depth (m)', 'heart_rate': 'Heart Rate (bpm)', 'deployment_id': 'Deployment ID'},\n",
    "    opacity=0.3\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(\n",
    "    xaxis_title='depth (m)',\n",
    "    yaxis_title='Heart Rate (bpm)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Add a single regression line (not per deployment)\n",
    "trend_fig = px.scatter(\n",
    "    merged_df,\n",
    "    x='depth',\n",
    "    y='heart_rate',\n",
    "    trendline='ols'\n",
    ")\n",
    "\n",
    "# Extract just the trendline (usually the 1st or 2nd trace depending on how many groups)\n",
    "trendline_trace = [trace for trace in trend_fig.data if trace.mode == 'lines'][0]\n",
    "\n",
    "# Add the regression line to your original colorful scatter plot\n",
    "fig.add_trace(trendline_trace)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_pca_plotly(\n",
    "    merged_df,\n",
    "    variables=None,\n",
    "    color_by='deployment_id',  # can be continuous (e.g., heart_rate) or categorical\n",
    "    n_components=2,\n",
    "    opacity=0.7\n",
    "):\n",
    "    if variables is None:\n",
    "        variables = ['heart_rate', 'stroke_rate', 'pitch', 'roll', 'heading', 'depth']\n",
    "\n",
    "    # Ensure all required columns are present\n",
    "    cols_needed = list(set(variables + [color_by, 'datetime']))\n",
    "    data = merged_df[cols_needed].dropna().copy()\n",
    "\n",
    "    # Standardize input variables\n",
    "    X = data[variables].values\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(X_scaled)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "    # Build DataFrame for Plotly\n",
    "    pca_df = pd.DataFrame(components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    pca_df['datetime'] = data['datetime'].values\n",
    "    pca_df[color_by] = data[color_by].values\n",
    "\n",
    "    # Include original variables for hover tooltips\n",
    "    for var in variables:\n",
    "        pca_df[var] = data[var].values\n",
    "\n",
    "    # Choose color scale depending on type\n",
    "    color_args = {'color': color_by}\n",
    "    if pd.api.types.is_numeric_dtype(data[color_by]):\n",
    "        color_args['color_continuous_scale'] = 'Viridis'\n",
    "    else:\n",
    "        color_args['color_discrete_sequence'] = px.colors.qualitative.Set2\n",
    "\n",
    "    # Plot\n",
    "    fig = px.scatter(\n",
    "        pca_df,\n",
    "        x='PC1', y='PC2',\n",
    "        title=f'PCA of Biologging Data colored by {color_by}',\n",
    "        labels={\n",
    "            'PC1': f'PC1 ({explained_var[0]*100:.1f}% variance)',\n",
    "            'PC2': f'PC2 ({explained_var[1]*100:.1f}% variance)',\n",
    "            color_by: color_by\n",
    "        },\n",
    "        hover_data=['datetime'] + variables + [color_by],\n",
    "        opacity=opacity,\n",
    "        **color_args\n",
    "    )\n",
    "\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()\n",
    "\n",
    "    return pca_df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_plotly(\n",
    "    megadata_pkl['derived_data']['merged_heart_rate_stroke_rate_prh_depth'],\n",
    "    color_by='heart_rate'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_plotly(\n",
    "    megadata_pkl['derived_data']['merged_heart_rate_stroke_rate_prh_depth'],\n",
    "    color_by='stroke_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_plotly(\n",
    "    megadata_pkl['derived_data']['merged_heart_rate_stroke_rate_prh_depth'],\n",
    "    color_by='roll'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_plotly(\n",
    "    megadata_pkl['derived_data']['merged_heart_rate_stroke_rate_prh_depth'],\n",
    "    color_by='depth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyologger utilities\n",
    "from pyologger.utils.folder_manager import *\n",
    "from pyologger.plot_data.plotter import *\n",
    "\n",
    "dataset_id = \"oror-adult-orca_hr-sr-vid_sw_JKB-PP\"\n",
    "deployment_id = \"2024-01-24_oror-001\"\n",
    "\n",
    "# Load important file paths and configurations\n",
    "config, data_dir, color_mapping_path, montage_path = load_configuration()\n",
    "# Streamlit load data\n",
    "animal_id, dataset_id, deployment_id, dataset_folder, deployment_folder, data_pkl, param_manager = select_and_load_deployment(\n",
    "    data_dir, dataset_id=dataset_id, deployment_id=deployment_id\n",
    "    )\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "netcdf_path = os.path.join(deployment_folder, 'outputs', f'{deployment_id}_output.nc')\n",
    "\n",
    "# Open the NetCDF file\n",
    "data = xr.open_dataset(netcdf_path)\n",
    "\n",
    "# Display the contents of the NetCDF file\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
