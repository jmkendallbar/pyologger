{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline for DiveDB\n",
    "Uses classes `info` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jessiekb/Documents/GitHub/pyologger\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from notion_client import Client\n",
    "from dotenv import load_dotenv\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import plot_tag_data\n",
    "import pickle\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query metadata\n",
    "Use Notion and [info entry form](https://forms.fillout.com/t/8UNuTLMaRfus) to start a recording and to generate identifiers for the Recording and Deployment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Notion secret token.\n",
      "Loaded database ID for dep_DB.\n",
      "Loaded database ID for rec_DB.\n",
      "Loaded database ID for logger_DB.\n",
      "Loaded database ID for animal_DB.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Processing Deployment Data:\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Asks the user for input to select a deployment folder to kick off the data reading process. In your folder name, you can have any suffix after Deployment ID. It will check and stop if there are two that fit.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Starts the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieve necessary data from the metadata database, including logger information.\n",
    "   - **Function Used:** `metadata.fetch_databases()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Group files by logger ID for processing.\n",
    "   - **Function Used:** `read_files()` (This is the main function)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verify if the outputs folder already contains processed files for each logger. Skip reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process UBE Files**:\n",
    "   - **Description:** For each UFI logger with UBE files, process and save the data.\n",
    "   - **Function Used:** `process_ube_file()`\n",
    "\n",
    "7. **Process CSV Files**:\n",
    "   - **Description:** For each logger with multiple CSV files, concatenate them, and save the combined data.\n",
    "   - **Function Used:** `concatenate_and_save_csvs()`\n",
    "\n",
    "8. **Final Outputs**:\n",
    "   - **Description:** Ensure all processed data is saved in the outputs folder with appropriate filenames.\n",
    "   - **Functions Used:** `save_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rec Date</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Animal</th>\n",
       "      <th>Start Time Precision</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Actual Start Time</th>\n",
       "      <th>Time Zone</th>\n",
       "      <th>Start time</th>\n",
       "      <th>Deployment Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-19</td>\n",
       "      <td>Third day of deployments at SeaWorld with Cork...</td>\n",
       "      <td>orca</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>2024-06-19_oror-001a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-18</td>\n",
       "      <td>Deployment second day at Sea World</td>\n",
       "      <td>orca</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-18_oror-001a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-17</td>\n",
       "      <td>ECG recordings with Ashley and Paul at SeaWorl...</td>\n",
       "      <td>orca</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-17_oror-002-001a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>Shuka ECG and CATS</td>\n",
       "      <td>orca</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-01-16_oror-002a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-06</td>\n",
       "      <td>Boat calibrations with Bill Hagey</td>\n",
       "      <td>boat</td>\n",
       "      <td>Approximative</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>09:31:30</td>\n",
       "      <td>2024-06-06_boat-001a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rec Date                                              Notes Animal  \\\n",
       "0  2024-06-19  Third day of deployments at SeaWorld with Cork...   orca   \n",
       "1  2024-06-18                 Deployment second day at Sea World   orca   \n",
       "2  2024-06-17  ECG recordings with Ashley and Paul at SeaWorl...   orca   \n",
       "3  2024-01-16                                 Shuka ECG and CATS   orca   \n",
       "4  2024-06-06                  Boat calibrations with Bill Hagey   boat   \n",
       "\n",
       "  Start Time Precision End Time Actual Start Time            Time Zone  \\\n",
       "0                 None     None              None  America/Los_Angeles   \n",
       "1                 None     None              None  America/Los_Angeles   \n",
       "2                 None     None              None  America/Los_Angeles   \n",
       "3                 None     None              None  America/Los_Angeles   \n",
       "4        Approximative     None              None  America/Los_Angeles   \n",
       "\n",
       "  Start time           Deployment Name  \n",
       "0   08:00:00      2024-06-19_oror-001a  \n",
       "1       None      2024-06-18_oror-001a  \n",
       "2       None  2024-06-17_oror-002-001a  \n",
       "3       None      2024-01-16_oror-002a  \n",
       "4   09:31:30      2024-06-06_boat-001a  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find your deployment ID index and remember it for the next cell, where you have to enter it.\n",
    "dep_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Displaying deployments to help you select one.\n",
      "            Deployment Name                                              Notes\n",
      "0      2024-06-19_oror-001a  Third day of deployments at SeaWorld with Cork...\n",
      "1      2024-06-18_oror-001a                 Deployment second day at Sea World\n",
      "2  2024-06-17_oror-002-001a  ECG recordings with Ashley and Paul at SeaWorl...\n",
      "3      2024-01-16_oror-002a                                 Shuka ECG and CATS\n",
      "4      2024-06-06_boat-001a                  Boat calibrations with Bill Hagey\n",
      "Step 1: You selected the deployment: 2024-01-16_oror-002a\n",
      "Description: Shuka ECG and CATS\n",
      "Step 2: Deployment folder path: /Users/jessiekb/Documents/GitHub/pyologger/data/2024-01-16_oror-002a\n",
      "Folder /Users/jessiekb/Documents/GitHub/pyologger/data/2024-01-16_oror-002a not found. Searching for folders with a similar name...\n",
      "Using the found folder: /Users/jessiekb/Documents/GitHub/pyologger/data/2024-01-16_oror-002a_Shuka-HR\n",
      "Ready to process deployment folder: /Users/jessiekb/Documents/GitHub/pyologger/data/2024-01-16_oror-002a_Shuka-HR\n",
      "Step 2: Deployment folder initialized at: /Users/jessiekb/Documents/GitHub/pyologger/data/2024-01-16_oror-002a_Shuka-HR\n",
      "Step 3: Fetching metadata...\n",
      "Metadata fetched successfully.\n",
      "Step 3.5: Importing notes...\n",
      "'datetime' column not found. Combining 'date' and 'time' columns.\n",
      "Localizing datetime using timezone America/Los_Angeles.\n",
      "Converting to UTC and Unix.\n",
      "Sampling frequency: 1.0036 Hz with a maximum time difference of 191.001 seconds\n",
      "Notes imported, processed, and sorted chronologically from 2024-01-16_oror-002a_00_Notes.xlsx.\n",
      "Notes imported successfully.\n",
      "Step 4: Organizing files by logger ID...\n",
      "Loggers with files: UF-01, CC-96\n",
      "Loggers without files: CO-68, NL-03, LL-06, MN-03, CC-35, MN-04, CO-66, WC-75, NL-01, CO-67, WC-93, MM-02, LL-05, UF-03, NL-02, MN-01, LL-04, UF-02, NL-04, UF-04\n",
      "Existing files in output folder: ['CC-96.csv', '2024-01-16_oror-002a.html', 'UF-01.csv', 'data.pkl', 'CC-96.parquet', 'UF-01.parquet']\n",
      "Files found for logger ID UF-01: ['UF-01.csv', 'UF-01.parquet']\n",
      "Files found for logger ID CC-96: ['CC-96.csv', 'CC-96.parquet']\n",
      "All necessary files are already processed and available in the outputs folder.\n",
      "All necessary files are already processed. Skipping further processing.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the metadata and dep_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(dep_db, data_dir)\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=True, save_parq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>short_description</th>\n",
       "      <th>long_description</th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime_utc</th>\n",
       "      <th>time_unix_ms</th>\n",
       "      <th>sec_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>10:01:00.170000</td>\n",
       "      <td>point</td>\n",
       "      <td>heartbeat_manual_ok</td>\n",
       "      <td>63.829787</td>\n",
       "      <td>heartbeat detection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-16 10:01:00.170000-08:00</td>\n",
       "      <td>2024-01-16 18:01:00.170000+00:00</td>\n",
       "      <td>1705428060170</td>\n",
       "      <td>-334.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>10:01:02.780000</td>\n",
       "      <td>point</td>\n",
       "      <td>heartbeat_manual_reject</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heartbeat detection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-16 10:01:02.780000-08:00</td>\n",
       "      <td>2024-01-16 18:01:02.780000+00:00</td>\n",
       "      <td>1705428062780</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>10:01:03.959000</td>\n",
       "      <td>point</td>\n",
       "      <td>exhalation_breath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exhalation followed by breath</td>\n",
       "      <td>start exhale  Breath; snapshots for first brea...</td>\n",
       "      <td>2024-01-16 10:01:03.959000-08:00</td>\n",
       "      <td>2024-01-16 18:01:03.959000+00:00</td>\n",
       "      <td>1705428063959</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>10:01:07</td>\n",
       "      <td>point</td>\n",
       "      <td>heartbeat_manual_reject</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heartbeat detection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-16 10:01:07-08:00</td>\n",
       "      <td>2024-01-16 18:01:07+00:00</td>\n",
       "      <td>1705428067000</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>10:01:07.830000</td>\n",
       "      <td>point</td>\n",
       "      <td>heartbeat_manual_ok</td>\n",
       "      <td>72.289157</td>\n",
       "      <td>heartbeat detection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-16 10:01:07.830000-08:00</td>\n",
       "      <td>2024-01-16 18:01:07.830000+00:00</td>\n",
       "      <td>1705428067830</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date             time   type                      key      value  \\\n",
       "0 2024-01-16  10:01:00.170000  point      heartbeat_manual_ok  63.829787   \n",
       "1 2024-01-16  10:01:02.780000  point  heartbeat_manual_reject        NaN   \n",
       "2 2024-01-16  10:01:03.959000  point        exhalation_breath        NaN   \n",
       "3 2024-01-16         10:01:07  point  heartbeat_manual_reject        NaN   \n",
       "4 2024-01-16  10:01:07.830000  point      heartbeat_manual_ok  72.289157   \n",
       "\n",
       "               short_description  \\\n",
       "0            heartbeat detection   \n",
       "1            heartbeat detection   \n",
       "2  exhalation followed by breath   \n",
       "3            heartbeat detection   \n",
       "4            heartbeat detection   \n",
       "\n",
       "                                    long_description  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  start exhale  Breath; snapshots for first brea...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                          datetime                     datetime_utc  \\\n",
       "0 2024-01-16 10:01:00.170000-08:00 2024-01-16 18:01:00.170000+00:00   \n",
       "1 2024-01-16 10:01:02.780000-08:00 2024-01-16 18:01:02.780000+00:00   \n",
       "2 2024-01-16 10:01:03.959000-08:00 2024-01-16 18:01:03.959000+00:00   \n",
       "3        2024-01-16 10:01:07-08:00        2024-01-16 18:01:07+00:00   \n",
       "4 2024-01-16 10:01:07.830000-08:00 2024-01-16 18:01:07.830000+00:00   \n",
       "\n",
       "    time_unix_ms  sec_diff  \n",
       "0  1705428060170   -334.86  \n",
       "1  1705428062780      2.61  \n",
       "2  1705428063959       NaN  \n",
       "3  1705428067000      4.22  \n",
       "4  1705428067830      0.83  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally look at first notes that have been read in\n",
    "#datareader.selected_deployment['Time Zone']\n",
    "#datareader.info['UF-01']\n",
    "datareader.notes_df[0:5]\n",
    "#datareader.data['CC-96']\n",
    "#datareader.data['UF-01']\n",
    "#datareader.metadata['channelnames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datareader.data['CC-96']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test = pd.read_csv(os.path.join(deployment_folder, \"2024-01-16_oror-002a_CC-96_001.csv\"))\n",
    "datareader.selected_deployment['Time Zone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the pickle file output\n",
    "\n",
    "Load in the generated pickle file to inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency for CC-96: 400 Hz\n",
      "Sampling frequency for UF-01: 100 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'400'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")\n",
    "\n",
    "data_pkl.info['CC-96']['datetime_metadata']['fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dives'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_pkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCC-96\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dives'"
     ]
    }
   ],
   "source": [
    "data_pkl.info['CC-96']['dives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.notes_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.data['UF-01']  #data['UF-01'][0:5] # browse column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.data['CC-96'][0:5] # browse column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data for plots\n",
    "Downsample high-resolution data and filter notes down to notes of interest to include in plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.info['CC-96']['datetime_metadata']['fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.data['CC-96']['depth'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "### Generic plot function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrations\n",
    "### Check Accel and Mag Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def check_AM(A, M=None, fs=None, find_incl=True):\n",
    "    \"\"\"\n",
    "    Compute field intensity of acceleration and magnetometer data,\n",
    "    and the inclination angle of the magnetic field (in degrees).\n",
    "    \n",
    "    Parameters:\n",
    "    A (numpy.ndarray or dict): An accelerometer sensor matrix with columns [ax, ay, az].\n",
    "                               Can also be a sensor data dictionary.\n",
    "    M (numpy.ndarray or None): A magnetometer sensor matrix with columns [mx, my, mz]. Optional.\n",
    "    fs (float or None): The sampling rate of the sensor data in Hz. Required if A is not a sensor dictionary.\n",
    "    find_incl (bool): Whether to compute and return the inclination angle. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    dict or numpy.ndarray: \n",
    "        - If find_incl is False: returns the field intensity as a numpy array.\n",
    "        - If find_incl is True: returns a dictionary with field intensity and inclination angle (in degrees).\n",
    "    \"\"\"\n",
    "    \n",
    "    fc = 5  # low-pass filter frequency in Hz\n",
    "\n",
    "    if isinstance(A, dict):\n",
    "        if M is not None:\n",
    "            if A['sampling_rate'] == M['sampling_rate'] and len(A['data']) == len(M['data']):\n",
    "                fs = A['sampling_rate']\n",
    "                A = A['data']\n",
    "                M = M['data']\n",
    "        else:\n",
    "            fs = A['sampling_rate']\n",
    "            A = A['data']\n",
    "        if len(A) == 0:\n",
    "            raise ValueError(\"No data found in input argument A\")\n",
    "    else:\n",
    "        if M is None and fs is None:\n",
    "            raise ValueError(\"Sampling rate (fs) is required if A is not a sensor dictionary\")\n",
    "        if fs is None:\n",
    "            raise ValueError(\"Need to specify sampling frequency for matrix arguments\")\n",
    "\n",
    "    # Handle single vector inputs\n",
    "    if M is not None and M.ndim == 1:\n",
    "        M = M.reshape(1, -1)\n",
    "    if A.ndim == 1:\n",
    "        A = A.reshape(1, -1)\n",
    "\n",
    "    # Check that sizes of A and M are compatible\n",
    "    if M is not None and A.shape[0] != M.shape[0]:\n",
    "        n = min(A.shape[0], M.shape[0])\n",
    "        A = A[:n, :]\n",
    "        M = M[:n, :]\n",
    "\n",
    "    # Low-pass filter the data if sampling rate is greater than 10 Hz\n",
    "    if fs > 10:\n",
    "        nf = int(round(4 * fs / fc))\n",
    "        if A.shape[0] > nf:\n",
    "            b, a = butter(4, fc / (fs / 2), btype='low')\n",
    "            A = filtfilt(b, a, A, axis=0)\n",
    "            if M is not None:\n",
    "                M = filtfilt(b, a, M, axis=0)\n",
    "\n",
    "    # Compute field intensity of the first input argument (A)\n",
    "    fstr = np.sqrt(np.sum(A**2, axis=1))\n",
    "    fstr = fstr.reshape(-1, 1)\n",
    "\n",
    "    if M is not None:\n",
    "        # Compute field intensity of the second input argument (M)\n",
    "        fstr2 = np.sqrt(np.sum(M**2, axis=1))\n",
    "        fstr2 = fstr2.reshape(-1, 1)\n",
    "        fstr = np.hstack((fstr, fstr2))\n",
    "\n",
    "    if find_incl and M is not None:\n",
    "        AMprod = np.sum(A * M, axis=1)\n",
    "        incl = -np.degrees(np.arcsin(AMprod / (fstr[:, 0] * fstr[:, 1])))\n",
    "        return {'fstr': fstr, 'incl': incl}\n",
    "    else:\n",
    "        return fstr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accel and Mag implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX'].values\n",
    "accY = data_pkl.data['CC-96']['accY'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ'].values\n",
    "magX = data_pkl.data['CC-96']['magX'].values\n",
    "magY = data_pkl.data['CC-96']['magY'].values\n",
    "magZ = data_pkl.data['CC-96']['magZ'].values\n",
    "\n",
    "# Combine the accelerometer and magnetometer data into nx3 matrices\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "mag_data = np.vstack((magX, magY, magZ)).T\n",
    "\n",
    "# Get the sampling rate from the data structure\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "# Call the check_AM function\n",
    "AMcheck = check_AM(acc_data, mag_data, sampling_rate)\n",
    "\n",
    "# Access the field intensity and inclination angle\n",
    "field_intensity_acc = AMcheck['fstr'][:, 0]  # Field intensity of accelerometer data\n",
    "field_intensity_mag = AMcheck['fstr'][:, 1]  # Field intensity of magnetometer data\n",
    "inclination_angle = AMcheck['incl']\n",
    "\n",
    "# Print results\n",
    "print(\"Field Intensity:\\n\", field_intensity_acc)\n",
    "print(\"Field Intensity:\\n\", field_intensity_mag)\n",
    "print(\"Inclination Angle (degrees):\\n\", inclination_angle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new fields to data_pkl.data['CC-96']\n",
    "data_pkl.data['CC-96']['field_intensity_acc'] = field_intensity_acc\n",
    "data_pkl.data['CC-96']['field_intensity_mag'] = field_intensity_mag\n",
    "data_pkl.data['CC-96']['inclination_angle'] = inclination_angle\n",
    "\n",
    "# Example usage in Streamlit\n",
    "imu_channels_to_plot = ['accX', 'accY', 'accZ', 'field_intensity_acc', 'field_intensity_mag', 'inclination_angle']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, ephys_channels=ephys_channels_to_plot, imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, time_range=(start_time, end_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import solve, inv\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "def fix_offset_3d(X):\n",
    "    \"\"\"\n",
    "    Estimate the offset in each axis of a triaxial field measurement.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray or dict): A sensor matrix or dictionary containing measurements from a triaxial field sensor.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with two elements:\n",
    "        - 'X': The adjusted triaxial sensor measurements (same size and units as input).\n",
    "        - 'G': A calibration dictionary containing the offset added to each column of X.\n",
    "    \"\"\"\n",
    "    poly1 = np.ones((3, 1))\n",
    "    poly2 = np.zeros((3, 1))\n",
    "    poly = np.hstack((poly1, poly2))\n",
    "    G = {'poly': poly}\n",
    "\n",
    "    if X is None:\n",
    "        raise ValueError(\"Input for X is required\")\n",
    "\n",
    "    if isinstance(X, dict):\n",
    "        x = X['data']\n",
    "    else:\n",
    "        x = X\n",
    "\n",
    "    if x.shape[1] != 3:\n",
    "        raise ValueError(\"Input data must be from a 3-axis sensor\")\n",
    "\n",
    "    # Filter valid (complete) rows\n",
    "    valid_rows = np.all(np.isfinite(x), axis=1)\n",
    "    x_valid = x[valid_rows, :]\n",
    "\n",
    "    # Compute the squared magnitude of each vector and the mean magnitude\n",
    "    bsq = np.sum(x_valid**2, axis=1)\n",
    "    mb = np.sqrt(np.mean(bsq))\n",
    "\n",
    "    XX = np.hstack((2 * x_valid, np.full((len(x_valid), 1), mb)))\n",
    "\n",
    "    R = np.dot(XX.T, XX)\n",
    "\n",
    "    if np.linalg.cond(R) > 1e3:\n",
    "        raise ValueError(\"Condition too poor to get reliable solution\")\n",
    "\n",
    "    P = np.dot(bsq, XX)\n",
    "    H = -solve(R, P)\n",
    "\n",
    "    G['poly'] = np.hstack((poly1, H[:3].reshape(3, 1)))\n",
    "\n",
    "    # Adjust the sensor data by adding the offset\n",
    "    x = x + H[:3]\n",
    "\n",
    "    if not isinstance(X, dict):\n",
    "        return {'X': x, 'G': G}\n",
    "\n",
    "    X['data'] = x\n",
    "\n",
    "    # Check and adjust for cal_map or cal_cross if present\n",
    "    if 'cal_map' in X:\n",
    "        G['poly'][:, 1] = np.dot(inv(X['cal_map']), G['poly'][:, 1])\n",
    "\n",
    "    if 'cal_cross' in X:\n",
    "        G['poly'][:, 1] = np.dot(inv(X['cal_cross']), G['poly'][:, 1])\n",
    "\n",
    "    X['cal_poly'] = G['poly']\n",
    "\n",
    "    # Update the history\n",
    "    if 'history' in X and X['history']:\n",
    "        X['history'].append('fix_offset_3d')\n",
    "    else:\n",
    "        X['history'] = ['fix_offset_3d']\n",
    "\n",
    "    return {'X': X, 'G': G}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX'].values\n",
    "accY = data_pkl.data['CC-96']['accY'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ'].values\n",
    "\n",
    "# Combine into a single matrix\n",
    "acc_data = np.vstack((accX, accY, accZ)).T  # Shape should be (n_samples, 3)\n",
    "\n",
    "# Apply the fix_offset_3d function\n",
    "result = fix_offset_3d(acc_data)\n",
    "\n",
    "# Extract the adjusted data and calibration info\n",
    "adjusted_data_acc = result['X']\n",
    "calibration_info_acc = result['G']\n",
    "\n",
    "print(\"Adjusted Data:\\n\", adjusted_data_acc)\n",
    "print(\"Calibration Info:\\n\", calibration_info_acc)\n",
    "\n",
    "data_pkl.data['CC-96']['accX_adjusted'] = adjusted_data_acc[:, 0]\n",
    "data_pkl.data['CC-96']['accY_adjusted'] = adjusted_data_acc[:, 1]\n",
    "data_pkl.data['CC-96']['accZ_adjusted'] = adjusted_data_acc[:, 2]\n",
    "data_pkl.info['CC-96']['calibration_info'] = {}\n",
    "data_pkl.info['CC-96']['calibration_info'] = calibration_info_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "magX = data_pkl.data['CC-96']['magX'].values\n",
    "magY = data_pkl.data['CC-96']['magY'].values\n",
    "magZ = data_pkl.data['CC-96']['magZ'].values\n",
    "\n",
    "# Combine into a single matrix\n",
    "mag_data = np.vstack((magX, magY, magZ)).T  # Shape should be (n_samples, 3)\n",
    "\n",
    "# Apply the fix_offset_3d function\n",
    "result = fix_offset_3d(mag_data)\n",
    "\n",
    "# Extract the adjusted data and calibration info\n",
    "adjusted_data_mag = result['X']\n",
    "calibration_info_mag = result['G']\n",
    "\n",
    "print(\"Adjusted Data:\\n\", adjusted_data_mag)\n",
    "print(\"Calibration Info:\\n\", calibration_info_mag)\n",
    "\n",
    "data_pkl.data['CC-96']['magX_adjusted'] = adjusted_data_mag[:, 0]\n",
    "data_pkl.data['CC-96']['magY_adjusted'] = adjusted_data_mag[:, 1]\n",
    "data_pkl.data['CC-96']['magZ_adjusted'] = adjusted_data_mag[:, 2]\n",
    "data_pkl.info['CC-96']['calibration_info'] = {}\n",
    "data_pkl.info['CC-96']['calibration_info'] = calibration_info_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX_adjusted'].values\n",
    "accY = data_pkl.data['CC-96']['accY_adjusted'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ_adjusted'].values\n",
    "magX = data_pkl.data['CC-96']['magX_adjusted'].values\n",
    "magY = data_pkl.data['CC-96']['magY_adjusted'].values\n",
    "magZ = data_pkl.data['CC-96']['magZ_adjusted'].values\n",
    "\n",
    "# Combine the accelerometer and magnetometer data into nx3 matrices\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "mag_data = np.vstack((magX, magY, magZ)).T\n",
    "\n",
    "# Get the sampling rate from the data structure\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "# Call the check_AM function\n",
    "AMcheck2 = check_AM(acc_data, mag_data, sampling_rate)\n",
    "\n",
    "# Access the field intensity and inclination angle\n",
    "field_intensity_acc2 = AMcheck2['fstr'][:, 0]  # Field intensity of accelerometer data\n",
    "field_intensity_mag2 = AMcheck2['fstr'][:, 1]  # Field intensity of magnetometer data\n",
    "inclination_angle2 = AMcheck2['incl']\n",
    "\n",
    "# Print results\n",
    "print(\"Field Intensity:\\n\", field_intensity_acc2)\n",
    "print(\"Field Intensity:\\n\", field_intensity_mag2)\n",
    "print(\"Inclination Angle (degrees):\\n\", inclination_angle2)\n",
    "\n",
    "# Save new fields to data_pkl.data['CC-96']\n",
    "data_pkl.data['CC-96']['field_intensity_acc2'] = field_intensity_acc2\n",
    "data_pkl.data['CC-96']['field_intensity_mag2'] = field_intensity_mag2\n",
    "data_pkl.data['CC-96']['inclination_angle2'] = inclination_angle2\n",
    "\n",
    "# Example usage in Streamlit\n",
    "imu_channels_to_plot = ['accX', 'accY', 'accZ', 'field_intensity_acc2', 'field_intensity_mag2', 'inclination_angle2']\n",
    "ephys_channels_to_plot = []\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "# Get the overlapping time range\n",
    "imu_df = data_pkl.data[imu_logger_to_use]\n",
    "ephys_df = data_pkl.data[ephys_logger_to_use]\n",
    "start_time = max(imu_df['datetime'].min(), ephys_df['datetime'].min()).to_pydatetime()\n",
    "end_time = min(imu_df['datetime'].max(), ephys_df['datetime'].max()).to_pydatetime()\n",
    "\n",
    "plot_tag_data_interactive(data_pkl, imu_channels_to_plot, ephys_channels=ephys_channels_to_plot, imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, time_range=(start_time, end_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Inclination Angle\n",
    "\n",
    "Here is where you would fix the axes of triaxial sensor data (if the sensor axis differs from the tag axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag frame to animal frame\n",
    "\n",
    "### Find dives method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import decimate, medfilt\n",
    "from scipy.linalg import norm, svd\n",
    "from scipy.stats import iqr, linregress\n",
    "\n",
    "def calculate_depth_rate(depth_data, sampling_rate, smoothing_factor=0.2):\n",
    "    \"\"\"\n",
    "    Calculate the rate of change of depth (depth rate).\n",
    "    \n",
    "    Parameters:\n",
    "    depth_data (numpy.ndarray): Depth data in meters.\n",
    "    sampling_rate (float): Sampling rate in Hz.\n",
    "    smoothing_factor (float): Smoothing factor to reduce noise in the derivative.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Smoothed depth rate.\n",
    "    \"\"\"\n",
    "    depth_diff = np.gradient(depth_data) * sampling_rate\n",
    "    smoothed_depth_rate = medfilt(depth_diff, kernel_size=int(smoothing_factor * sampling_rate))\n",
    "    return smoothed_depth_rate\n",
    "\n",
    "def prh_predictor2(depth_data, accel_data, sampling_rate, max_depth_threshold=10):\n",
    "    \"\"\"\n",
    "    Predict the tag position on a diving animal from depth and acceleration data.\n",
    "    \n",
    "    Parameters:\n",
    "    depth_data (numpy.ndarray): Depth vector in meters.\n",
    "    accel_data (numpy.ndarray): Acceleration matrix with columns ax, ay, az.\n",
    "    sampling_rate (float): The sampling rate of the sensor data in Hz.\n",
    "    max_depth_threshold (float): The maximum depth of near-surface dives. Default is 10 m.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with columns 'cue', 'p0', 'r0', 'h0', and 'q'.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_segment_length = 30  # minimum surface segment length in seconds\n",
    "    max_segment_length = 300  # maximum surface segment length in seconds\n",
    "    gap_time = 5  # gap time in seconds to avoid dive edges\n",
    "    \n",
    "    if depth_data is None or accel_data is None:\n",
    "        raise ValueError(\"prh_predictor2 requires inputs depth_data and accel_data.\")\n",
    "    \n",
    "    if sampling_rate is None:\n",
    "        raise ValueError(\"sampling_rate must be specified.\")\n",
    "    \n",
    "    # Decimate data to 5Hz if needed\n",
    "    if sampling_rate >= 7.5:\n",
    "        decimation_factor = round(sampling_rate / 5)\n",
    "        depth_data = decimate(depth_data, decimation_factor, zero_phase=True)\n",
    "        accel_data = decimate(accel_data, decimation_factor, axis=0, zero_phase=True)\n",
    "        sampling_rate /= decimation_factor\n",
    "\n",
    "    # Normalize acceleration to 1 g\n",
    "    accel_data = accel_data / np.linalg.norm(accel_data, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Calculate depth rate\n",
    "    depth_rate = calculate_depth_rate(depth_data, sampling_rate)\n",
    "\n",
    "    # Detect dive start/ends using the find_dives function\n",
    "    dive_times = find_dives(depth_data, min_depth_threshold=max_depth_threshold, \n",
    "                            sampling_rate=sampling_rate, duration_threshold=10)\n",
    "\n",
    "    if dive_times.shape[0] == 0:\n",
    "        raise ValueError(f\"No dives deeper than {max_depth_threshold:.0f} found - change max_depth_threshold.\")\n",
    "    \n",
    "    # Augment all dive-start and dive-end times by gap_time seconds\n",
    "    dive_times['start'] -= gap_time\n",
    "    dive_times['end'] += gap_time\n",
    "    \n",
    "    # Check for segments before first dive and after last dive\n",
    "    first_segment_start = max(dive_times.iloc[0]['start'] - max_segment_length, 0)\n",
    "    first_segment_end = dive_times.iloc[0]['start']\n",
    "    last_segment_start = dive_times.iloc[-1]['end']\n",
    "    last_segment_end = min(dive_times.iloc[-1]['end'] + max_segment_length, len(depth_data) / sampling_rate)\n",
    "    \n",
    "    # Adjust the first and last segments based on depth\n",
    "    first_segment_indices = np.where(depth_data[int(sampling_rate * first_segment_start):int(sampling_rate * first_segment_end)] > max_depth_threshold)[0]\n",
    "    if len(first_segment_indices) > 0:\n",
    "        first_segment_start += first_segment_indices[-1] / sampling_rate\n",
    "\n",
    "    last_segment_indices = np.where(depth_data[int(sampling_rate * last_segment_start):int(sampling_rate * last_segment_end)] > max_depth_threshold)[0]\n",
    "    if len(last_segment_indices) > 0:\n",
    "        last_segment_end = last_segment_start + (last_segment_indices[0] - 1) / sampling_rate\n",
    "\n",
    "    # Combine all segments\n",
    "    all_segments = np.vstack([\n",
    "        [first_segment_start, first_segment_end],\n",
    "        dive_times[['end', 'start']].values[:-1],\n",
    "        [last_segment_start, last_segment_end]\n",
    "    ])\n",
    "\n",
    "    # Filter out segments that are too short\n",
    "    segment_durations = np.diff(all_segments, axis=1)[:, 0]\n",
    "    valid_segments = all_segments[segment_durations > min_segment_length]\n",
    "    \n",
    "    # Break up long surfacing intervals\n",
    "    while True:\n",
    "        long_segments = np.where(np.diff(valid_segments, axis=1)[:, 0] > max_segment_length)[0]\n",
    "        if len(long_segments) == 0:\n",
    "            break\n",
    "        segment_index = long_segments[0]\n",
    "        valid_segments = np.vstack([\n",
    "            valid_segments[:segment_index],\n",
    "            [valid_segments[segment_index, 0], valid_segments[segment_index, 0] + max_segment_length],\n",
    "            [valid_segments[segment_index, 0] + max_segment_length, valid_segments[segment_index, 1]],\n",
    "            valid_segments[segment_index + 1:]\n",
    "        ])\n",
    "    \n",
    "    # Check for segments with sufficient variation in orientation\n",
    "    # orientation_variation = np.zeros(valid_segments.shape[0])\n",
    "    # for segment_index in range(valid_segments.shape[0]):\n",
    "    #     indices = np.arange(int(valid_segments[segment_index, 0] * sampling_rate), \n",
    "    #                         int(valid_segments[segment_index, 1] * sampling_rate))\n",
    "        \n",
    "    #     # Boundary check to avoid out-of-bounds indices\n",
    "    #     indices = indices[indices < accel_data.shape[0]]\n",
    "\n",
    "    #     orientation_variation[segment_index] = norm(np.std(accel_data[indices, :], axis=0))\n",
    "    \n",
    "    # variation_threshold = np.median(orientation_variation) + 1.5 * iqr(orientation_variation) * np.array([-1, 1])\n",
    "    # valid_segments = valid_segments[(orientation_variation > variation_threshold[0]) & \n",
    "    #                                  (orientation_variation < variation_threshold[1])]\n",
    "\n",
    "    # PRH inference\n",
    "    prh_data = np.empty((valid_segments.shape[0], 5))\n",
    "    for segment_index in range(valid_segments.shape[0]):\n",
    "        indices = np.arange(int(valid_segments[segment_index, 0] * sampling_rate), \n",
    "                            int(valid_segments[segment_index, 1] * sampling_rate))\n",
    "        \n",
    "        # Boundary check to avoid out-of-bounds indices\n",
    "        indices = indices[indices < accel_data.shape[0]]\n",
    "\n",
    "        prh = applymethod2(accel_data[indices, :], depth_rate, sampling_rate, valid_segments[segment_index, :])\n",
    "        if prh is not None:\n",
    "            prh_data[segment_index, :] = np.hstack([np.mean(valid_segments[segment_index, :]), prh])\n",
    "    \n",
    "    # Convert prh_data array to DataFrame\n",
    "    prh_df = pd.DataFrame(prh_data, columns=['cue', 'p0', 'r0', 'h0', 'q'])\n",
    "\n",
    "    return prh_df\n",
    "\n",
    "def applymethod2(accel_data, depth_rate, sampling_rate, segment_times):\n",
    "    \"\"\"\n",
    "    Apply PRH predictor method 2 to estimate tag-to-animal orientation.\n",
    "\n",
    "    Parameters:\n",
    "    accel_data (numpy.ndarray): Acceleration data for the segment.\n",
    "    depth_rate (numpy.ndarray): Depth rate data for the segment.\n",
    "    sampling_rate (float): The sampling rate of the sensor data in Hz.\n",
    "    segment_times (numpy.ndarray): Start and end times of the segment.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Estimated orientation angles [p0, r0, h0, q].\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the segment indices\n",
    "    segment_indices = np.arange(int(segment_times[0] * sampling_rate), int(segment_times[1] * sampling_rate))\n",
    "    segment_indices = segment_indices[segment_indices < accel_data.shape[0]]\n",
    "    \n",
    "    As = accel_data[segment_indices, :]\n",
    "    vs = depth_rate[segment_indices]\n",
    "\n",
    "    # Energy ratio between plane-of-motion and axis of rotation\n",
    "    QQ = As.T @ As  # Form outer product of acceleration\n",
    "    if np.any(np.isnan(QQ)):\n",
    "        return None\n",
    "\n",
    "    svd_out = svd(QQ)\n",
    "    pow_ratio = svd_out[1][2] / svd_out[1][1]  # Power ratio from singular values\n",
    "\n",
    "    # Axis of rotation to restore V to tag Y axis\n",
    "    aa = np.arccos(np.dot([0, 1, 0], svd_out[2][:, 2]))\n",
    "    Phi = np.cross([0, 1, 0], svd_out[2][:, 2]) / np.sin(aa)\n",
    "    S = np.array([[0, -Phi[2], Phi[1]],\n",
    "                  [Phi[2], 0, -Phi[0]],\n",
    "                  [-Phi[1], Phi[0], 0]])\n",
    "\n",
    "    Q = np.eye(3) + (1 - np.cos(aa)) * S @ S - np.sin(aa) * S  # Generate rotation matrix\n",
    "\n",
    "    am = np.mean(As, axis=0) @ Q.T\n",
    "    p0 = np.arctan2(am[0], am[2])\n",
    "    Q = euler2rotmat(p=p0, r=0, h=0) @ Q\n",
    "\n",
    "    prh = np.array([np.arcsin(Q[2, 0]), np.arctan2(Q[2, 1], Q[2, 2]), np.arctan2(Q[1, 0], Q[0, 0])])\n",
    "\n",
    "    aa_transformed = As @ Q[1, :].T\n",
    "    prh_quality = np.mean([pow_ratio, np.std(aa_transformed)])\n",
    "\n",
    "    # Check that h0 is not 180 degrees out by checking the regression\n",
    "    # between Aa[:, 0] and depth_rate is negative.\n",
    "    Q_final = euler2rotmat(prh[0], prh[1], prh[2])\n",
    "    Aa = As @ Q_final.T\n",
    "    slope, _, _, _, _ = linregress(Aa[:, 0], vs)\n",
    "\n",
    "    if slope > 0:\n",
    "        prh[2] = (prh[2] - np.pi) % (2 * np.pi)  # Correct if necessary\n",
    "\n",
    "    # Constrain r0 and h0 to the interval -pi:pi\n",
    "    for i in range(1, 3):\n",
    "        if abs(prh[i]) > np.pi:\n",
    "            prh[i] -= np.sign(prh[i]) * 2 * np.pi\n",
    "\n",
    "    return np.hstack((prh, prh_quality))\n",
    "\n",
    "def euler2rotmat(p, r, h):\n",
    "    \"\"\"\n",
    "    Convert Euler angles to a rotation matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    p (float): Pitch angle in radians.\n",
    "    r (float): Roll angle in radians.\n",
    "    h (float): Heading angle in radians.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: 3x3 rotation matrix.\n",
    "    \"\"\"\n",
    "    # Rotation matrix from Euler angles (assuming the same order of rotations as in R)\n",
    "    cp, sp = np.cos(p), np.sin(p)\n",
    "    cr, sr = np.cos(r), np.sin(r)\n",
    "    ch, sh = np.cos(h), np.sin(h)\n",
    "    \n",
    "    rot_matrix = np.array([\n",
    "        [ch * cr, ch * sr * sp - sh * cp, ch * sr * cp + sh * sp],\n",
    "        [sh * cr, sh * sr * sp + ch * cp, sh * sr * cp - ch * sp],\n",
    "        [-sr, cr * sp, cr * cp]\n",
    "    ])\n",
    "    \n",
    "    return rot_matrix\n",
    "\n",
    "\n",
    "# Example usage\n",
    "P = data_pkl.data['CC-96']['corrdepth'].values\n",
    "A = np.vstack((data_pkl.data['CC-96']['accX'].values, \n",
    "               data_pkl.data['CC-96']['accY'].values, \n",
    "               data_pkl.data['CC-96']['accZ'].values)).T\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "PRH = prh_predictor2(P, A, sampling_rate=sampling_rate, max_depth_threshold=1)\n",
    "print(PRH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_intensity_acc2 = AMcheck2['fstr'][:, 0]  # Field intensity of accelerometer data\n",
    "field_intensity_mag2 = AMcheck2['fstr'][:, 1]  # Field intensity of magnetometer data\n",
    "inclination_angle2 = AMcheck2['incl']\n",
    "\n",
    "# Print results\n",
    "print(\"Field Intensity:\\n\", field_intensity_acc2)\n",
    "print(\"Field Intensity:\\n\", field_intensity_mag2)\n",
    "print(\"Inclination Angle (degrees):\\n\", inclination_angle2)\n",
    "\n",
    "# Save new fields to data_pkl.data['CC-96']\n",
    "data_pkl.data['CC-96']['field_intensity_acc2'] = field_intensity_acc2\n",
    "data_pkl.data['CC-96']['field_intensity_mag2'] = field_intensity_mag2\n",
    "data_pkl.data['CC-96']['inclination_angle2'] = inclination_angle2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRH method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRH method 2\n",
    "for short-surfacing animals like beaked whales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import decimate\n",
    "from scipy.linalg import norm\n",
    "from scipy.stats import iqr\n",
    "\n",
    "# Helper function for PRH inference\n",
    "def prh_predictor2_gui(depth_data, accel_data, sampling_rate, max_depth_threshold=10):\n",
    "    \"\"\"\n",
    "    Predict the tag position on a diving animal from depth and acceleration data.\n",
    "    Returns: DataFrame with p0, r0, h0, and quality estimates.\n",
    "    \"\"\"\n",
    "    min_segment_length = 30  # minimum surface segment length in seconds\n",
    "    max_segment_length = 300  # maximum surface segment length in seconds\n",
    "    gap_time = 5  # gap time in seconds to avoid dive edges\n",
    "\n",
    "    # Decimate data to 5Hz if needed\n",
    "    if sampling_rate >= 7.5:\n",
    "        decimation_factor = round(sampling_rate / 5)\n",
    "        depth_data = decimate(depth_data, decimation_factor, zero_phase=True)\n",
    "        accel_data = decimate(accel_data, decimation_factor, axis=0, zero_phase=True)\n",
    "        sampling_rate /= decimation_factor\n",
    "\n",
    "    # Normalize acceleration to 1 g\n",
    "    accel_data = accel_data / np.linalg.norm(accel_data, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Detect dive start/ends using the find_dives function\n",
    "    dive_times = find_dives(depth_data, min_depth_threshold=max_depth_threshold, \n",
    "                            sampling_rate=sampling_rate, duration_threshold=10)\n",
    "\n",
    "    if dive_times.shape[0] == 0:\n",
    "        raise ValueError(f\"No dives deeper than {max_depth_threshold:.0f} found - change max_depth_threshold.\")\n",
    "\n",
    "    # Augment all dive-start and dive-end times by gap_time seconds\n",
    "    dive_times['start'] -= gap_time\n",
    "    dive_times['end'] += gap_time\n",
    "\n",
    "    # Initialize the PRH and quality estimates DataFrame\n",
    "    prh_data = []\n",
    "\n",
    "    for i, row in dive_times.iterrows():\n",
    "        start_time = int(row['start'] * sampling_rate)\n",
    "        end_time = int(row['end'] * sampling_rate)\n",
    "\n",
    "        # Analyze orientation segments\n",
    "        accel_segment = accel_data[start_time:end_time, :]\n",
    "        orientation_variation = norm(np.std(accel_segment, axis=0))\n",
    "        quality = np.abs(orientation_variation) / np.mean(orientation_variation)\n",
    "\n",
    "        # Estimate p0, r0, h0 (random example for simplicity)\n",
    "        p0 = np.mean(accel_segment[:, 0])\n",
    "        r0 = np.mean(accel_segment[:, 1])\n",
    "        h0 = np.mean(accel_segment[:, 2])\n",
    "\n",
    "        # Append the estimates to the list\n",
    "        prh_data.append({\n",
    "            'cue': row['tmax'],\n",
    "            'p0': p0,\n",
    "            'r0': r0,\n",
    "            'h0': h0,\n",
    "            'quality': quality\n",
    "        })\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    prh_data = pd.DataFrame(prh_data)\n",
    "\n",
    "    return prh_data\n",
    "\n",
    "# Example usage\n",
    "P = data_pkl.data['CC-96']['corrdepth'].values\n",
    "A = np.vstack((data_pkl.data['CC-96']['accX'].values, \n",
    "               data_pkl.data['CC-96']['accY'].values, \n",
    "               data_pkl.data['CC-96']['accZ'].values)).T\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "PRH = prh_predictor2(P, A, sampling_rate=sampling_rate, max_depth_threshold=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Stroke Frequency function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, welch\n",
    "from scipy.fft import fft\n",
    "from scipy import polyfit\n",
    "\n",
    "def dsf(acc_data, sampling_rate, fc=2.5, Nfft=None):\n",
    "    \"\"\"\n",
    "    Estimate the dominant stroke frequency from triaxial accelerometer data.\n",
    "\n",
    "    Parameters:\n",
    "    acc_data (numpy.ndarray): nx3 acceleration matrix with columns [ax, ay, az].\n",
    "    sampling_rate (float): The sampling rate of the sensor data in Hz.\n",
    "    fc (float, optional): The cut-off frequency in Hz of a low-pass filter to apply to acc_data. Defaults to 2.5 Hz.\n",
    "    Nfft (int, optional): The FFT length and therefore the frequency resolution. Defaults to the power of two closest to 20*sampling_rate.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with two elements:\n",
    "        - 'fpk': The dominant stroke frequency (Hz).\n",
    "        - 'q': The quality of the peak.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure acc_data is a numpy array\n",
    "    acc_data = np.asarray(acc_data)\n",
    "\n",
    "    # Determine the default FFT length if not provided\n",
    "    if Nfft is None:\n",
    "        Nfft = int(2**np.round(np.log2(20 * sampling_rate)))\n",
    "\n",
    "    # Low-pass filter\n",
    "    if fc and fc < (sampling_rate / 2):\n",
    "        b, a = butter(6, fc / (sampling_rate / 2), btype='low')\n",
    "        acc_data_filtered = filtfilt(b, a, acc_data, axis=0)\n",
    "    else:\n",
    "        acc_data_filtered = acc_data\n",
    "\n",
    "    # Calculate the power spectral density for each axis\n",
    "    freqs, power_spectrum = welch(acc_data_filtered, fs=sampling_rate, nperseg=Nfft, axis=0)\n",
    "\n",
    "    # Sum the power spectral densities across the three axes\n",
    "    summed_power_spectrum = np.sum(power_spectrum, axis=1)\n",
    "\n",
    "    # Find the frequency with the maximum power\n",
    "    max_power = np.max(summed_power_spectrum)\n",
    "    peak_index = np.argmax(summed_power_spectrum)\n",
    "\n",
    "    if 1 < peak_index < len(freqs) - 1:\n",
    "        # Quadratic interpolation to refine peak frequency\n",
    "        p = polyfit(freqs[peak_index-1:peak_index+2], summed_power_spectrum[peak_index-1:peak_index+2], 2)\n",
    "        fpk = -p[1] / (2 * p[0])\n",
    "    else:\n",
    "        fpk = freqs[peak_index]\n",
    "\n",
    "    # Quality of the peak\n",
    "    q = max_power / np.mean(summed_power_spectrum)\n",
    "\n",
    "    return {'fpk': fpk, 'q': q}\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX'].values\n",
    "accY = data_pkl.data['CC-96']['accY'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ'].values\n",
    "\n",
    "# Combine the accelerometer data into an nx3 matrix\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "\n",
    "# Get the sampling rate from the data structure\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])  # Replace with the correct path to your sampling rate if needed\n",
    "\n",
    "# Call the dsf function\n",
    "result = dsf(acc_data, sampling_rate)\n",
    "\n",
    "# Print the results\n",
    "print(\"Dominant Stroke Frequency (fpk):\", result['fpk'], \"Hz\")\n",
    "print(\"Quality of the Peak (q):\", result['q'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complementary Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def comp_filt(X, sampling_rate=None, fc=None):\n",
    "    \"\"\"\n",
    "    Complementary filtering of a signal.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray or dict): A sensor vector or matrix (signal in each column) or a sensor data dictionary.\n",
    "    sampling_rate (float): The sampling rate of the sensor data in Hz.\n",
    "    fc (list or float): Cut-off frequency/frequencies of the complementary filters in Hz. If one frequency is given, \n",
    "                        X will be split into low- and high-frequency components. If a list of frequencies is given, \n",
    "                        X will be split into multiple complementary bands.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with filtered signals. The keys correspond to the frequency bands: 'lowpass', 'highpass', and 'bandpass'.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(X, dict):\n",
    "        sampling_rate = X['sampling_rate']\n",
    "        X = X['data']\n",
    "    else:\n",
    "        if fc is None or sampling_rate is None:\n",
    "            raise ValueError(\"inputs X, sampling_rate, and fc are all required if X is not a dictionary\")\n",
    "\n",
    "    # Ensure fc is a list for consistency\n",
    "    if isinstance(fc, float) or isinstance(fc, int):\n",
    "        fc = [fc]\n",
    "\n",
    "    nf = [int(4 * sampling_rate / f) for f in fc]\n",
    "    Xf = {}\n",
    "\n",
    "    # Apply the complementary filters\n",
    "    for i, f in enumerate(fc):\n",
    "        b, a = butter(4, f / (sampling_rate / 2), btype='low')\n",
    "        lowpass = filtfilt(b, a, X, axis=0)\n",
    "        Xf[f'band_{i}'] = lowpass\n",
    "        X = X - lowpass  # Highpass component\n",
    "\n",
    "    # Store the final highpass component\n",
    "    Xf['highpass'] = X\n",
    "\n",
    "    # Handle the case where there's only one frequency cutoff\n",
    "    if len(fc) == 1:\n",
    "        Xf = {'lowpass': Xf['band_0'], 'highpass': Xf['highpass']}\n",
    "    \n",
    "    return Xf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complementary Filter Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming `data_pkl` is already loaded and contains your data\n",
    "accX = data_pkl.data['CC-96']['accX'].values\n",
    "accY = data_pkl.data['CC-96']['accY'].values\n",
    "accZ = data_pkl.data['CC-96']['accZ'].values\n",
    "\n",
    "# Combine the accelerometer data into an nx3 matrix\n",
    "acc_data = np.vstack((accX, accY, accZ)).T\n",
    "\n",
    "# Get the sampling rate from the data structure\n",
    "sampling_rate = int(data_pkl.info['CC-96']['datetime_metadata']['fs'])\n",
    "\n",
    "# Define the cut-off frequency (e.g., 0.15 Hz)\n",
    "fc = 0.15\n",
    "\n",
    "# Apply the complementary filter function\n",
    "filtered_signals = comp_filt(acc_data, sampling_rate, fc)\n",
    "\n",
    "# Extract lowpass and highpass components\n",
    "lowpass_signal = filtered_signals['lowpass']\n",
    "highpass_signal = filtered_signals['highpass']\n",
    "\n",
    "# Create a time axis (assuming continuous data)\n",
    "time_axis = np.arange(len(acc_data)) / sampling_rate\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
    "                    subplot_titles=(\"Low-Frequency Component\", \"High-Frequency Component\"))\n",
    "\n",
    "# Plot the low-frequency component\n",
    "fig.add_trace(go.Scatter(x=time_axis[::100], y=lowpass_signal[::100, 0], mode='lines', name='Lowpass X'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=time_axis[::100], y=lowpass_signal[::100, 1], mode='lines', name='Lowpass Y'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=time_axis[::100], y=lowpass_signal[::100, 2], mode='lines', name='Lowpass Z'), row=1, col=1)\n",
    "\n",
    "# Plot the high-frequency component\n",
    "fig.add_trace(go.Scatter(x=time_axis[::100], y=highpass_signal[::100, 0], mode='lines', name='Highpass X'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=time_axis[::100], y=highpass_signal[::100, 1], mode='lines', name='Highpass Y'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=time_axis[::100], y=highpass_signal[::100, 2], mode='lines', name='Highpass Z'), row=2, col=1)\n",
    "\n",
    "# Update the layout for better visualization\n",
    "fig.update_layout(title=\"Complementary Filtered Signals\", height=600, showlegend=True)\n",
    "fig.update_xaxes(title_text=\"Time (seconds)\")\n",
    "fig.update_yaxes(title_text=\"Signal Amplitude\")\n",
    "\n",
    "# Show the plot in Streamlit (if using Streamlit)\n",
    "# st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Or display the plot in a Jupyter notebook or other environments\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Color mapping dictionary with pastel, colorblind-friendly colors\n",
    "color_mapping = {\n",
    "    'ECG': '#FFCCCC',              # Light Red with alpha in rgba\n",
    "    'Depth': '#00008B',            # Dark Blue\n",
    "    'Accelerometer X [m/s²]': '#87CEFA',          # Light Blue\n",
    "    'Accelerometer Y [m/s²]': '#98FB98',          # Pale Green\n",
    "    'Accelerometer Z [m/s²]': '#FF6347',          # Light Coral\n",
    "    'Gyro X': '#9370DB',           # Medium Purple\n",
    "    'Gyro Y': '#BA55D3',           # Medium Orchid\n",
    "    'Gyro Z': '#8A2BE2',           # Blue Violet\n",
    "    'Mag X': '#FFD700',            # Gold\n",
    "    'Mag Y': '#FFA500',            # Orange\n",
    "    'Mag Z': '#FF8C00',            # Dark Orange\n",
    "    'Filtered Heartbeats': '#808080',  # Gray for dotted lines\n",
    "}\n",
    "\n",
    "def plot_tag_data(data_pkl, imu_channels, ephys_channels=None, imu_logger=None, ephys_logger=None, imu_sampling_rate=10, ephys_sampling_rate=50, draw=True):\n",
    "    if not imu_logger and not ephys_logger:\n",
    "        raise ValueError(\"At least one logger (imu_logger or ephys_logger) must be specified.\")\n",
    "\n",
    "    # Ensure the order of channels: ECG, Depth, Accel, Gyro, Mag\n",
    "    ordered_channels = []\n",
    "    if ephys_channels and 'ecg' in [ch.lower() for ch in ephys_channels]:\n",
    "        ordered_channels.append(('ECG', 'ecg'))\n",
    "    if 'depth' in [ch.lower() for ch in imu_channels]:\n",
    "        ordered_channels.append(('Depth', 'depth'))\n",
    "    if any(ch.lower() in ['accx', 'accy', 'accz'] for ch in imu_channels):\n",
    "        ordered_channels.append(('Accel', ['accX', 'accY', 'accZ']))\n",
    "    if any(ch.lower() in ['gyrx', 'gyry', 'gyrz'] for ch in imu_channels):\n",
    "        ordered_channels.append(('Gyro', ['gyrX', 'gyrY', 'gyrZ']))\n",
    "    if any(ch.lower() in ['magx', 'magy', 'magz'] for ch in imu_channels):\n",
    "        ordered_channels.append(('Mag', ['magX', 'magY', 'magZ']))\n",
    "\n",
    "    # Calculate the number of rows needed\n",
    "    num_rows = len(ordered_channels)\n",
    "\n",
    "    fig = make_subplots(rows=num_rows, cols=1, shared_xaxes=True, vertical_spacing=0.03)\n",
    "    \n",
    "    def downsample(df, original_fs, target_fs):\n",
    "        if target_fs >= original_fs:\n",
    "            return df\n",
    "        conversion_factor = int(original_fs / target_fs)\n",
    "        return df.iloc[::conversion_factor, :]\n",
    "\n",
    "    if imu_logger:\n",
    "        imu_df = data_pkl.data[imu_logger]\n",
    "        imu_fs = 1 / imu_df['datetime'].diff().dt.total_seconds().mean()\n",
    "        imu_df_downsampled = downsample(imu_df, imu_fs, imu_sampling_rate)\n",
    "        imu_info = data_pkl.info[imu_logger]['channelinfo']\n",
    "    \n",
    "    if ephys_logger:\n",
    "        ephys_df = data_pkl.data[ephys_logger]\n",
    "        ephys_fs = 1 / ephys_df['datetime'].diff().dt.total_seconds().mean()\n",
    "        ephys_df_downsampled = downsample(ephys_df, ephys_fs, ephys_sampling_rate)\n",
    "        ephys_info = data_pkl.info[ephys_logger]['channelinfo']\n",
    "\n",
    "    row_counter = 1\n",
    "    \n",
    "    for channel_type, channels in ordered_channels:\n",
    "        if channel_type == 'ECG' and ephys_channels and 'ecg' in [ch.lower() for ch in ephys_channels]:\n",
    "            # Plot ECG\n",
    "            channel = 'ecg'\n",
    "            df = ephys_df_downsampled\n",
    "            info = ephys_info\n",
    "            original_name = info[channel]['original_name']\n",
    "            unit = info[channel]['unit']\n",
    "\n",
    "            y_data = df[channel]\n",
    "            x_data = df['datetime']\n",
    "\n",
    "            y_label = f\"{original_name} [{unit}]\"\n",
    "            color = color_mapping.get(original_name, color_mapping['ECG'])\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_data,\n",
    "                y=y_data,\n",
    "                mode='lines',\n",
    "                name=y_label,\n",
    "                line=dict(color=color)\n",
    "            ), row=row_counter, col=1)\n",
    "\n",
    "            # Add vertical lines for heartbeats\n",
    "            filtered_notes = data_pkl.notes_df[data_pkl.notes_df['key'] == 'heartbeat_manual_ok']\n",
    "            if not filtered_notes.empty:\n",
    "                for dt in filtered_notes['datetime']:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[dt, dt],\n",
    "                        y=[y_data.min(), y_data.max()],\n",
    "                        mode='lines',\n",
    "                        line=dict(color=color_mapping['Filtered Heartbeats'], width=1, dash='dot'),\n",
    "                        showlegend=False\n",
    "                    ), row=row_counter, col=1)\n",
    "\n",
    "            fig.update_yaxes(title_text=y_label, row=row_counter, col=1)\n",
    "            row_counter += 1\n",
    "\n",
    "        elif channel_type == 'Depth' and 'depth' in [ch.lower() for ch in imu_channels]:\n",
    "            # Plot Depth\n",
    "            channel = 'depth'\n",
    "            df = imu_df_downsampled\n",
    "            info = imu_info\n",
    "            original_name = info[channel]['original_name']\n",
    "            unit = info[channel]['unit']\n",
    "\n",
    "            y_data = df[channel]\n",
    "            x_data = df['datetime']\n",
    "\n",
    "            y_label = f\"{original_name} [{unit}]\"\n",
    "            color = color_mapping.get(original_name, color_mapping['Depth'])\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_data,\n",
    "                y=y_data,\n",
    "                mode='lines',\n",
    "                name=y_label,\n",
    "                line=dict(color=color)\n",
    "            ), row=row_counter, col=1)\n",
    "\n",
    "            fig.update_yaxes(title_text=y_label, autorange=\"reversed\", row=row_counter, col=1)\n",
    "            row_counter += 1\n",
    "\n",
    "        elif channel_type in ['Accel', 'Gyro', 'Mag']:\n",
    "            # Plot Accel, Gyro, or Mag channels together\n",
    "            for sub_channel in channels:\n",
    "                if sub_channel in imu_df_downsampled.columns:\n",
    "                    df = imu_df_downsampled\n",
    "                    info = imu_info\n",
    "                    original_name = info[sub_channel]['original_name']\n",
    "                    unit = info[sub_channel]['unit']\n",
    "\n",
    "                    y_data = df[sub_channel]\n",
    "                    x_data = df['datetime']\n",
    "\n",
    "                    y_label = f\"{original_name} [{unit}]\"\n",
    "                    color = color_mapping.get(original_name, '#000000')\n",
    "\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=x_data,\n",
    "                        y=y_data,\n",
    "                        mode='lines',\n",
    "                        name=y_label,\n",
    "                        line=dict(color=color)\n",
    "                    ), row=row_counter, col=1)\n",
    "\n",
    "            fig.update_yaxes(title_text=f\"{channel_type} [{unit}]\", row=row_counter, col=1)\n",
    "            row_counter += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=200 * num_rows,\n",
    "        width=1200,\n",
    "        title_text=f\"{data_pkl.selected_deployment['Deployment Name']}\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Datetime\", row=row_counter-1, col=1)\n",
    "\n",
    "    if draw:\n",
    "        fig.show()\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "# Example usage:\n",
    "# Specify channels and loggers\n",
    "imu_channels_to_plot = ['depth', 'accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ', 'magX', 'magY', 'magZ']\n",
    "ephys_channels_to_plot = ['ecg']\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "plot_tag_data(data_pkl, imu_channels_to_plot, ephys_channels=ephys_channels_to_plot, imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use, imu_sampling_rate=10, ephys_sampling_rate=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.info['channelnames']['CC-96']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming your new data columns are as follows:\n",
    "# ECG signal column: 'ecg'\n",
    "# Depth column: 'depth1'\n",
    "# Accelerometer columns: 'accX', 'accY', 'accZ'\n",
    "# Gyroscope column: 'gyrY'\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=5, cols=1, shared_xaxes=True, vertical_spacing=0.03)\n",
    "\n",
    "# Add Heart Rate (bpm) plot at the top\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=filtered_notes['datetime'], \n",
    "    y=filtered_notes['value'], \n",
    "    mode='markers', \n",
    "    marker=dict(color='gray', size=8, symbol='circle-open'),\n",
    "    name='Heart rate (bpm)'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Add ECG plot with light red color and alpha 0.2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ecg_df50['datetime'], \n",
    "    y=ecg_df50['ecg'], \n",
    "    mode='lines', \n",
    "    name='ECG [mV]', \n",
    "    line=dict(color='rgba(255, 0, 0, 0.2)')\n",
    "), row=2, col=1)\n",
    "\n",
    "# Add vertical dotted lines for detected heartbeats\n",
    "for dt in filtered_notes['datetime']:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[dt, dt], \n",
    "        y=[ecg_df50['ecg'].min(), ecg_df50['ecg'].max()], \n",
    "        mode='lines', \n",
    "        line=dict(color='gray', width=1, dash='dot'),\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "\n",
    "# Add Depth plot with dark blue color\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['depth1'], \n",
    "    mode='lines', \n",
    "    name='Depth [m]', \n",
    "    line=dict(color='darkblue')\n",
    "), row=3, col=1)\n",
    "fig.update_yaxes(autorange=\"reversed\", row=3, col=1)\n",
    "\n",
    "# Add Accelerometer plots on the same y-axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['accX'], \n",
    "    mode='lines', \n",
    "    name='Accel X [m/s²]', \n",
    "    line=dict(color='blue')\n",
    "), row=4, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['accY'], \n",
    "    mode='lines', \n",
    "    name='Accel Y [m/s²]', \n",
    "    line=dict(color='green')\n",
    "), row=4, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['accZ'], \n",
    "    mode='lines', \n",
    "    name='Accel Z [m/s²]', \n",
    "    line=dict(color='red')\n",
    "), row=4, col=1)\n",
    "\n",
    "# Add Gyroscope Y plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['gyrY'], \n",
    "    mode='lines', \n",
    "    name='Gyr Y [mrad/s]', \n",
    "    line=dict(color='purple')\n",
    "), row=5, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    width=1200, \n",
    "    title_text=f\"{data_pkl.selected_deployment['Deployment Name']}\", \n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Datetime\", row=5, col=1)\n",
    "\n",
    "# Update y-axes labels\n",
    "fig.update_yaxes(title_text=\"Heart rate (bpm)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"ECG [mV]\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Depth [m]\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accelerometer [m/s²]\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Gyr Y [mrad/s]\", row=5, col=1)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the interactive plot as an HTML file\n",
    "fig.write_html(os.path.join(deployment_folder, \"outputs\", f\"{data_pkl.selected_deployment['Deployment Name']}.html\")) \n",
    "data_pkl.selected_deployment['Deployment Name']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
