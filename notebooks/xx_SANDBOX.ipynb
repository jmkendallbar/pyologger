{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.param_manager import ParamManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jessiekb/Documents/GitHub/pyologger\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    *empty*\n",
      "Attributes: (12/64)\n",
      "    File_Creation_Date:                21-Jun-2024 20:31:18\n",
      "    File_MATLAB_Version:               9.14.0.2489007 (R2023a) Update 6\n",
      "    File_R_Version:                    4.2.1\n",
      "    File_aniMotum_Version:             1.1-04\n",
      "    File_IKNOS_DA-ZOC_Version:         2.3/1.2\n",
      "    File_Contents:                     This file contains processed biologgin...\n",
      "    ...                                ...\n",
      "    Tags_TDR2_ID:                      \n",
      "    Tags_TDR2_Comments:                \n",
      "    Tags_TDR3_Manufacturer:            \n",
      "    Tags_TDR3_Model:                   \n",
      "    Tags_TDR3_ID:                      \n",
      "    Tags_TDR3_Comments:                \n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import pickle\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.calibrate_data.calibrate_acc_mag import *\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")\n",
    "import xarray as xr\n",
    "\n",
    "# Path to your NetCDF file\n",
    "file_path = os.path.join(data_dir, 'sample_netcdf_files', 'NESE_TrackingDiving_Data','2004001_TrackTDR_Processed.nc')\n",
    "\n",
    "# Load the NetCDF file into an xarray Dataset\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Display the contents of the Dataset\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xarray.Dataset {\n",
      "dimensions:\n",
      "\n",
      "variables:\n",
      "\n",
      "// global attributes:\n",
      "\t:File_Creation_Date = 21-Jun-2024 20:31:18 ;\n",
      "\t:File_MATLAB_Version = 9.14.0.2489007 (R2023a) Update 6 ;\n",
      "\t:File_R_Version = 4.2.1 ;\n",
      "\t:File_aniMotum_Version = 1.1-04 ;\n",
      "\t:File_IKNOS_DA-ZOC_Version = 2.3/1.2 ;\n",
      "\t:File_Contents = This file contains processed biologging data from one deployment (single individual,single trip). Both tracking data (processed with aniMotum) and dive statistics(processed with custom code) are provided if available. Dive statistics from the native sampling rate of the instrument (e.g. TDR1) and at 8 second (or 0.125 Hz) sampling intervals (TDR1_8S) are provided. For additional processing details please see the associated paper (Costa et al., 2023).Raw data are also available at https://doi.org/10.7291/D10D61 ;\n",
      "\t:Data_Owner = Daniel Costa ;\n",
      "\t:Data_Public = Yes: data can be used freely as long as data owner is properly cited. We strongly recommend reaching out to the data owner or another of the coauthors (D.Crocker, R.Holser, P.Robinson) for additional information about the study system. Offers of coauthorship would be appreciated, especially given the unique natural history of this organism and the considerable effort required to collect these data. ;\n",
      "\t:Citation_Paper = Costa, D.P., Holser, R.R., et al. (2024), Two Decades of Three-Dimensional Movement Data from Adult Female Northern Elephant Seals. Scientific Data. ;\n",
      "\t:Citation_Paper_DOI =  ;\n",
      "\t:Citation_Dataset = Costa, D.P., Holser, R.R., et al. (2024), Northern Elephant Seal Tracking and Diving Data - Processed, Dryad, Dataset, https://doi.org/10.7291/D18D7W ;\n",
      "\t:Citation_Dataset_DOI = 10.7291/D18D7W ;\n",
      "\t:Data_Type = Tracking and diving time-series data ;\n",
      "\t:Data_Assembly_By = UCSC/Rachel Holser ;\n",
      "\t:Data_Timezone = UTC ;\n",
      "\t:Animal_ID = R881 ;\n",
      "\t:Animal_Species = Mirounga angustirostris ;\n",
      "\t:Animal_Species_CommonName = northern elephant seal ;\n",
      "\t:Animal_Sex = F ;\n",
      "\t:Animal_AgeClass = Adult ;\n",
      "\t:Animal_BirthYear = 2000.0 ;\n",
      "\t:Animal_HadPup = N/A ;\n",
      "\t:Animal_OtherDeployments = 2004001 ;\n",
      "\t:Deployment_ID = 2004001.0 ;\n",
      "\t:Deployment_Year = 2004.0 ;\n",
      "\t:Deployment_Trip = PB ;\n",
      "\t:Deployment_InstrumentsRecovered? = Y ;\n",
      "\t:Deployment_Manipulation? = N ;\n",
      "\t:Deployment_ManipulationType =  ;\n",
      "\t:Deployment_Departure_Location = ANNU ;\n",
      "\t:Deployment_Departure_Lat = 37.116396 ;\n",
      "\t:Deployment_Departure_Lon = -122.330756 ;\n",
      "\t:Deployment_Departure_Datetime = 02/23/2004 09:45 ;\n",
      "\t:Deployment_Arrival_Location = ANNU ;\n",
      "\t:Deployment_Arrival_Lat = 37.116396 ;\n",
      "\t:Deployment_Arrival_Lon = -122.330756 ;\n",
      "\t:Deployment_Arrival_Datetime = 05/07/2004 23:00 ;\n",
      "\t:Data_Track_QCFlag = 1.0 ;\n",
      "\t:Data_TDR1_QCFlag = 1.0 ;\n",
      "\t:Data_TDR2_QCFlag = 5.0 ;\n",
      "\t:Data_TDR3_QCFlag = 5.0 ;\n",
      "\t:Data_TDR1_SamplingFrequency_Hz = nan ;\n",
      "\t:Data_TDR1_DepthResolution_m = nan ;\n",
      "\t:Data_TDR2_SamplingFrequency_Hz = nan ;\n",
      "\t:Data_TDR2_DepthResolution_m = nan ;\n",
      "\t:Data_TDR3_SamplingFrequency_Hz = nan ;\n",
      "\t:Data_TDR3_DepthResolution_m = nan ;\n",
      "\t:Tags_SatTag_Manufacturer = SMRU ;\n",
      "\t:Tags_SatTag_Model =  ;\n",
      "\t:Tags_SatTag_ID =  ;\n",
      "\t:Tags_PTT = 37594.0 ;\n",
      "\t:Tags_SatTag_Comments =  ;\n",
      "\t:Tags_TDR1_Manufacturer = Wildlife Computers ;\n",
      "\t:Tags_TDR1_Model = Mk9 ;\n",
      "\t:Tags_TDR1_ID = 390832 ;\n",
      "\t:Tags_TDR1_Comments =  ;\n",
      "\t:Tags_TDR2_Manufacturer =  ;\n",
      "\t:Tags_TDR2_Model =  ;\n",
      "\t:Tags_TDR2_ID =  ;\n",
      "\t:Tags_TDR2_Comments =  ;\n",
      "\t:Tags_TDR3_Manufacturer =  ;\n",
      "\t:Tags_TDR3_Model =  ;\n",
      "\t:Tags_TDR3_ID =  ;\n",
      "\t:Tags_TDR3_Comments =  ;\n",
      "}"
     ]
    }
   ],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(ds.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Path to your NetCDF file\n",
    "file_path = os.path.join(data_dir, 'sample_netcdf_files', 'TESTTEMPLATE_deployment_data.nc')\n",
    "\n",
    "# Load the NetCDF file into an xarray Dataset\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Display the contents of the Dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Path to your NetCDF file\n",
    "file_path = os.path.join(data_dir, 'sample_netcdf_files', '2024-01-16_oror-002a.nc')\n",
    "\n",
    "# Load the NetCDF file into an xarray Dataset\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Display the contents of the Dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sensor_data_ecg\n",
    "# Plotting the sensor_data_ecg variable\n",
    "ds.sensor_data_ecg.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string attribute back to a dictionary\n",
    "sensor_data_str = ds.attrs['sensor_data']\n",
    "sensor_data = json.loads(sensor_data_str)\n",
    "\n",
    "# Access the gyroscope DataFrame\n",
    "gyroscope_df = pd.DataFrame(sensor_data['gyroscope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dephist_deploy_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "param_manager = ParamManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to DiveDB\n",
    "\n",
    "Make sure your local DiveDB servers are running. To do so:\n",
    "- Navigate to the DiveDB directory\n",
    "- Run the command: `make up`\n",
    "- Wait until all services are running (Django, Postgres, Jupyter)\n",
    "- Make sure you have run the latest migrations: `make migrate`\n",
    "- Make sure you imported the latest logger and animal databases: `make importmetadata`\n",
    "\n",
    "Then, you're ready to upload data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow Django to run with async unsafe to run outside of Django server\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "metadata = {\n",
    "    \"animal\": datareader.animal_info[\"Animal ID\"],\n",
    "    \"deployment\": datareader.deployment_info[\"Deployment ID\"],\n",
    "    \"recording\": datareader.deployment_info[\"Recording ID\"].split(\", \")[1]\n",
    "}\n",
    "\n",
    "data_uploader.upload_netcdf('./data/2024-01-16_oror-002a/outputs/deployment_data.nc', metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "df = duckpond.get_delta_data(    \n",
    "    signal_names=[\"sensor_data_ecg\", \"sensor_data_light\", \"sensor_data_temperature\", \"sensor_data_depth\"],\n",
    "    animal_ids=\"mian-001\", # Make sure this matches the animal ID you uploaded\n",
    "    frequency=100,\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In progress: export to EDF\n",
    "\n",
    "Ideally, this would let you maintain the varied sampling frequencies from sensor_data and take the metadata from sensor_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "def export_concatenated_to_edf(concatenated_df, highest_sampling_frequency, latest_start_time, edf_filename_template):\n",
    "    \"\"\"\n",
    "    Exports the concatenated DataFrame to an EDF file.\n",
    "\n",
    "    Parameters:\n",
    "    - concatenated_df: The DataFrame containing concatenated data from all loggers.\n",
    "    - highest_sampling_frequency: The highest sampling frequency among the loggers.\n",
    "    - latest_start_time: The latest start time among the loggers.\n",
    "    - edf_filename_template: Template string for the EDF filename.\n",
    "                             The string should contain `{sensor}` to be replaced with 'ALL'.\n",
    "    \"\"\"\n",
    "    if concatenated_df is None or concatenated_df.empty:\n",
    "        print(\"No data available for export. Exiting.\")\n",
    "        return\n",
    "\n",
    "    ch_names = concatenated_df.columns.tolist()\n",
    "    sfreq = highest_sampling_frequency\n",
    "\n",
    "    # Check if there are any channels to process\n",
    "    if len(ch_names) == 0:\n",
    "        print(\"No valid channels found to export. Exiting.\")\n",
    "        return\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='misc')  # Adjust ch_types as necessary\n",
    "\n",
    "    # Convert datetime to (seconds, microseconds) tuple for the latest start time\n",
    "    meas_date = (int(latest_start_time.timestamp()), int((latest_start_time.timestamp() % 1) * 1e6))\n",
    "\n",
    "    # Create MNE RawArray\n",
    "    data = concatenated_df.values.T\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.set_meas_date(meas_date)\n",
    "\n",
    "    # Step 8: Define the EDF filename and save the EDF file\n",
    "    edf_filename = edf_filename_template.format(sensor='ALL')\n",
    "\n",
    "    print(f\"Saving EDF file as {edf_filename} with shape {data.shape}.\")\n",
    "\n",
    "    # Ensure that data is within the physical range EDF expects\n",
    "    raw.export(edf_filename, fmt='edf')\n",
    "\n",
    "    print(f\"EDF file saved as {edf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.sampling import *\n",
    "import pandas as pd\n",
    "def concatenate_logger_data(datareader):\n",
    "    \"\"\"\n",
    "    Concatenates data from all loggers stored in `datareader.data`.\n",
    "\n",
    "    Parameters:\n",
    "    - datareader: The DataReader object containing logger data in `datareader.data`.\n",
    "\n",
    "    Returns:\n",
    "    - concatenated_df: A pandas DataFrame with the concatenated data from all loggers.\n",
    "    - highest_sampling_frequency: The highest sampling frequency found among the loggers.\n",
    "    - latest_start_time: The latest start time among the loggers.\n",
    "    \"\"\"\n",
    "    logger_data_info = {}\n",
    "\n",
    "    # Step 1: Extract start time, end time, and sampling frequency for each logger\n",
    "    for logger_id, df in datareader.data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"Logger {logger_id} does not contain a valid DataFrame. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if 'datetime' not in df.columns:\n",
    "            print(f\"Logger {logger_id} does not have a 'datetime' column. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        start_time = df['datetime'].iloc[0]\n",
    "        end_time = df['datetime'].iloc[-1]\n",
    "        sampling_frequency = round(1 / df['datetime'].diff().dt.total_seconds().mean())\n",
    "\n",
    "        logger_data_info[logger_id] = {\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'sampling_frequency': sampling_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"Logger {logger_id}: start_time={start_time}, end_time={end_time}, sampling_frequency={sampling_frequency} Hz\")\n",
    "\n",
    "    if not logger_data_info:\n",
    "        print(\"No valid logger data found. Exiting.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Step 2: Determine the latest start time, earliest end time, and highest sampling frequency\n",
    "    latest_start_time = max(info['start_time'] for info in logger_data_info.values())\n",
    "    earliest_end_time = min(info['end_time'] for info in logger_data_info.values())\n",
    "    highest_sampling_frequency = max(info['sampling_frequency'] for info in logger_data_info.values())\n",
    "\n",
    "    print(f\"Latest start time: {latest_start_time}\")\n",
    "    print(f\"Earliest end time: {earliest_end_time}\")\n",
    "    print(f\"Highest sampling frequency: {highest_sampling_frequency} Hz\")\n",
    "\n",
    "    # Step 3: Initialize an empty DataFrame for concatenation\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Step 4: Crop dataframes, upsample as necessary, and concatenate\n",
    "    for logger_id, df in datareader.data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "\n",
    "        # Crop dataframe\n",
    "        df_cropped = df[(df['datetime'] >= latest_start_time) & (df['datetime'] <= earliest_end_time)]\n",
    "        print(f\"Logger {logger_id}: Cropped data from {len(df)} rows to {len(df_cropped)} rows.\")\n",
    "\n",
    "        # Determine upsampling factor\n",
    "        upsampling_factor = highest_sampling_frequency / logger_data_info[logger_id]['sampling_frequency']\n",
    "\n",
    "        if upsampling_factor > 1:\n",
    "            original_length = len(df_cropped)\n",
    "            df_cropped = df_cropped.set_index('datetime')\n",
    "\n",
    "            # Upsample each sensor column that is not \"extra\"\n",
    "            for column in df_cropped.columns:\n",
    "                sensor_info = None\n",
    "\n",
    "                # Search for the sensor type in `datareader.sensor_info`\n",
    "                for sensor_name, sensor_details in datareader.sensor_info.items():\n",
    "                    if column in sensor_details['channels']:\n",
    "                        sensor_info = sensor_details\n",
    "                        break\n",
    "\n",
    "                if not sensor_info:\n",
    "                    continue\n",
    "\n",
    "                sensor_type = sensor_info['metadata'][column]['sensor']\n",
    "                if sensor_type != 'extra':\n",
    "                    print(f\"Upsampling column {column} from logger {logger_id} by factor {upsampling_factor}.\")\n",
    "                    df_cropped[column] = upsample(df_cropped[column].values, int(upsampling_factor), original_length)\n",
    "\n",
    "            df_cropped = df_cropped.reset_index()\n",
    "\n",
    "        # Remove \"extra\" sensor columns and append to the concatenated DataFrame\n",
    "        columns_to_keep = []\n",
    "        for column in df_cropped.columns:\n",
    "            sensor_info = None\n",
    "\n",
    "#datareader.sensor_data['accelerometer']\n",
    "#datareader.files_info"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
