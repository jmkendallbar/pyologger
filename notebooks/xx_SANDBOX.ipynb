{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary pyologger utilities\n",
    "from pyologger.utils.config_manager import ConfigManager\n",
    "from pyologger.load_data.datareader import DataReader\n",
    "from pyologger.load_data.metadata import Metadata\n",
    "from pyologger.plot_data.plotter import *\n",
    "from pyologger.process_data.sampling import upsample\n",
    "from pyologger.calibrate_data.tag2animal import *\n",
    "from pyologger.calibrate_data.zoc import *\n",
    "from pyologger.plot_data.plotter import plot_depth_correction\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "color_mapping_path = os.path.join(root_dir, \"color_mappings.json\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "deployment_db = metadata.get_metadata(\"deployment_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "recording_db = metadata.get_metadata(\"recording_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "\n",
    "# Assuming you have the metadata and deployment_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder, deployment_id = datareader.check_deployment_folder(deployment_db, data_dir)\n",
    "config_manager = ConfigManager(deployment_folder=deployment_folder, deployment_id=deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to DiveDB\n",
    "\n",
    "Make sure your local DiveDB servers are running. To do so:\n",
    "- Navigate to the DiveDB directory\n",
    "- Run the command: `make up`\n",
    "- Wait until all services are running (Django, Postgres, Jupyter)\n",
    "- Make sure you have run the latest migrations: `make migrate`\n",
    "- Make sure you imported the latest logger and animal databases: `make importmetadata`\n",
    "\n",
    "Then, you're ready to upload data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow Django to run with async unsafe to run outside of Django server\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "metadata = {\n",
    "    \"animal\": datareader.animal_info[\"Animal ID\"],\n",
    "    \"deployment\": datareader.deployment_info[\"Deployment ID\"],\n",
    "    \"recording\": datareader.deployment_info[\"Recording ID\"].split(\", \")[1]\n",
    "}\n",
    "\n",
    "data_uploader.upload_netcdf('./data/2024-01-16_oror-002a/outputs/deployment_data.nc', metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "df = duckpond.get_delta_data(    \n",
    "    signal_names=[\"sensor_data_ecg\", \"sensor_data_light\", \"sensor_data_temperature\", \"sensor_data_depth\"],\n",
    "    animal_ids=\"mian-001\", # Make sure this matches the animal ID you uploaded\n",
    "    frequency=100,\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In progress: export to EDF\n",
    "\n",
    "Ideally, this would let you maintain the varied sampling frequencies from sensor_data and take the metadata from sensor_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "def export_concatenated_to_edf(concatenated_df, highest_sampling_frequency, latest_start_time, edf_filename_template):\n",
    "    \"\"\"\n",
    "    Exports the concatenated DataFrame to an EDF file.\n",
    "\n",
    "    Parameters:\n",
    "    - concatenated_df: The DataFrame containing concatenated data from all loggers.\n",
    "    - highest_sampling_frequency: The highest sampling frequency among the loggers.\n",
    "    - latest_start_time: The latest start time among the loggers.\n",
    "    - edf_filename_template: Template string for the EDF filename.\n",
    "                             The string should contain `{sensor}` to be replaced with 'ALL'.\n",
    "    \"\"\"\n",
    "    if concatenated_df is None or concatenated_df.empty:\n",
    "        print(\"No data available for export. Exiting.\")\n",
    "        return\n",
    "\n",
    "    ch_names = concatenated_df.columns.tolist()\n",
    "    sfreq = highest_sampling_frequency\n",
    "\n",
    "    # Check if there are any channels to process\n",
    "    if len(ch_names) == 0:\n",
    "        print(\"No valid channels found to export. Exiting.\")\n",
    "        return\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='misc')  # Adjust ch_types as necessary\n",
    "\n",
    "    # Convert datetime to (seconds, microseconds) tuple for the latest start time\n",
    "    meas_date = (int(latest_start_time.timestamp()), int((latest_start_time.timestamp() % 1) * 1e6))\n",
    "\n",
    "    # Create MNE RawArray\n",
    "    data = concatenated_df.values.T\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.set_meas_date(meas_date)\n",
    "\n",
    "    # Step 8: Define the EDF filename and save the EDF file\n",
    "    edf_filename = edf_filename_template.format(sensor='ALL')\n",
    "\n",
    "    print(f\"Saving EDF file as {edf_filename} with shape {data.shape}.\")\n",
    "\n",
    "    # Ensure that data is within the physical range EDF expects\n",
    "    raw.export(edf_filename, fmt='edf')\n",
    "\n",
    "    print(f\"EDF file saved as {edf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyologger.process_data.sampling import *\n",
    "import pandas as pd\n",
    "def concatenate_logger_data(datareader):\n",
    "    \"\"\"\n",
    "    Concatenates data from all loggers stored in `datareader.data`.\n",
    "\n",
    "    Parameters:\n",
    "    - datareader: The DataReader object containing logger data in `datareader.data`.\n",
    "\n",
    "    Returns:\n",
    "    - concatenated_df: A pandas DataFrame with the concatenated data from all loggers.\n",
    "    - highest_sampling_frequency: The highest sampling frequency found among the loggers.\n",
    "    - latest_start_time: The latest start time among the loggers.\n",
    "    \"\"\"\n",
    "    logger_data_info = {}\n",
    "\n",
    "    # Step 1: Extract start time, end time, and sampling frequency for each logger\n",
    "    for logger_id, df in datareader.data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"Logger {logger_id} does not contain a valid DataFrame. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if 'datetime' not in df.columns:\n",
    "            print(f\"Logger {logger_id} does not have a 'datetime' column. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        start_time = df['datetime'].iloc[0]\n",
    "        end_time = df['datetime'].iloc[-1]\n",
    "        sampling_frequency = round(1 / df['datetime'].diff().dt.total_seconds().mean())\n",
    "\n",
    "        logger_data_info[logger_id] = {\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'sampling_frequency': sampling_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"Logger {logger_id}: start_time={start_time}, end_time={end_time}, sampling_frequency={sampling_frequency} Hz\")\n",
    "\n",
    "    if not logger_data_info:\n",
    "        print(\"No valid logger data found. Exiting.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Step 2: Determine the latest start time, earliest end time, and highest sampling frequency\n",
    "    latest_start_time = max(info['start_time'] for info in logger_data_info.values())\n",
    "    earliest_end_time = min(info['end_time'] for info in logger_data_info.values())\n",
    "    highest_sampling_frequency = max(info['sampling_frequency'] for info in logger_data_info.values())\n",
    "\n",
    "    print(f\"Latest start time: {latest_start_time}\")\n",
    "    print(f\"Earliest end time: {earliest_end_time}\")\n",
    "    print(f\"Highest sampling frequency: {highest_sampling_frequency} Hz\")\n",
    "\n",
    "    # Step 3: Initialize an empty DataFrame for concatenation\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Step 4: Crop dataframes, upsample as necessary, and concatenate\n",
    "    for logger_id, df in datareader.data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "\n",
    "        # Crop dataframe\n",
    "        df_cropped = df[(df['datetime'] >= latest_start_time) & (df['datetime'] <= earliest_end_time)]\n",
    "        print(f\"Logger {logger_id}: Cropped data from {len(df)} rows to {len(df_cropped)} rows.\")\n",
    "\n",
    "        # Determine upsampling factor\n",
    "        upsampling_factor = highest_sampling_frequency / logger_data_info[logger_id]['sampling_frequency']\n",
    "\n",
    "        if upsampling_factor > 1:\n",
    "            original_length = len(df_cropped)\n",
    "            df_cropped = df_cropped.set_index('datetime')\n",
    "\n",
    "            # Upsample each sensor column that is not \"extra\"\n",
    "            for column in df_cropped.columns:\n",
    "                sensor_info = None\n",
    "\n",
    "                # Search for the sensor type in `datareader.sensor_info`\n",
    "                for sensor_name, sensor_details in datareader.sensor_info.items():\n",
    "                    if column in sensor_details['channels']:\n",
    "                        sensor_info = sensor_details\n",
    "                        break\n",
    "\n",
    "                if not sensor_info:\n",
    "                    continue\n",
    "\n",
    "                sensor_type = sensor_info['metadata'][column]['sensor']\n",
    "                if sensor_type != 'extra':\n",
    "                    print(f\"Upsampling column {column} from logger {logger_id} by factor {upsampling_factor}.\")\n",
    "                    df_cropped[column] = upsample(df_cropped[column].values, int(upsampling_factor), original_length)\n",
    "\n",
    "            df_cropped = df_cropped.reset_index()\n",
    "\n",
    "        # Remove \"extra\" sensor columns and append to the concatenated DataFrame\n",
    "        columns_to_keep = []\n",
    "        for column in df_cropped.columns:\n",
    "            sensor_info = None\n",
    "\n",
    "#datareader.sensor_data['accelerometer']\n",
    "#datareader.files_info"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
