{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Querying and Exporting\n",
    "\n",
    "This notebook demonstrates the process of querying data from Delta Lake and exporting it in various formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the servers:\n",
    "To launch the server, open the Docker Desktop app and open a terminal in the root of the [DiveDB repo](https://github.com/ecophysviz-lab/DiveDB). Then run the following command:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Django server, Postgres database, and Jupyter server using the environment variables defined in the `.env` file accross all containers. \n",
    "\n",
    "**Ensure you have the same env variables in Pylogger**\n",
    "\n",
    "#### Understanding expected file paths:\n",
    "DiveDB expects the following paths to be set in the `.env` file:\n",
    "- `CONTAINER_DATA_PATH`\n",
    "- `LOCAL_DATA_PATH`\n",
    "- `HOST_DELTA_LAKE_PATH`\n",
    "- `CONTAINER_DELTA_LAKE_PATH`\n",
    "- `HOST_FILE_STORAGE_PATH`\n",
    "- `CONTAINER_FILE_STORAGE_PATH`\n",
    "\n",
    "These paths are used to mount the Delta Lake and file storage to the containers. The \"LOCAL_\" and \"HOST_\" paths can be wherever makes sense for your local machine. The \"CONTAINER_\" paths are the paths that the containers expect. We recommend you keep the \"CONTAINER_\" paths as they are in the `.env.example` file.\n",
    "\n",
    "#### When is the server ready?\n",
    "There are 3 processes that need to be running for the server to be ready:\n",
    "1. The Django server (`web`)\n",
    "2. The Postgres database (`metadata_database`)\n",
    "3. The Jupyter server (`jupyter`)\n",
    "\n",
    "Jupyter is almost always the last to start up. You'll know it's ready when you see the following logs in the terminal:\n",
    "```bash\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying from Delta Lake\n",
    "We connect to our datastores using the `DuckPond` class. DuckPond is a wrapper around a DuckDB connection with access to both our Metadata Database and our measurements stored in Delta Lake. The ability to query both sources of data from a single connection is useful for quickly accessing data for analysis.\n",
    "\n",
    "There are two main ways to query data from Delta Lake:\n",
    "1. Using the DuckPond `get_delta_data` method\n",
    "2. Using the DuckPond connection to query directly\n",
    "\n",
    "### Using the DuckPond `get_delta_data` method\n",
    "DuckPond's `get_delta_data` method constructs a query based on the parameters you pass to it and returns a DuckDB DataFrame. It is useful for quickly accessing data for analysis. It takes the following optional parameters:\n",
    "- `signal_names`: A string or list of signal names to query.\n",
    "- `logger_ids`: A string or list of logger IDs to query.\n",
    "- `animal_ids`: A string or list of animal IDs to query.\n",
    "- `deployment_ids`: A string or list of deployment IDs to query.\n",
    "- `recording_ids`: A string or list of recording IDs to query.\n",
    "- `date_range`: A tuple of start and end dates to query.\n",
    "- `limit`: The maximum number of rows to return.\n",
    "\n",
    "The `get_delta_data` method returns a [DuckDB DuckDBPyConnection](https://duckdb.org/docs/api/python/reference/#duckdb.DuckDBPyConnection) which can be used to convert the data in many different formats including the following ([see documentation for a full list](https://duckdb.org/docs/api/python/conversion#result-conversion-duckdb-results-to-python))\n",
    "- NumPy Array (`.fetchnumpy()`)\n",
    "- Pandas DataFrame (`.df()`)\n",
    "- Arrows Table (`.arrow()`)\n",
    "- Polars DataFrame (`.pl()`)\n",
    "\n",
    "Until a conversion method is called, the data is not loaded into memory. This allows for large queries to be run without using too much memory.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "conn = duckpond.get_delta_data(    \n",
    "    logger_ids=\"NL-01\",\n",
    "    animal_ids=\"mian-001\",\n",
    "    signal_names=[\"sensor_data_temperature\"],\n",
    ")\n",
    "\n",
    "display(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the DuckPond connection to query directly\n",
    "More complex queries can be run directly on the DuckPond connection. This is useful for queries that may not be supported by the `get_delta_data` method which has those involving grouping or aggregations. \n",
    "\n",
    "DuckDB runs sql very similar in syntax to other SQL databases. A full breakdown of the syntax can be found [in the documenation](https://duckdb.org/docs/sql/introduction).\n",
    "\n",
    "The connection object can be found in the `duckpond.conn` attribute. To run queries, use the `sql` method which also returns a [DuckDB DuckDBPyConnection](https://duckdb.org/docs/api/python/reference/#duckdb.DuckDBPyConnection) which can be used to convert the data in many different formats including the following ([see documentation for a full list](https://duckdb.org/docs/api/python/conversion#result-conversion-duckdb-results-to-python))\n",
    "- NumPy Array (`.fetchnumpy()`)\n",
    "- Pandas DataFrame (`.df()`)\n",
    "- Arrows Table (`.arrow()`)\n",
    "- Polars DataFrame (`.pl()`)\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "df = duckpond.conn.sql(f\"\"\"\n",
    "SELECT logger, signal_name, avg(value) as mean_data\n",
    "FROM (\n",
    "    SELECT logger, signal_name, unnest(values) as value\n",
    "    FROM DeltaLake\n",
    "    WHERE signal_name = 'sensor_data_temperature'\n",
    ")\n",
    "GROUP BY logger, signal_name\n",
    "\"\"\").df()\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Queries\n",
    "Queries can be chained together to form a pipeline. This is useful for running complex queries that involve multiple steps.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "# Get the filtered data\n",
    "# filtered_data = duckpond.get_delta_data(    \n",
    "#     logger_ids=\"NL-01\",\n",
    "#     animal_ids=\"mian-001\",\n",
    "#     signal_names=[\"sensor_data_light\", \"sensor_data_temperature\"],\n",
    "# )\n",
    "\n",
    "# Perform the aggregation on the filtered data\n",
    "df = duckpond.conn.sql(\"\"\"\n",
    "SELECT install_path\n",
    "FROM duckdb_extensions();\n",
    "\n",
    "\n",
    "\"\"\").df()\n",
    "\n",
    "display(df[\"install_path\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Variables\n",
    "Sometimes we don't want to hardcode variables in our queries. We can use the `execute` method to pass variables to the query.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "signal_name = \"sensor_data_temperature\"\n",
    "df = duckpond.conn.execute(f\"\"\"\n",
    "SELECT logger, signal_name, avg(value) as mean_data\n",
    "FROM (\n",
    "    SELECT logger, signal_name, unnest(values) as value\n",
    "    FROM DeltaLake\n",
    "    WHERE signal_name = $1\n",
    ")\n",
    "GROUP BY logger, signal_name\n",
    "\"\"\", [signal_name]).df()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Metadata Database\n",
    "We can also query the Metadata Database directly. This is useful for querying data that is not stored in Delta Lake and joining it for queries on measurement data.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "# Show all tables we have access to\n",
    "print(duckpond.get_db_schema())\n",
    "\n",
    "df = duckpond.conn.sql(\"\"\"\n",
    "SELECT unnest(values) as value\n",
    "FROM DeltaLake \n",
    "JOIN Metadata.public.Animals ON DeltaLake.animal = Animals.id\n",
    "WHERE Animals.project_id = 'test12_Wednesday'\n",
    "AND signal_name = 'sensor_data_temperature'\n",
    "\"\"\").df()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data to EDF\n",
    "When it's easier to work with EDF files, we can export the data to an EDF file. This is useful for working with the data in other software packages.\n",
    "\n",
    "The `create_mne_edf` function takes a DuckDB connection and a file path and creates an EDF file. \n",
    "\n",
    "*Note: it currently requires a lot of memory. Can be improved.*\n",
    "*Note: it's lacking support for most info fields in the EDF file. Can be improved.*\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.utils.edf import create_mne_edf\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "conn = duckpond.get_delta_data(    \n",
    "    logger_ids=\"NL-02\",\n",
    "    animal_ids=\"mian-003\",\n",
    "    signal_names=[\"ECG_ICA2\", \"EEG_ICA5\"],\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "create_mne_edf(conn, \"test.edf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data to MNE Signal Array\n",
    "For working with the data in MNE, we can export the data to an MNE Signal Array. This is useful for manipulating the data in MNE.\n",
    "\n",
    "The `create_mne_array` function takes a DuckDB connection and returns an MNE RawArray.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.utils.edf import create_mne_array\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "conn = duckpond.get_delta_data(    \n",
    "    logger_ids=\"NL-02\",\n",
    "    animal_ids=\"mian-003\",\n",
    "    signal_names=\"ECG_ICA2\",\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "raw = create_mne_array(conn, resample=100, l_freq=1, h_freq=20)\n",
    "display(raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
