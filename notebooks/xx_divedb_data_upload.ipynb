{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Uploader\n",
    "\n",
    "This notebook demonstrates the process of uploading EDF files data to Delta Lake and OpenStack Swift for long-term storage. \n",
    "\n",
    "It also includes the setup and execution of the data upload process, as well as querying the uploaded data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the servers:\n",
    "To launch the server, open the Docker Desktop app and open a terminal in the root of the [DiveDB repo](https://github.com/ecophysviz-lab/DiveDB). Then run the following command:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Django server, Postgres database, and Jupyter server using the environment variables defined in the `.env` file accross all containers. \n",
    "\n",
    "**Ensure you have the same env variables in Pylogger**\n",
    "\n",
    "#### Understanding expected file paths:\n",
    "DiveDB expects the following paths to be set in the `.env` file:\n",
    "- `CONTAINER_DATA_PATH`\n",
    "- `LOCAL_DATA_PATH`\n",
    "- `HOST_DELTA_LAKE_PATH`\n",
    "- `CONTAINER_DELTA_LAKE_PATH`\n",
    "- `HOST_FILE_STORAGE_PATH`\n",
    "- `CONTAINER_FILE_STORAGE_PATH`\n",
    "\n",
    "These paths are used to mount the Delta Lake and file storage to the containers. The \"LOCAL_\" and \"HOST_\" paths can be wherever makes sense for your local machine. The \"CONTAINER_\" paths are the paths that the containers expect. We recommend you keep the \"CONTAINER_\" paths as they are in the `.env.example` file.\n",
    "\n",
    "#### When is the server ready?\n",
    "There are 3 processes that need to be running for the server to be ready:\n",
    "1. The Django server (`web`)\n",
    "2. The Postgres database (`metadata_database`)\n",
    "3. The Jupyter server (`jupyter`)\n",
    "\n",
    "Jupyter is almost always the last to start up. You'll know it's ready when you see the following logs in the terminal:\n",
    "```bash\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to upload data:\n",
    "There are two aspects to any data upload:\n",
    "1. A table containing measurements and time\n",
    "    - This can be an EDF, CSV, or a data\n",
    "2. The metadata for the measurements\n",
    "    - This describes the context of the measurements using the following fields:\n",
    "        - animal\n",
    "        - deployment\n",
    "        - logger\n",
    "        - recording\n",
    "\n",
    "There are several ways to define your metadata. \n",
    "\n",
    "#### Option 1: Supplied Metadata CSV File\n",
    "If you have a metadata file, you can use the `metadata_map` dictionary to map the columns in the CSV file to the corresponding mode. You can then pass the metadata file to the `upload_edf` function which will interactively extract the metadata for your measurements using the data in the metadata file and in the Metadata Database.\n",
    "\n",
    "#### Option 2: Interactive Selection (in the works)\n",
    "If you provide no metadata, you can use the `upload_edf` function to interactively select the metadata for your measurements using the data in the Metadata Database.\n",
    "\n",
    "#### Option 3: Supplied Metadata Dictionary\n",
    "If you know the metadata for your measurements, you can pass a dictionary to the `upload_edf` function. The dictionary should represent metadata existing in the Metadata database and contain the following fields:\n",
    "- animal: The animal ID\n",
    "- deployment: The deployment name\n",
    "- logger: The logger ID\n",
    "- recording: The recording name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading EDFs\n",
    "The `edf_file_paths` list contains the paths to the EDF files that we want to upload. It can point to files on your local machine or on a remote server.\n",
    "In this example, the files are located in the ../data/files/ directory and are named test12_Wednesday_05_DAY1_PROCESSED.edf and test12_Wednesday_05_DAY2_PROCESSED.edf.\n",
    "\n",
    "The `metadata_file_path` variable holds the path to the CSV file containing metadata for the EDF files (named Sleep Study Metadata.csv). \n",
    "\n",
    "The `metadata_map` dictionary is used to map the columns in the CSV metadata file to the corresponding mode. The keys in the dictionary represent the fields in the database, and the values represent the column names in the CSV file. For example:\n",
    "- \"animal\" maps to the \"Nickname\" column in the CSV file.\n",
    "- \"deployment\" maps to the \"Deployment\" column in the CSV file.\n",
    "- \"logger\" maps to the \"Logger Used\" column in the CSV file.\n",
    "- \"recording\" maps to the \"Recording ID\" column in the CSV file.\n",
    "\n",
    "The upload_edf function will perform the following: \n",
    "- use the metadata map to extract the metadata for your measurements using the data in the metadata file and in the Metadata Database\n",
    "- upload copies of the EDF files to OpenStack Swift\n",
    "- upload the measurements to Delta Lake (by default, 10M measurements per batch)\n",
    "\n",
    "The process takes between 5-10 minutes to complete per gigabyte. (*note: we can speed this up by parellizing the upload process*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Supplied Metadata CSV File\n",
    "\n",
    "import os\n",
    "\n",
    "# Allow Django to run with async unsafe to run outside of Django server\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "\n",
    "edf_file_paths = [\n",
    "    \"../data/files/test12_Wednesday_05_DAY1_PROCESSED.edf\",\n",
    "    \"../data/files/test12_Wednesday_05_DAY2_PROCESSED.edf\"\n",
    "]\n",
    "\n",
    "metadata_file_path = \"../data/files/Sleep Study Metadata.csv\"\n",
    "\n",
    "metadata_map = {\n",
    "    \"animal\": \"Nickname\",\n",
    "    \"deployment\": \"Deployment\",\n",
    "    \"logger\": \"Logger Used\",\n",
    "    \"recording\": \"Recording ID\"\n",
    "}\n",
    "\n",
    "data_uploader.upload_edf(edf_file_paths, metadata_file_path, metadata_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Supplied Metadata\n",
    "\n",
    "import os\n",
    "\n",
    "# Allow Django to run with async unsafe to run outside of Django server\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "\n",
    "edf_file_paths = [\n",
    "    \"../data/files/test12_Wednesday_05_DAY1_PROCESSED.edf\",\n",
    "    \"../data/files/test12_Wednesday_05_DAY2_PROCESSED.edf\"\n",
    "]\n",
    "metadata = {\n",
    "    \"animal\": \"test12_Wednesday\",\n",
    "    \"deployment\": \"2024-06-06_boat-001a\",\n",
    "    \"logger\": \"NL-02\",\n",
    "    \"recording\": \"2019-10-25_mian-001a_NL-02_001\"\n",
    "}\n",
    "\n",
    "data_uploader.upload_edf(edf_file_paths, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading NetCDF files\n",
    "The `netcdf_file_path` list contains the paths to the NetCDF files that we want to upload. It can point to files on your local machine or on a remote server.\n",
    "In this example, the file is located in the ../data/files/ directory and is named deployment_data.nc.\n",
    "\n",
    "The upload_netcdf function will perform the following: \n",
    "- use the provided metadata dictionary to extract the metadata for your measurements\n",
    "- upload copies of the NetCDF files to OpenStack Swift\n",
    "- upload the measurements to Delta Lake\n",
    "\n",
    "The process takes between 30 secs to 1 min to complete per gigabyte â€” about 2/3rds of the time is used to upload the files to OpenStack Swift. (*note: we can speed this up by parellizing the upload process*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Allow Django to run with async unsafe to run outside of Django server\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "metadata = {\n",
    "    \"deployment\": \"2024-01-16_oror-002a\",\n",
    "}\n",
    "\n",
    "data_uploader.upload_netcdf(\"../data/2024-01-16_oror-002a/outputs/deployment_data.nc\", metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
