{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline for DiveDB\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from notion_client import Client\n",
    "from dotenv import load_dotenv\n",
    "from datareader import DataReader\n",
    "from metadata import Metadata\n",
    "from datastreams import DataStreams\n",
    "from loggerdata import LoggerData\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import nbformat\n",
    "print(nbformat.__version__)\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "outputs_dir = os.path.join(root_dir, \"outputs\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query metadata\n",
    "Use Notion and [metadata entry form](https://forms.fillout.com/t/8UNuTLMaRfus) to start a recording and to generate identifiers for the Recording and Deployment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Processing Deployment Data:\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Asks the user for input to select a deployment folder to kick off the data reading process. In your folder name, you can have any suffix after Deployment ID. It will check and stop if there are two that fit.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Starts the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieve necessary data from the metadata database, including logger information.\n",
    "   - **Function Used:** `metadata.fetch_databases()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Group files by logger ID for processing.\n",
    "   - **Function Used:** `read_files()` (This is the main function)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verify if the outputs folder already contains processed files for each logger. Skip reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process UBE Files**:\n",
    "   - **Description:** For each UFI logger with UBE files, process and save the data.\n",
    "   - **Function Used:** `process_ube_file()`\n",
    "\n",
    "7. **Process CSV Files**:\n",
    "   - **Description:** For each logger with multiple CSV files, concatenate them, and save the combined data.\n",
    "   - **Function Used:** `concatenate_and_save_csvs()`\n",
    "\n",
    "8. **Final Outputs**:\n",
    "   - **Description:** Ensure all processed data is saved in the outputs folder with appropriate filenames.\n",
    "   - **Functions Used:** `save_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your deployment ID index and remember it for the next cell, where you have to enter it.\n",
    "dep_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the metadata and dep_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(dep_db, data_dir)\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate consecutive files\n",
    "\n",
    "Checks to see if there are multiple files from a single logger to concatenate in the outputs folder. If needed, it concatenates them and outputs a concatenated file. For any concatenated files, the intermediate files that were previously in `outputs` will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize data into DataStream object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(outputs_dir, 'data_reader.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataStreams object\n",
    "data = DataStreams()\n",
    "\n",
    "# Define the folder path to the \"outputs\" directory\n",
    "folder_path = os.path.join(deployment_folder, \"outputs\") \n",
    "\n",
    "# Load data from the \"outputs\" folder\n",
    "load_data_from_outputs(folder_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign a time zone to the deployment\n",
    "This time zone will apply to the deployment and be used to align timestamps. Check to make sure the timezone is recognized and present in our mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provided_timezone = selected_deployment['Time Zone']\n",
    "print(pytz.all_timezones)\n",
    "\n",
    "if provided_timezone not in pytz.all_timezones:\n",
    "    raise ValueError(f\"Unrecognized time zone: {provided_timezone}. Please provide a valid time zone from pytz.all_timezones.\")\n",
    "else:\n",
    "    print(f\"Time zone recognized:\\n{provided_timezone}\")\n",
    "\n",
    "# Assuming you have a DataFrame final_df with Date (local) and Time (local) columns\n",
    "final_df['datetime'] = pd.to_datetime(final_df[\" Date (local)\"] + ' ' + final_df[\" Time (local)\"], format='%d.%m.%Y %H:%M:%S.%f')\n",
    "\n",
    "# Localize to the correct timezone\n",
    "final_df['datetime'] = final_df['datetime'].dt.tz_localize(pytz.timezone(provided_timezone))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for gaps in timeseries data\n",
    "Especially with concatenated data, we must check that there are no gaps in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time differences and cumulative sum of differences\n",
    "sec_diff = final_df['datetime'].diff().dt.total_seconds()\n",
    "final_df['cum_diff'] = np.cumsum(sec_diff)\n",
    "\n",
    "# Check for inconsistencies (time jumps)\n",
    "mean_diff = sec_diff.mean()\n",
    "time_jumps = sec_diff[sec_diff > mean_diff * 2]  # Define a threshold for time jumps\n",
    "\n",
    "print(CO_df['datetime'][1]-CO_df['datetime'][0])\n",
    "CO_fs = 1/(CO_df['datetime'][1]-CO_df['datetime'][0]).total_seconds()\n",
    "CO_max_timediff = np.max(np.diff(CO_df['datetime']))\n",
    "print(f\"CATS Sampling frequency: {CO_fs} Hz with a maximum time difference of {CO_max_timediff}\")\n",
    "\n",
    "# Report any inconsistencies\n",
    "if not time_jumps.empty:\n",
    "    print(f\"Time jumps detected:\\n{time_jumps}\")\n",
    "else:\n",
    "    print(\"No significant time jumps detected.\")\n",
    "    print(f\"Sampling frequency: {1 / mean_diff} Hz\")\n",
    "\n",
    "# Plot cumulative differences\n",
    "plt.plot(final_df['datetime'], final_df['cum_diff'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Difference (seconds)')\n",
    "plt.title('Cumulative Difference over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prep\n",
    "CO_df = final_df \n",
    "\n",
    "\n",
    "\n",
    "# Load the data_reader object from the pickle file\n",
    "with open('outputs/data_reader.pkl', 'rb') as file:\n",
    "    data_reader = pickle.load(file)\n",
    "\n",
    "# Get the ECG and timestamp data\n",
    "ecg_df = data_reader.data_raw['2024-06-17_oror-002-001a_UF-04_001']\n",
    "ecg_df['datetime'] = pd.to_datetime(ecg_df['timestamp'])\n",
    "ecg_df['datetime'] = ecg_df['datetime'].dt.tz_localize(pytz.timezone('America/Los_Angeles'))\n",
    "print(ecg_df['datetime'][0])\n",
    "print(ecg_df)\n",
    "\n",
    "print(ecg_df['datetime'][1]-ecg_df['datetime'][0])\n",
    "ecg_fs = 1/(ecg_df['datetime'][1]-ecg_df['datetime'][0]).total_seconds()\n",
    "ecg_max_timediff = np.max(np.diff(ecg_df['datetime']))\n",
    "print(f\"ECG Sampling frequency: {ecg_fs} Hz with a maximum time difference of {ecg_max_timediff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sampling_rate = 10\n",
    "ecg_conversion = int(ecg_fs / new_sampling_rate)\n",
    "CATS_conversion = int(CO_fs / new_sampling_rate)\n",
    "\n",
    "ecg_df10 = ecg_df.iloc[::ecg_conversion, :] # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "CO_df10 = CO_df.iloc[::CATS_conversion, :] # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 10))\n",
    "\n",
    "axs[0].plot(CO_df10['datetime'], CO_df10['Accelerometer X [m/s²]'])\n",
    "axs[0].set_ylabel('Accelerometer X [m/s²]')\n",
    "\n",
    "axs[1].plot(CO_df10['datetime'], CO_df10['Accelerometer Y [m/s²]'])\n",
    "axs[1].set_ylabel('Accelerometer Y [m/s²]')\n",
    "\n",
    "axs[2].plot(CO_df10['datetime'], CO_df10['Accelerometer Z [m/s²]'])\n",
    "axs[2].set_ylabel('Accelerometer Z [m/s²]')\n",
    "\n",
    "axs[3].plot(CO_df10['datetime'], CO_df10['Depth (100bar) [m]'])\n",
    "axs[3].set_ylabel('Depth (100bar) [m]')\n",
    "\n",
    "axs[4].plot(ecg_df10['datetime'], ecg_df10['ecg'])\n",
    "axs[4].set_ylabel('ECG [mV]')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "new_CATS_sampling_rate = 10\n",
    "new_ecg_sampling_rate = 50\n",
    "ecg_conversion = int(ecg_fs / new_ecg_sampling_rate)\n",
    "CATS_conversion = int(CO_fs / new_CATS_sampling_rate)\n",
    "\n",
    "ecg_df50 = ecg_df.iloc[::ecg_conversion, :]  # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "CO_df10 = CO_df.iloc[::CATS_conversion, :]  # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "\n",
    "# Add ECG plot\n",
    "fig.add_trace(go.Scatter(x=ecg_df10['datetime'], y=ecg_df10['ecg'], mode='lines', name='ECG [mV]', line=dict(color='orange')), row=1, col=1)\n",
    "\n",
    "# Add Depth plot\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Depth (100bar) [m]'], mode='lines', name='Depth [m]', line=dict(color='purple')), row=2, col=1)\n",
    "fig.update_yaxes(autorange=\"reversed\", row=2, col=1)\n",
    "\n",
    "# Add Accelerometer plots\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer X [m/s²]'], mode='lines', name='Accel X [m/s²]', line=dict(color='blue')), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer Y [m/s²]'], mode='lines', name='Accel Y [m/s²]', line=dict(color='green')), row=4, col=1)\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer Z [m/s²]'], mode='lines', name='Accel Z [m/s²]', line=dict(color='red')), row=5, col=1)\n",
    "\n",
    "# Add Gyroscope Y plot\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Gyroscope X [mrad/s]'], mode='lines', name='Gyr X [mrad/s]', line=dict(color='pink')), row=6, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Subsampled Data Plots\", showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Datetime\", row=6, col=1)\n",
    "\n",
    "# Update y-axes labels\n",
    "fig.update_yaxes(title_text=\"ECG [mV]\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Depth [m]\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel X [m/s²]\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel Y [m/s²]\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel Z [m/s²]\", row=5, col=1)\n",
    "fig.update_yaxes(title_text=\"Gyr X [mrad/s]\", row=6, col=1)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases()\n",
    "\n",
    "# Get the logger database\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "\n",
    "# Determine unique LoggerIDs from the logger metadata dataframe\n",
    "logger_ids = set(logger_db['LoggerID'])\n",
    "print(f\"Unique Logger IDs: {logger_ids}\")\n",
    "\n",
    "# Breakdown of loggers by type\n",
    "logger_breakdown = logger_db.groupby(['Manufacturer', 'Type']).size().reset_index(name='Count')\n",
    "print(\"Logger Breakdown by Manufacturer and Type:\")\n",
    "print(logger_breakdown)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
