{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline for DiveDB\n",
    "Uses classes `info` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from notion_client import Client\n",
    "from dotenv import load_dotenv\n",
    "from datareader import DataReader\n",
    "from metadata import Metadata\n",
    "from loggerdata import LoggerData\n",
    "#from plotter import plot_tag_data\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import nbformat\n",
    "print(nbformat.__version__)\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "# os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/pyologger\")\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query metadata\n",
    "Use Notion and [info entry form](https://forms.fillout.com/t/8UNuTLMaRfus) to start a recording and to generate identifiers for the Recording and Deployment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the info class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases(verbose=False)\n",
    "\n",
    "# Save databases\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Processing Deployment Data:\n",
    "\n",
    "1. **Select Deployment Folder**:\n",
    "   - **Description:** Asks the user for input to select a deployment folder to kick off the data reading process. In your folder name, you can have any suffix after Deployment ID. It will check and stop if there are two that fit.\n",
    "   - **Function Used:** `check_deployment_folder()`\n",
    "\n",
    "2. **Initialize Deployment Folder**:\n",
    "   - **Description:** Starts the main `read_files` process with the selected deployment folder.\n",
    "   - **Function Used:** `read_files()`\n",
    "\n",
    "3. **Fetch Metadata**:\n",
    "   - **Description:** Retrieve necessary data from the metadata database, including logger information.\n",
    "   - **Function Used:** `metadata.fetch_databases()`\n",
    "\n",
    "4. **Organize Files by Logger ID**:\n",
    "   - **Description:** Group files by logger ID for processing.\n",
    "   - **Function Used:** `read_files()` (This is the main function)\n",
    "\n",
    "5. **Check for Existing Processed Files**:\n",
    "   - **Description:** Verify if the outputs folder already contains processed files for each logger. Skip reprocessing if all necessary files are present.\n",
    "   - **Function Used:** `check_outputs_folder()`\n",
    "\n",
    "6. **Process UBE Files**:\n",
    "   - **Description:** For each UFI logger with UBE files, process and save the data.\n",
    "   - **Function Used:** `process_ube_file()`\n",
    "\n",
    "7. **Process CSV Files**:\n",
    "   - **Description:** For each logger with multiple CSV files, concatenate them, and save the combined data.\n",
    "   - **Function Used:** `concatenate_and_save_csvs()`\n",
    "\n",
    "8. **Final Outputs**:\n",
    "   - **Description:** Ensure all processed data is saved in the outputs folder with appropriate filenames.\n",
    "   - **Functions Used:** `save_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your deployment ID index and remember it for the next cell, where you have to enter it.\n",
    "dep_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the metadata and dep_db loaded:\n",
    "datareader = DataReader()\n",
    "deployment_folder = datareader.check_deployment_folder(dep_db, data_dir)\n",
    "\n",
    "if deployment_folder:\n",
    "    datareader.read_files(metadata, save_csv=True, save_parq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally look at first notes that have been read in\n",
    "#datareader.selected_deployment['Time Zone']\n",
    "#datareader.info['UF-01']\n",
    "datareader.notes_df[0:5]\n",
    "#datareader.data['CC-96']\n",
    "#datareader.data['UF-01']\n",
    "#datareader.metadata['channelnames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datareader.data['CC-96']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test = pd.read_csv(os.path.join(deployment_folder, \"2024-01-16_oror-002a_CC-96_001.csv\"))\n",
    "datareader.selected_deployment['Time Zone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the pickle file output\n",
    "\n",
    "Load in the generated pickle file to inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data_reader object from the pickle file\n",
    "pkl_path = os.path.join(deployment_folder, 'outputs', 'data.pkl')\n",
    "\n",
    "with open(pkl_path, 'rb') as file:\n",
    "    data_pkl = pickle.load(file)\n",
    "\n",
    "for logger_id, info in data_pkl.info.items():\n",
    "    sampling_frequency = info.get('datetime_metadata', {}).get('fs', None)\n",
    "    if sampling_frequency is not None:\n",
    "        # Format the sampling frequency to 5 significant digits\n",
    "        print(f\"Sampling frequency for {logger_id}: {sampling_frequency} Hz\")\n",
    "    else:\n",
    "        print(f\"No sampling frequency available for {logger_id}\")\n",
    "\n",
    "data_pkl.info['CC-96']['datetime_metadata']['fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.notes_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.data['UF-01']  #data['UF-01'][0:5] # browse column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.data['CC-96'][0:5] # browse column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data for plots\n",
    "Downsample high-resolution data and filter notes down to notes of interest to include in plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming you have loaded your data into data_pkl\n",
    "imu_df = data_pkl.data['CC-96']\n",
    "ecg_df = data_pkl.data['UF-01']\n",
    "\n",
    "# Calculate sampling frequencies\n",
    "CO_fs = 1 / imu_df['datetime'].diff().dt.total_seconds().mean()\n",
    "ecg_fs = 1 / ecg_df['datetime'].diff().dt.total_seconds().mean()\n",
    "\n",
    "# Define new desired sampling rates\n",
    "new_CATS_sampling_rate = 10  # Hz\n",
    "new_ecg_sampling_rate = 50  # Hz\n",
    "\n",
    "# Calculate the downsampling conversion factors\n",
    "CATS_conversion = int(CO_fs / new_CATS_sampling_rate)\n",
    "ecg_conversion = int(ecg_fs / new_ecg_sampling_rate)\n",
    "\n",
    "# Subsample the dataframes\n",
    "ecg_df50 = ecg_df.iloc[::ecg_conversion, :]\n",
    "imu_df10 = imu_df.iloc[::CATS_conversion, :]\n",
    "\n",
    "# Filter notes for those with key == 'heartbeat_manual_ok'\n",
    "filtered_notes = data_pkl.notes_df[data_pkl.notes_df['key'] == 'heartbeat_manual_ok']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.info['CC-96']['channelinfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Color mapping dictionary with pastel, colorblind-friendly colors\n",
    "color_mapping = {\n",
    "    'ECG': '#FFCCCC',              # Light Red with alpha in rgba\n",
    "    'Depth': '#00008B',            # Dark Blue\n",
    "    'Accelerometer X [m/s²]': '#87CEFA',          # Light Blue\n",
    "    'Accelerometer Y [m/s²]': '#98FB98',          # Pale Green\n",
    "    'Accelerometer Z [m/s²]': '#FF6347',          # Light Coral\n",
    "    'Gyro X': '#9370DB',           # Medium Purple\n",
    "    'Gyro Y': '#BA55D3',           # Medium Orchid\n",
    "    'Gyro Z': '#8A2BE2',           # Blue Violet\n",
    "    'Mag X': '#FFD700',            # Gold\n",
    "    'Mag Y': '#FFA500',            # Orange\n",
    "    'Mag Z': '#FF8C00',            # Dark Orange\n",
    "    'Filtered Heartbeats': '#808080',  # Gray for dotted lines\n",
    "}\n",
    "\n",
    "def plot_tag_data(data_pkl, imu_channels, ephys_channels=None, imu_logger=None, ephys_logger=None, imu_sampling_rate=10, ephys_sampling_rate=50, draw=True):\n",
    "    if not imu_logger and not ephys_logger:\n",
    "        raise ValueError(\"At least one logger (imu_logger or ephys_logger) must be specified.\")\n",
    "\n",
    "    # Ensure the order of channels: ECG, Depth, Accel, Gyro, Mag\n",
    "    ordered_channels = []\n",
    "    if ephys_channels and 'ecg' in [ch.lower() for ch in ephys_channels]:\n",
    "        ordered_channels.append(('ECG', 'ecg'))\n",
    "    if 'depth' in [ch.lower() for ch in imu_channels]:\n",
    "        ordered_channels.append(('Depth', 'depth'))\n",
    "    if any(ch.lower() in ['accx', 'accy', 'accz'] for ch in imu_channels):\n",
    "        ordered_channels.append(('Accel', ['accX', 'accY', 'accZ']))\n",
    "    if any(ch.lower() in ['gyrx', 'gyry', 'gyrz'] for ch in imu_channels):\n",
    "        ordered_channels.append(('Gyro', ['gyrX', 'gyrY', 'gyrZ']))\n",
    "    if any(ch.lower() in ['magx', 'magy', 'magz'] for ch in imu_channels):\n",
    "        ordered_channels.append(('Mag', ['magX', 'magY', 'magZ']))\n",
    "\n",
    "    # Calculate the number of rows needed\n",
    "    num_rows = len(ordered_channels)\n",
    "\n",
    "    fig = make_subplots(rows=num_rows, cols=1, shared_xaxes=True, vertical_spacing=0.03)\n",
    "    \n",
    "    def downsample(df, original_fs, target_fs):\n",
    "        if target_fs >= original_fs:\n",
    "            return df\n",
    "        conversion_factor = int(original_fs / target_fs)\n",
    "        return df.iloc[::conversion_factor, :]\n",
    "\n",
    "    if imu_logger:\n",
    "        imu_df = data_pkl.data[imu_logger]\n",
    "        imu_fs = 1 / imu_df['datetime'].diff().dt.total_seconds().mean()\n",
    "        imu_df_downsampled = downsample(imu_df, imu_fs, imu_sampling_rate)\n",
    "        imu_info = data_pkl.info[imu_logger]['channelinfo']\n",
    "    \n",
    "    if ephys_logger:\n",
    "        ephys_df = data_pkl.data[ephys_logger]\n",
    "        ephys_fs = 1 / ephys_df['datetime'].diff().dt.total_seconds().mean()\n",
    "        ephys_df_downsampled = downsample(ephys_df, ephys_fs, ephys_sampling_rate)\n",
    "        ephys_info = data_pkl.info[ephys_logger]['channelinfo']\n",
    "\n",
    "    row_counter = 1\n",
    "    \n",
    "    for channel_type, channels in ordered_channels:\n",
    "        if channel_type == 'ECG' and ephys_channels and 'ecg' in [ch.lower() for ch in ephys_channels]:\n",
    "            # Plot ECG\n",
    "            channel = 'ecg'\n",
    "            df = ephys_df_downsampled\n",
    "            info = ephys_info\n",
    "            original_name = info[channel]['original_name']\n",
    "            unit = info[channel]['unit']\n",
    "\n",
    "            y_data = df[channel]\n",
    "            x_data = df['datetime']\n",
    "\n",
    "            y_label = f\"{original_name} [{unit}]\"\n",
    "            color = color_mapping.get(original_name, color_mapping['ECG'])\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_data,\n",
    "                y=y_data,\n",
    "                mode='lines',\n",
    "                name=y_label,\n",
    "                line=dict(color=color)\n",
    "            ), row=row_counter, col=1)\n",
    "\n",
    "            # Add vertical lines for heartbeats\n",
    "            filtered_notes = data_pkl.notes_df[data_pkl.notes_df['key'] == 'heartbeat_manual_ok']\n",
    "            if not filtered_notes.empty:\n",
    "                for dt in filtered_notes['datetime']:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[dt, dt],\n",
    "                        y=[y_data.min(), y_data.max()],\n",
    "                        mode='lines',\n",
    "                        line=dict(color=color_mapping['Filtered Heartbeats'], width=1, dash='dot'),\n",
    "                        showlegend=False\n",
    "                    ), row=row_counter, col=1)\n",
    "\n",
    "            fig.update_yaxes(title_text=y_label, row=row_counter, col=1)\n",
    "            row_counter += 1\n",
    "\n",
    "        elif channel_type == 'Depth' and 'depth' in [ch.lower() for ch in imu_channels]:\n",
    "            # Plot Depth\n",
    "            channel = 'depth'\n",
    "            df = imu_df_downsampled\n",
    "            info = imu_info\n",
    "            original_name = info[channel]['original_name']\n",
    "            unit = info[channel]['unit']\n",
    "\n",
    "            y_data = df[channel]\n",
    "            x_data = df['datetime']\n",
    "\n",
    "            y_label = f\"{original_name} [{unit}]\"\n",
    "            color = color_mapping.get(original_name, color_mapping['Depth'])\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_data,\n",
    "                y=y_data,\n",
    "                mode='lines',\n",
    "                name=y_label,\n",
    "                line=dict(color=color)\n",
    "            ), row=row_counter, col=1)\n",
    "\n",
    "            fig.update_yaxes(title_text=y_label, autorange=\"reversed\", row=row_counter, col=1)\n",
    "            row_counter += 1\n",
    "\n",
    "        elif channel_type in ['Accel', 'Gyro', 'Mag']:\n",
    "            # Plot Accel, Gyro, or Mag channels together\n",
    "            for sub_channel in channels:\n",
    "                if sub_channel in imu_df_downsampled.columns:\n",
    "                    df = imu_df_downsampled\n",
    "                    info = imu_info\n",
    "                    original_name = info[sub_channel]['original_name']\n",
    "                    unit = info[sub_channel]['unit']\n",
    "\n",
    "                    y_data = df[sub_channel]\n",
    "                    x_data = df['datetime']\n",
    "\n",
    "                    y_label = f\"{original_name} [{unit}]\"\n",
    "                    color = color_mapping.get(original_name, '#000000')\n",
    "\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=x_data,\n",
    "                        y=y_data,\n",
    "                        mode='lines',\n",
    "                        name=y_label,\n",
    "                        line=dict(color=color)\n",
    "                    ), row=row_counter, col=1)\n",
    "\n",
    "            fig.update_yaxes(title_text=f\"{channel_type} [{unit}]\", row=row_counter, col=1)\n",
    "            row_counter += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=200 * num_rows,\n",
    "        width=1200,\n",
    "        title_text=f\"{data_pkl.selected_deployment['Deployment Name']}\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Datetime\", row=row_counter-1, col=1)\n",
    "\n",
    "    if draw:\n",
    "        fig.show()\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "# Example usage:\n",
    "# Specify channels and loggers\n",
    "imu_channels_to_plot = ['depth', 'accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ', 'magX', 'magY', 'magZ']\n",
    "ephys_channels_to_plot = ['ecg']\n",
    "imu_logger_to_use = 'CC-96'\n",
    "ephys_logger_to_use = 'UF-01'\n",
    "\n",
    "plot_tag_data(data_pkl, imu_channels_to_plot, ephys_channels=ephys_channels_to_plot, imu_logger=imu_logger_to_use, ephys_logger=ephys_logger_to_use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl.info['channelnames']['CC-96']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming your new data columns are as follows:\n",
    "# ECG signal column: 'ecg'\n",
    "# Depth column: 'depth1'\n",
    "# Accelerometer columns: 'accX', 'accY', 'accZ'\n",
    "# Gyroscope column: 'gyrY'\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=5, cols=1, shared_xaxes=True, vertical_spacing=0.03)\n",
    "\n",
    "# Add Heart Rate (bpm) plot at the top\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=filtered_notes['datetime'], \n",
    "    y=filtered_notes['value'], \n",
    "    mode='markers', \n",
    "    marker=dict(color='gray', size=8, symbol='circle-open'),\n",
    "    name='Heart rate (bpm)'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Add ECG plot with light red color and alpha 0.2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ecg_df50['datetime'], \n",
    "    y=ecg_df50['ecg'], \n",
    "    mode='lines', \n",
    "    name='ECG [mV]', \n",
    "    line=dict(color='rgba(255, 0, 0, 0.2)')\n",
    "), row=2, col=1)\n",
    "\n",
    "# Add vertical dotted lines for detected heartbeats\n",
    "for dt in filtered_notes['datetime']:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[dt, dt], \n",
    "        y=[ecg_df50['ecg'].min(), ecg_df50['ecg'].max()], \n",
    "        mode='lines', \n",
    "        line=dict(color='gray', width=1, dash='dot'),\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "\n",
    "# Add Depth plot with dark blue color\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['depth1'], \n",
    "    mode='lines', \n",
    "    name='Depth [m]', \n",
    "    line=dict(color='darkblue')\n",
    "), row=3, col=1)\n",
    "fig.update_yaxes(autorange=\"reversed\", row=3, col=1)\n",
    "\n",
    "# Add Accelerometer plots on the same y-axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['accX'], \n",
    "    mode='lines', \n",
    "    name='Accel X [m/s²]', \n",
    "    line=dict(color='blue')\n",
    "), row=4, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['accY'], \n",
    "    mode='lines', \n",
    "    name='Accel Y [m/s²]', \n",
    "    line=dict(color='green')\n",
    "), row=4, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['accZ'], \n",
    "    mode='lines', \n",
    "    name='Accel Z [m/s²]', \n",
    "    line=dict(color='red')\n",
    "), row=4, col=1)\n",
    "\n",
    "# Add Gyroscope Y plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CO_df10['datetime'], \n",
    "    y=CO_df10['gyrY'], \n",
    "    mode='lines', \n",
    "    name='Gyr Y [mrad/s]', \n",
    "    line=dict(color='purple')\n",
    "), row=5, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    width=1200, \n",
    "    title_text=f\"{data_pkl.selected_deployment['Deployment Name']}\", \n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Datetime\", row=5, col=1)\n",
    "\n",
    "# Update y-axes labels\n",
    "fig.update_yaxes(title_text=\"Heart rate (bpm)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"ECG [mV]\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Depth [m]\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accelerometer [m/s²]\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Gyr Y [mrad/s]\", row=5, col=1)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the interactive plot as an HTML file\n",
    "fig.write_html(os.path.join(deployment_folder, \"outputs\", f\"{data_pkl.selected_deployment['Deployment Name']}.html\")) \n",
    "data_pkl.selected_deployment['Deployment Name']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
