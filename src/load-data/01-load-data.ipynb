{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline for DiveDB\n",
    "Uses classes `Metadata` and `DataReader` to facilitate data intake, processing, and alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set working directory (adjust to fit your preferences)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from notion_client import Client\n",
    "from dotenv import load_dotenv\n",
    "from datareader import DataReader\n",
    "from metadata import Metadata\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import nbformat\n",
    "print(nbformat.__version__)\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "os.chdir(\"/Users/fbar/Documents/GitHub/pyologger\")\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query metadata\n",
    "Use Notion and [metadata entry form](https://forms.fillout.com/t/8UNuTLMaRfus) to start a recording and to generate identifiers for the Recording and Deployment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all deployments to identify deployment in question\n",
    "dep_db = metadata.get_metadata(\"dep_DB\")\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "rec_db = metadata.get_metadata(\"rec_DB\")\n",
    "animal_db = metadata.get_metadata(\"animal_DB\")\n",
    "dep_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a deployment\n",
    "Select a deployment by entering the index for the deployment in the table above that you would like to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Display relevant information to help the user decide\n",
    "print(\"Please choose a deployment by selecting its index:\")\n",
    "dep_db[['Deployment Name', 'Notes']]\n",
    "\n",
    "# Step 2: Prompt the user for input\n",
    "selected_index = int(input(\"Enter the index of the deployment you want to work with: \"))\n",
    "\n",
    "# Step 3: Process the user's selection\n",
    "if 0 <= selected_index < len(dep_db):\n",
    "    selected_deployment = dep_db.iloc[selected_index]\n",
    "    print(f\"You selected the deployment: {selected_deployment['Deployment Name']}\")\n",
    "    print(f\"Description: {selected_deployment['Notes']}\")\n",
    "else:\n",
    "    print(\"Invalid index selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks to see if folder exists\n",
    "Code searches for a folder with an exact match, also acceptable if a suffix has been added. Errors will be flagged if there is more than one folder matching deployment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to deployment folder\n",
    "deployment_folder = os.path.join(data_dir, selected_deployment['Deployment Name'])\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Deployment folder path: {deployment_folder}\")\n",
    "\n",
    "# Step 1: Check if the folder exists\n",
    "if os.path.exists(deployment_folder):\n",
    "    print(f\"Deployment folder found: {deployment_folder}\")\n",
    "else:\n",
    "    # Step 2: If not found, search for a folder that starts with the deployment name\n",
    "    print(f\"Folder {deployment_folder} not found. Searching for folders with a similar name...\")\n",
    "    \n",
    "    # Get a list of all folders in the data directory\n",
    "    possible_folders = [folder for folder in os.listdir(data_dir) \n",
    "                        if folder.startswith(selected_deployment['Deployment Name'])]\n",
    "    \n",
    "    if len(possible_folders) == 1:\n",
    "        # If exactly one match is found, use that folder\n",
    "        deployment_folder = os.path.join(data_dir, possible_folders[0])\n",
    "        print(f\"Using the found folder: {deployment_folder}\")\n",
    "    elif len(possible_folders) > 1:\n",
    "        # If multiple matches are found, ask the user to select one\n",
    "        print(\"Multiple matching folders found. Please select one:\")\n",
    "        for i, folder in enumerate(possible_folders):\n",
    "            print(f\"{i}: {folder}\")\n",
    "        selected_index = int(input(\"Enter the index of the folder you want to use: \"))\n",
    "        if 0 <= selected_index < len(possible_folders):\n",
    "            deployment_folder = os.path.join(data_dir, possible_folders[selected_index])\n",
    "            print(f\"Using the selected folder: {deployment_folder}\")\n",
    "        else:\n",
    "            print(\"Invalid selection. Aborting.\")\n",
    "            deployment_folder = None\n",
    "    else:\n",
    "        # If no matches are found, return an error\n",
    "        print(\"Error: Folder not found.\")\n",
    "        deployment_folder = None\n",
    "\n",
    "# Continue processing if a valid folder was found\n",
    "if deployment_folder:\n",
    "    # Perform actions with the selected deployment folder\n",
    "    pass  # Replace with your processing logic\n",
    "else:\n",
    "    print(\"Processing aborted due to missing folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "Using `DataReader` class to query the files in the folder using logger IDs. Will flag unrecognized file types and summarize which loggers' data has been recognized. Will output copies of processed CSV data into an `outputs` folder that will be created within the original deployment folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the datareader class\n",
    "datareader = DataReader(deployment_folder)\n",
    "\n",
    "# Call the read_files method\n",
    "datareader.read_files(metadata, save_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concatenate_csvs(folder_path, deployment_name):\n",
    "    # List CSV files that start with the given Deployment Name\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv') and file.startswith(deployment_name)]\n",
    "    \n",
    "    # Dictionary to hold lists of files grouped by logger ID\n",
    "    logger_groups = {}\n",
    "\n",
    "    for file in csv_files:\n",
    "        # Extract the logger ID, assuming it's the part after the \"Deployment Name\" and before the last underscore\n",
    "        logger_id = file.split('_')[-2]\n",
    "        \n",
    "        # Group files by their logger ID\n",
    "        if logger_id not in logger_groups:\n",
    "            logger_groups[logger_id] = []\n",
    "        logger_groups[logger_id].append(file)\n",
    "    \n",
    "    # Sort each group's files by the number in the filename\n",
    "    for logger_id in logger_groups:\n",
    "        logger_groups[logger_id].sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    \n",
    "    # Concatenate dataframes for each logger ID if more than one file exists, and save the result\n",
    "    concatenated_dfs = []\n",
    "    for logger_id, files in logger_groups.items():\n",
    "        if len(files) > 1:  # Only concatenate if there is more than one file\n",
    "            print(f\"Concatenating {len(files)} files for logger: {logger_id}\")\n",
    "            dfs = [pd.read_csv(os.path.join(folder_path, file)) for file in files]\n",
    "            concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "            \n",
    "            # Save the concatenated dataframe to a CSV file\n",
    "            output_filename = f\"{deployment_name}_{logger_id}_ALL.csv\"\n",
    "            output_filepath = os.path.join(folder_path, output_filename)\n",
    "            concatenated_df.to_csv(output_filepath, index=False)\n",
    "            print(f\"Saved concatenated file to: {output_filepath}\")\n",
    "            concatenated_dfs.append(concatenated_df)\n",
    "        else:\n",
    "            print(f\"Skipping concatenation for logger: {logger_id}, only one file found.\")\n",
    "    \n",
    "    # Optionally, return the final concatenated dataframe\n",
    "    if concatenated_dfs:\n",
    "        return pd.concat(concatenated_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty dataframe if no concatenation was performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate consecutive files\n",
    "\n",
    "Checks to see if there are multiple files from a single logger to concatenate in the outputs folder. If needed, it concatenates them and outputs a concatenated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to concatenate CSV files and save the result\n",
    "folder_path = os.path.join(deployment_folder, \"outputs\")\n",
    "deployment_name = selected_deployment['Deployment Name']\n",
    "\n",
    "concatenated_data = read_and_concatenate_csvs(folder_path, deployment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['datetime'] = pd.to_datetime(final_df[\" Date (local)\"] + ' ' + final_df[\" Time (local)\"], format='%d.%m.%Y %H:%M:%S.%f')\n",
    "final_df['datetime'] = final_df['datetime'].dt.tz_localize(pytz.timezone('America/Los_Angeles'))\n",
    "print(final_df['datetime'][0])\n",
    "\n",
    "# Calculate time differences and cumulative sum of differences\n",
    "sec_diff = final_df['datetime'].diff().dt.total_seconds()\n",
    "final_df['cum_diff'] = np.cumsum(sec_diff)\n",
    "\n",
    "# Check for inconsistencies (time jumps)\n",
    "mean_diff = sec_diff.mean()\n",
    "time_jumps = sec_diff[sec_diff > mean_diff * 2]  # Define a threshold for time jumps\n",
    "\n",
    "# Report any inconsistencies\n",
    "if not time_jumps.empty:\n",
    "    print(f\"Time jumps detected:\\n{time_jumps}\")\n",
    "else:\n",
    "    print(\"No significant time jumps detected.\")\n",
    "    print(f\"Sampling frequency: {1 / mean_diff} Hz\")\n",
    "\n",
    "# Plot cumulative differences\n",
    "plt.plot(final_df['datetime'], final_df['cum_diff'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Difference (seconds)')\n",
    "plt.title('Cumulative Difference over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prep\n",
    "CO_df = final_df \n",
    "\n",
    "print(CO_df['datetime'][1]-CO_df['datetime'][0])\n",
    "CO_fs = 1/(CO_df['datetime'][1]-CO_df['datetime'][0]).total_seconds()\n",
    "CO_max_timediff = np.max(np.diff(CO_df['datetime']))\n",
    "print(f\"CATS Sampling frequency: {CO_fs} Hz with a maximum time difference of {CO_max_timediff}\")\n",
    "\n",
    "# Load the data_reader object from the pickle file\n",
    "with open('outputs/data_reader.pkl', 'rb') as file:\n",
    "    data_reader = pickle.load(file)\n",
    "\n",
    "# Get the ECG and timestamp data\n",
    "ecg_df = data_reader.data_raw['2024-06-17_oror-002-001a_UF-04_001']\n",
    "ecg_df['datetime'] = pd.to_datetime(ecg_df['timestamp'])\n",
    "ecg_df['datetime'] = ecg_df['datetime'].dt.tz_localize(pytz.timezone('America/Los_Angeles'))\n",
    "print(ecg_df['datetime'][0])\n",
    "print(ecg_df)\n",
    "\n",
    "print(ecg_df['datetime'][1]-ecg_df['datetime'][0])\n",
    "ecg_fs = 1/(ecg_df['datetime'][1]-ecg_df['datetime'][0]).total_seconds()\n",
    "ecg_max_timediff = np.max(np.diff(ecg_df['datetime']))\n",
    "print(f\"ECG Sampling frequency: {ecg_fs} Hz with a maximum time difference of {ecg_max_timediff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sampling_rate = 10\n",
    "ecg_conversion = int(ecg_fs / new_sampling_rate)\n",
    "CATS_conversion = int(CO_fs / new_sampling_rate)\n",
    "\n",
    "ecg_df10 = ecg_df.iloc[::ecg_conversion, :] # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "CO_df10 = CO_df.iloc[::CATS_conversion, :] # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 10))\n",
    "\n",
    "axs[0].plot(CO_df10['datetime'], CO_df10['Accelerometer X [m/s²]'])\n",
    "axs[0].set_ylabel('Accelerometer X [m/s²]')\n",
    "\n",
    "axs[1].plot(CO_df10['datetime'], CO_df10['Accelerometer Y [m/s²]'])\n",
    "axs[1].set_ylabel('Accelerometer Y [m/s²]')\n",
    "\n",
    "axs[2].plot(CO_df10['datetime'], CO_df10['Accelerometer Z [m/s²]'])\n",
    "axs[2].set_ylabel('Accelerometer Z [m/s²]')\n",
    "\n",
    "axs[3].plot(CO_df10['datetime'], CO_df10['Depth (100bar) [m]'])\n",
    "axs[3].set_ylabel('Depth (100bar) [m]')\n",
    "\n",
    "axs[4].plot(ecg_df10['datetime'], ecg_df10['ecg'])\n",
    "axs[4].set_ylabel('ECG [mV]')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "new_CATS_sampling_rate = 10\n",
    "new_ecg_sampling_rate = 50\n",
    "ecg_conversion = int(ecg_fs / new_ecg_sampling_rate)\n",
    "CATS_conversion = int(CO_fs / new_CATS_sampling_rate)\n",
    "\n",
    "ecg_df50 = ecg_df.iloc[::ecg_conversion, :]  # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "CO_df10 = CO_df.iloc[::CATS_conversion, :]  # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "\n",
    "# Add ECG plot\n",
    "fig.add_trace(go.Scatter(x=ecg_df10['datetime'], y=ecg_df10['ecg'], mode='lines', name='ECG [mV]', line=dict(color='orange')), row=1, col=1)\n",
    "\n",
    "# Add Depth plot\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Depth (100bar) [m]'], mode='lines', name='Depth [m]', line=dict(color='purple')), row=2, col=1)\n",
    "fig.update_yaxes(autorange=\"reversed\", row=2, col=1)\n",
    "\n",
    "# Add Accelerometer plots\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer X [m/s²]'], mode='lines', name='Accel X [m/s²]', line=dict(color='blue')), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer Y [m/s²]'], mode='lines', name='Accel Y [m/s²]', line=dict(color='green')), row=4, col=1)\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer Z [m/s²]'], mode='lines', name='Accel Z [m/s²]', line=dict(color='red')), row=5, col=1)\n",
    "\n",
    "# Add Gyroscope Y plot\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Gyroscope X [mrad/s]'], mode='lines', name='Gyr X [mrad/s]', line=dict(color='pink')), row=6, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Subsampled Data Plots\", showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Datetime\", row=6, col=1)\n",
    "\n",
    "# Update y-axes labels\n",
    "fig.update_yaxes(title_text=\"ECG [mV]\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Depth [m]\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel X [m/s²]\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel Y [m/s²]\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel Z [m/s²]\", row=5, col=1)\n",
    "fig.update_yaxes(title_text=\"Gyr X [mrad/s]\", row=6, col=1)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases()\n",
    "\n",
    "# Get the logger database\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "\n",
    "# Determine unique LoggerIDs from the logger metadata dataframe\n",
    "logger_ids = set(logger_db['LoggerID'])\n",
    "print(f\"Unique Logger IDs: {logger_ids}\")\n",
    "\n",
    "# Breakdown of loggers by type\n",
    "logger_breakdown = logger_db.groupby(['Manufacturer', 'Type']).size().reset_index(name='Count')\n",
    "print(\"Logger Breakdown by Manufacturer and Type:\")\n",
    "print(logger_breakdown)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
