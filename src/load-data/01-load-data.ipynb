{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from notion_client import Client\n",
    "from dotenv import load_dotenv\n",
    "from datareader import DataReader\n",
    "from metadata import Metadata\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import nbformat\n",
    "print(nbformat.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current working directory to the root directory\n",
    "os.chdir(\"/Users/jessiekb/Documents/GitHub/Finescale-HR\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of CSV files in the outputs/ folder that match the specified criteria\n",
    "RecID = '2024-06-17_oror-002-001a_CO-68'\n",
    "folder_path = os.path.join(os.getcwd(), \"outputs\")\n",
    "\n",
    "# List and sort the CSV files\n",
    "csv_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.csv') and file.startswith(RecID)], key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "print(csv_files)\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Read and append each CSV file to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes in the list\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the final dataframe\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code to handle spaces in column names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and concatenate CSV files in order\n",
    "def read_and_concatenate_csvs(folder_path, RecID):\n",
    "    # List and sort the CSV files\n",
    "    csv_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.csv') and file.startswith(RecID)], key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Path and RecID\n",
    "folder_path = os.path.join(os.getcwd(), \"outputs\")\n",
    "RecID = '2024-06-17_oror-002-001a_CO-68'\n",
    "\n",
    "# Read and concatenate data\n",
    "final_df = read_and_concatenate_csvs(folder_path, RecID)\n",
    "\n",
    "# Get datetime\n",
    "\n",
    "final_df['datetime'] = pd.to_datetime(final_df[\" Date (local)\"] + ' ' + final_df[\" Time (local)\"], format='%d.%m.%Y %H:%M:%S.%f')\n",
    "final_df['datetime'] = final_df['datetime'].dt.tz_localize(pytz.timezone('America/Los_Angeles'))\n",
    "print(final_df['datetime'][0])\n",
    "\n",
    "# Calculate time differences and cumulative sum of differences\n",
    "sec_diff = final_df['datetime'].diff().dt.total_seconds()\n",
    "final_df['cum_diff'] = np.cumsum(sec_diff)\n",
    "\n",
    "# Check for inconsistencies (time jumps)\n",
    "mean_diff = sec_diff.mean()\n",
    "time_jumps = sec_diff[sec_diff > mean_diff * 2]  # Define a threshold for time jumps\n",
    "\n",
    "# Report any inconsistencies\n",
    "if not time_jumps.empty:\n",
    "    print(f\"Time jumps detected:\\n{time_jumps}\")\n",
    "else:\n",
    "    print(\"No significant time jumps detected.\")\n",
    "    print(f\"Sampling frequency: {1 / mean_diff} Hz\")\n",
    "\n",
    "# Plot cumulative differences\n",
    "plt.plot(final_df['datetime'], final_df['cum_diff'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Difference (seconds)')\n",
    "plt.title('Cumulative Difference over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prep\n",
    "CO_df = final_df \n",
    "\n",
    "print(CO_df['datetime'][1]-CO_df['datetime'][0])\n",
    "CO_fs = 1/(CO_df['datetime'][1]-CO_df['datetime'][0]).total_seconds()\n",
    "CO_max_timediff = np.max(np.diff(CO_df['datetime']))\n",
    "print(f\"CATS Sampling frequency: {CO_fs} Hz with a maximum time difference of {CO_max_timediff}\")\n",
    "\n",
    "# Load the data_reader object from the pickle file\n",
    "with open('outputs/data_reader.pkl', 'rb') as file:\n",
    "    data_reader = pickle.load(file)\n",
    "\n",
    "# Get the ECG and timestamp data\n",
    "ecg_df = data_reader.data_raw['2024-06-17_oror-002-001a_UF-04_001']\n",
    "ecg_df['datetime'] = pd.to_datetime(ecg_df['timestamp'])\n",
    "ecg_df['datetime'] = ecg_df['datetime'].dt.tz_localize(pytz.timezone('America/Los_Angeles'))\n",
    "print(ecg_df['datetime'][0])\n",
    "print(ecg_df)\n",
    "\n",
    "print(ecg_df['datetime'][1]-ecg_df['datetime'][0])\n",
    "ecg_fs = 1/(ecg_df['datetime'][1]-ecg_df['datetime'][0]).total_seconds()\n",
    "ecg_max_timediff = np.max(np.diff(ecg_df['datetime']))\n",
    "print(f\"ECG Sampling frequency: {ecg_fs} Hz with a maximum time difference of {ecg_max_timediff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sampling_rate = 10\n",
    "ecg_conversion = int(ecg_fs / new_sampling_rate)\n",
    "CATS_conversion = int(CO_fs / new_sampling_rate)\n",
    "\n",
    "ecg_df10 = ecg_df.iloc[::ecg_conversion, :] # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "CO_df10 = CO_df.iloc[::CATS_conversion, :] # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 10))\n",
    "\n",
    "axs[0].plot(CO_df10['datetime'], CO_df10['Accelerometer X [m/s²]'])\n",
    "axs[0].set_ylabel('Accelerometer X [m/s²]')\n",
    "\n",
    "axs[1].plot(CO_df10['datetime'], CO_df10['Accelerometer Y [m/s²]'])\n",
    "axs[1].set_ylabel('Accelerometer Y [m/s²]')\n",
    "\n",
    "axs[2].plot(CO_df10['datetime'], CO_df10['Accelerometer Z [m/s²]'])\n",
    "axs[2].set_ylabel('Accelerometer Z [m/s²]')\n",
    "\n",
    "axs[3].plot(CO_df10['datetime'], CO_df10['Depth (100bar) [m]'])\n",
    "axs[3].set_ylabel('Depth (100bar) [m]')\n",
    "\n",
    "axs[4].plot(ecg_df10['datetime'], ecg_df10['ecg'])\n",
    "axs[4].set_ylabel('ECG [mV]')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "new_CATS_sampling_rate = 10\n",
    "new_ecg_sampling_rate = 50\n",
    "ecg_conversion = int(ecg_fs / new_ecg_sampling_rate)\n",
    "CATS_conversion = int(CO_fs / new_CATS_sampling_rate)\n",
    "\n",
    "ecg_df50 = ecg_df.iloc[::ecg_conversion, :]  # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "CO_df10 = CO_df.iloc[::CATS_conversion, :]  # To subsample from 400Hz to 10Hz (1 out of every 40 samples)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "\n",
    "# Add ECG plot\n",
    "fig.add_trace(go.Scatter(x=ecg_df10['datetime'], y=ecg_df10['ecg'], mode='lines', name='ECG [mV]', line=dict(color='orange')), row=1, col=1)\n",
    "\n",
    "# Add Depth plot\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Depth (100bar) [m]'], mode='lines', name='Depth [m]', line=dict(color='purple')), row=2, col=1)\n",
    "fig.update_yaxes(autorange=\"reversed\", row=2, col=1)\n",
    "\n",
    "# Add Accelerometer plots\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer X [m/s²]'], mode='lines', name='Accel X [m/s²]', line=dict(color='blue')), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer Y [m/s²]'], mode='lines', name='Accel Y [m/s²]', line=dict(color='green')), row=4, col=1)\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Accelerometer Z [m/s²]'], mode='lines', name='Accel Z [m/s²]', line=dict(color='red')), row=5, col=1)\n",
    "\n",
    "# Add Gyroscope Y plot\n",
    "fig.add_trace(go.Scatter(x=CO_df10['datetime'], y=CO_df10['Gyroscope X [mrad/s]'], mode='lines', name='Gyr X [mrad/s]', line=dict(color='pink')), row=6, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Subsampled Data Plots\", showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Datetime\", row=6, col=1)\n",
    "\n",
    "# Update y-axes labels\n",
    "fig.update_yaxes(title_text=\"ECG [mV]\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Depth [m]\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel X [m/s²]\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel Y [m/s²]\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Accel Z [m/s²]\", row=5, col=1)\n",
    "fig.update_yaxes(title_text=\"Gyr X [mrad/s]\", row=6, col=1)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Metadata class\n",
    "metadata = Metadata()\n",
    "metadata.fetch_databases()\n",
    "\n",
    "# Get the logger database\n",
    "logger_db = metadata.get_metadata(\"logger_DB\")\n",
    "\n",
    "# Determine unique LoggerIDs from the logger metadata dataframe\n",
    "logger_ids = set(logger_db['LoggerID'])\n",
    "print(f\"Unique Logger IDs: {logger_ids}\")\n",
    "\n",
    "# Breakdown of loggers by type\n",
    "logger_breakdown = logger_db.groupby(['Manufacturer', 'Type']).size().reset_index(name='Count')\n",
    "print(\"Logger Breakdown by Manufacturer and Type:\")\n",
    "print(logger_breakdown)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finescale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
